name: Performance Tests

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'

# Add permissions for the workflow to comment on PRs
permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  performance-tests:
    runs-on: ubuntu-latest
    
    services:
      kafka:
        image: confluentinc/cp-kafka:7.9.1
        env:
          KAFKA_NODE_ID: 1
          KAFKA_PROCESS_ROLES: "broker,controller"
          KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://kafka:29093"
          KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://localhost:9092"
          KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:29093"
          KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
          KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
          CLUSTER_ID: "citest123456789012345678901"
          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
          KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
          KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
          KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
          KAFKA_LOG_DIRS: "/tmp/kraft-combined-logs"
        ports:
          - 9092:9092
          - 29093:29093
        options: >-
          --health-cmd "kafka-topics --bootstrap-server localhost:9092 --list"
          --health-interval 15s
          --health-timeout 10s
          --health-retries 10

    steps:
    - uses: actions/checkout@v4
    
    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
    
    - name: Wait for Kafka (KRaft)
      run: |
        echo "Waiting for Kafka KRaft to be ready..."
        timeout 120 bash -c 'until nc -z localhost 9092; do sleep 2; done'
        echo "Kafka port is open, waiting for cluster to be ready..."
        sleep 15
        echo "Kafka KRaft is ready!"
    
    - name: Run JSON Performance Test
      run: |
        echo "Running JSON Performance Test..."
        cargo run --release --example json_performance_test > json_perf_results.txt 2>&1
        cat json_perf_results.txt
        echo "Waiting 45 seconds before next performance test..."
        sleep 45
    
    - name: Run Raw Bytes Performance Test
      run: |
        echo "Running Raw Bytes Performance Test..."
        cargo run --release --example raw_bytes_performance_test > raw_perf_results.txt 2>&1
        cat raw_perf_results.txt
        echo "Waiting 45 seconds before next performance test..."
        sleep 45
    
    - name: Run Latency Performance Test
      run: |
        echo "Running Latency Performance Test..."
        cargo run --release --example latency_performance_test > latency_perf_results.txt 2>&1
        cat latency_perf_results.txt
        echo "Waiting 45 seconds before next performance test..."
        sleep 45
    
    - name: Run Resource Monitoring Test
      run: |
        echo "Running Resource Monitoring Test..."
        cargo run --release --example resource_monitoring_test > resource_perf_results.txt 2>&1
        cat resource_perf_results.txt
    
    - name: Extract Performance Metrics
      run: |
        echo "Extracting performance metrics..."
        
        # Debug: Show relevant lines from results files
        echo "=== JSON Performance Results (Messages Sent lines) ==="
        grep "Messages Sent" json_perf_results.txt || echo "No 'Messages Sent' lines found in json_perf_results.txt"
        
        echo "=== Raw Performance Results (Messages Sent lines) ==="
        grep "Messages Sent" raw_perf_results.txt || echo "No 'Messages Sent' lines found in raw_perf_results.txt"
        
        echo "=== Latency Performance Results (P95 Latency lines) ==="
        grep "P95 Latency" latency_perf_results.txt || echo "No 'P95 Latency' lines found in latency_perf_results.txt"
        
        # Extract JSON throughput (matches format: "Messages Sent:     12345 (123.4 msg/s)")
        JSON_THROUGHPUT=$(grep -oP 'Messages Sent:\s+\d+\s+\(\K\d+\.\d+(?=\s+msg/s\))' json_perf_results.txt | head -1 || echo "0")
        
        # Extract Raw throughput (matches format: "Messages Sent:     12345 (123.4 msg/s)")
        RAW_THROUGHPUT=$(grep -oP 'Messages Sent:\s+\d+\s+\(\K\d+\.\d+(?=\s+msg/s\))' raw_perf_results.txt | head -1 || echo "0")
        
        # Extract P95 latency (matches format: "   P95 Latency:       123.4 ms")
        P95_LATENCY=$(grep -oP 'P95 Latency:\s+\K\d+\.\d+(?=\s+ms)' latency_perf_results.txt | head -1 || echo "999")
        
        echo "JSON_THROUGHPUT=$JSON_THROUGHPUT" >> $GITHUB_ENV
        echo "RAW_THROUGHPUT=$RAW_THROUGHPUT" >> $GITHUB_ENV  
        echo "P95_LATENCY=$P95_LATENCY" >> $GITHUB_ENV
        
        echo "Performance Metrics:"
        echo "JSON Throughput: $JSON_THROUGHPUT msg/s"
        echo "Raw Throughput: $RAW_THROUGHPUT msg/s"
        echo "P95 Latency: $P95_LATENCY ms"
    
    - name: Performance Regression Check
      run: |
        echo "Checking for performance regressions..."
        
        # Define minimum acceptable performance thresholds
        MIN_JSON_THROUGHPUT=1000
        MIN_RAW_THROUGHPUT=5000
        MAX_P95_LATENCY=100
        
        # Check JSON throughput
        if (( $(echo "$JSON_THROUGHPUT < $MIN_JSON_THROUGHPUT" | bc -l) )); then
          echo "‚ùå JSON throughput regression: $JSON_THROUGHPUT < $MIN_JSON_THROUGHPUT msg/s"
          exit 1
        fi
        
        # Check Raw throughput
        if (( $(echo "$RAW_THROUGHPUT < $MIN_RAW_THROUGHPUT" | bc -l) )); then
          echo "‚ùå Raw throughput regression: $RAW_THROUGHPUT < $MIN_RAW_THROUGHPUT msg/s"
          exit 1
        fi
        
        # Check P95 latency
        if (( $(echo "$P95_LATENCY > $MAX_P95_LATENCY" | bc -l) )); then
          echo "‚ùå Latency regression: $P95_LATENCY > $MAX_P95_LATENCY ms"
          exit 1
        fi
        
        echo "‚úÖ All performance checks passed!"
    
    - name: Upload Performance Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-results
        path: |
          json_perf_results.txt
          raw_perf_results.txt
          latency_perf_results.txt
          resource_perf_results.txt
    
    - name: Comment Performance Results (PR only)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const jsonThroughput = process.env.JSON_THROUGHPUT;
          const rawThroughput = process.env.RAW_THROUGHPUT;
          const p95Latency = process.env.P95_LATENCY;
          
          const comment = `## üìä Performance Test Results
          
          | Metric | Value | Status |
          |--------|-------|--------|
          | JSON Throughput | ${jsonThroughput} msg/s | ${jsonThroughput > 1000 ? '‚úÖ' : '‚ùå'} |
          | Raw Throughput | ${rawThroughput} msg/s | ${rawThroughput > 5000 ? '‚úÖ' : '‚ùå'} |
          | P95 Latency | ${p95Latency} ms | ${p95Latency < 100 ? '‚úÖ' : '‚ùå'} |
          
          Performance tests completed automatically. Check the [workflow run](${context.payload.pull_request.html_url}/checks) for detailed results.`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });