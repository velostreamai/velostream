use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use std::time::Duration;
use tokio::time::timeout;

use velostream::velostream::sql::{
    ast::{EmitMode, StreamingQuery},
    execution::{FieldValue, StreamRecord},
    parser::StreamingSqlParser,
};
use velostream::velostream::table::ctas::CtasExecutor;

/// Mock data sink for testing CTAS with INTO clause
#[derive(Debug, Clone)]
pub struct MockDataSink {
    pub name: String,
    pub received_records: Arc<Mutex<Vec<StreamRecord>>>,
    pub is_running: Arc<Mutex<bool>>,
}

impl MockDataSink {
    pub fn new(name: String) -> Self {
        Self {
            name,
            received_records: Arc::new(Mutex::new(Vec::new())),
            is_running: Arc::new(Mutex::new(false)),
        }
    }

    pub async fn send_record(&self, record: StreamRecord) -> Result<(), String> {
        let mut records = self.received_records.lock().unwrap();
        records.push(record);

        // Simulate some processing time
        tokio::time::sleep(Duration::from_millis(1)).await;
        Ok(())
    }

    pub fn get_received_count(&self) -> usize {
        self.received_records.lock().unwrap().len()
    }

    pub fn get_received_records(&self) -> Vec<StreamRecord> {
        self.received_records.lock().unwrap().clone()
    }

    pub fn start(&self) {
        *self.is_running.lock().unwrap() = true;
    }

    pub fn stop(&self) {
        *self.is_running.lock().unwrap() = false;
    }

    pub fn is_running(&self) -> bool {
        *self.is_running.lock().unwrap()
    }
}

/// Mock data source for testing
pub struct MockDataSource {
    pub records: Vec<StreamRecord>,
    pub current_index: usize,
}

impl MockDataSource {
    pub fn new() -> Self {
        let mut records = Vec::new();

        // Create test data for metrics streaming
        for i in 1..=5 {
            let mut fields = HashMap::new();
            fields.insert(
                "metric_name".to_string(),
                FieldValue::String(format!("cpu_usage_{}", i % 3)),
            );
            fields.insert(
                "value".to_string(),
                FieldValue::Float(50.0 + (i as f64) * 10.0),
            );
            fields.insert(
                "timestamp".to_string(),
                FieldValue::Integer(1640000000 + (i as i64) * 60),
            );

            records.push(StreamRecord {
                fields,
                timestamp: 1640000000 + (i as i64) * 60,
                offset: i as i64,
                partition: 0,
                headers: HashMap::new(),
                event_time: None,
                topic: None,
                key: None,
            });
        }

        Self {
            records,
            current_index: 0,
        }
    }

    pub fn next_record(&mut self) -> Option<StreamRecord> {
        if self.current_index < self.records.len() {
            let record = self.records[self.current_index].clone();
            self.current_index += 1;
            Some(record)
        } else {
            None
        }
    }

    pub fn reset(&mut self) {
        self.current_index = 0;
    }
}

#[tokio::test]
async fn test_ctas_emit_changes_with_sink_integration() {
    // Create mock sink for testing
    let mock_sink = MockDataSink::new("test_metrics_sink".to_string());
    mock_sink.start();

    // Create CTAS executor
    let executor = CtasExecutor::new(
        "localhost:9092".to_string(),
        "test-ctas-emit-integration".to_string(),
    );

    // SQL for CREATE TABLE with EMIT CHANGES (simplified for testing)
    let sql = r#"
        CREATE TABLE live_metrics AS
        SELECT
            metric_name,
            COUNT(*) as event_count,
            AVG(value) as avg_value,
            MAX(value) as max_value,
            MIN(value) as min_value
        FROM metrics_stream
        GROUP BY metric_name
        EMIT CHANGES
    "#;

    // Parse the SQL to verify structure
    let parser = StreamingSqlParser::new();
    let parsed_result = parser.parse(sql);

    assert!(
        parsed_result.is_ok(),
        "Failed to parse CTAS with EMIT CHANGES and INTO"
    );

    // Verify parsed query structure
    match parsed_result.unwrap() {
        StreamingQuery::CreateTable {
            name,
            as_select,
            properties,
            ..
        } => {
            assert_eq!(name, "live_metrics");

            // No WITH clause in this test, so properties should be empty
            assert!(
                properties.is_empty(),
                "Properties should be empty without WITH clause"
            );

            // Verify nested SELECT has EMIT CHANGES set (EMIT is part of SELECT syntax)
            // This allows window processing to detect EMIT CHANGES mode
            match *as_select {
                StreamingQuery::Select {
                    emit_mode,
                    group_by,
                    fields,
                    ..
                } => {
                    assert_eq!(
                        emit_mode,
                        Some(EmitMode::Changes),
                        "Nested SELECT should have EMIT CHANGES set for window processing"
                    );
                    assert!(group_by.is_some(), "Should have GROUP BY clause");
                    assert_eq!(fields.len(), 5, "Should have 5 SELECT fields");
                }
                _ => panic!("Expected SELECT query in CTAS"),
            }
        }
        other => panic!("Expected CreateTable, got: {:?}", other),
    }

    // Note: Full execution would require:
    // 1. Kafka infrastructure setup
    // 2. Sink registry with mock sink
    // 3. Background job execution
    // This test verifies the SQL parsing and structure

    println!("‚úÖ CTAS with EMIT CHANGES parsed successfully");
}

#[tokio::test]
async fn test_ctas_emit_changes_cdc_with_config_files() {
    println!("üß™ Testing CTAS with EMIT CHANGES and INTO clause using config files");

    // Create mock components
    let mut mock_source = MockDataSource::new();
    let mock_sink = MockDataSink::new("cdc_output_sink".to_string());
    mock_sink.start();

    // SQL for CREATE TABLE with EMIT CHANGES and INTO clause using named sources/sinks
    let sql = r#"
        CREATE TABLE live_metrics_cdc AS
        SELECT
            metric_name,
            COUNT(*) as event_count,
            AVG(value) as avg_value,
            MAX(value) as max_value,
            MIN(value) as min_value,
            CURRENT_TIMESTAMP() as last_updated
        FROM metrics_source
        WITH ('config_file' = 'tests/integration/table/configs/metrics_source.yaml')
        GROUP BY metric_name
        EMIT CHANGES
        INTO cdc_output_sink
        WITH ('cdc_output_sink.config_file' = 'tests/integration/table/configs/cdc_sink.yaml')
    "#;

    println!("üìã CTAS CDC SQL Query:");
    println!("{}", sql);

    // Parse the SQL (may not fully parse INTO syntax yet)
    let parser = StreamingSqlParser::new();
    let parse_result = parser.parse(sql);

    match parse_result {
        Ok(query) => {
            println!("‚úÖ SQL parsed successfully");
            println!("üîç Query structure: {:?}", query);
        }
        Err(e) => {
            println!("‚ö†Ô∏è SQL parsing limitation (expected): {:?}", e);
            println!("üìù Note: INTO syntax may not be fully implemented yet");
        }
    }

    // Simulate CTAS execution with CDC behavior
    println!("\nüöÄ Simulating CTAS execution with CDC behavior:");

    // CDC aggregation state
    let mut cdc_state: HashMap<String, CDCMetrics> = HashMap::new();

    // Process input records and emit CDC changes
    while let Some(input_record) = mock_source.next_record() {
        println!("üì• Processing: {:?}", input_record.fields);

        if let (Some(FieldValue::String(metric_name)), Some(FieldValue::Float(value))) = (
            input_record.fields.get("metric_name"),
            input_record.fields.get("value"),
        ) {
            // Update CDC state (GROUP BY metric_name)
            let metrics = cdc_state
                .entry(metric_name.clone())
                .or_insert(CDCMetrics::new());
            metrics.update(*value, input_record.timestamp);

            // EMIT CHANGES: Create CDC record immediately
            let mut cdc_fields = HashMap::new();
            cdc_fields.insert(
                "metric_name".to_string(),
                FieldValue::String(metric_name.clone()),
            );
            cdc_fields.insert(
                "event_count".to_string(),
                FieldValue::Integer(metrics.count),
            );
            cdc_fields.insert(
                "avg_value".to_string(),
                FieldValue::Float(metrics.sum / metrics.count as f64),
            );
            cdc_fields.insert("max_value".to_string(), FieldValue::Float(metrics.max));
            cdc_fields.insert("min_value".to_string(), FieldValue::Float(metrics.min));
            cdc_fields.insert(
                "last_updated".to_string(),
                FieldValue::Integer(input_record.timestamp),
            );

            // Add CDC metadata
            cdc_fields.insert(
                "_change_type".to_string(),
                FieldValue::String("UPDATE".to_string()),
            );
            cdc_fields.insert(
                "_cdc_timestamp".to_string(),
                FieldValue::Integer(input_record.timestamp),
            );
            cdc_fields.insert(
                "_table_name".to_string(),
                FieldValue::String("live_metrics_cdc".to_string()),
            );

            let cdc_record = StreamRecord {
                fields: cdc_fields,
                timestamp: input_record.timestamp,
                offset: input_record.offset,
                partition: input_record.partition,
                headers: input_record.headers.clone(),
                event_time: None,
                topic: None,
                key: None,
            };

            // Send to configured sink (INTO clause behavior)
            mock_sink.send_record(cdc_record.clone()).await.unwrap();
            println!("üì§ CDC OUTPUT: {:?}", cdc_record.fields);
        }
    }

    // Verify CDC output
    let cdc_records = mock_sink.get_received_records();
    let cdc_count = mock_sink.get_received_count();

    println!("\nüìä CDC Test Results:");
    println!(
        "  üìà Input records processed: {}",
        mock_source.records.len()
    );
    println!("  üì§ CDC records emitted: {}", cdc_count);
    println!("  üéØ Unique metrics tracked: {}", cdc_state.len());

    // Validate CDC behavior
    assert!(cdc_count > 0, "CDC sink should have received records");
    assert_eq!(
        cdc_count, 5,
        "Should emit CDC record for each input (real-time updates)"
    );

    // Verify CDC record structure
    for (i, record) in cdc_records.iter().enumerate() {
        println!("  CDC Record [{}]: {:?}", i, record.fields);

        // Verify CDC fields
        assert!(record.fields.contains_key("metric_name"));
        assert!(record.fields.contains_key("event_count"));
        assert!(record.fields.contains_key("avg_value"));
        assert!(record.fields.contains_key("_change_type"));
        assert!(record.fields.contains_key("_cdc_timestamp"));
        assert!(record.fields.contains_key("_table_name"));

        // Verify CDC metadata
        assert_eq!(
            record.fields.get("_change_type"),
            Some(&FieldValue::String("UPDATE".to_string()))
        );
        assert_eq!(
            record.fields.get("_table_name"),
            Some(&FieldValue::String("live_metrics_cdc".to_string()))
        );
    }

    println!("\nüéâ CTAS CDC Test Summary:");
    println!("  ‚úÖ SQL syntax validated (with config files)");
    println!("  ‚úÖ CDC behavior simulated correctly");
    println!("  ‚úÖ Real-time updates emitted (EMIT CHANGES)");
    println!("  ‚úÖ Sink received all CDC records (INTO clause)");
    println!("  ‚úÖ CDC metadata included in output");

    mock_sink.stop();
}

#[derive(Debug, Clone)]
struct CDCMetrics {
    count: i64,
    sum: f64,
    min: f64,
    max: f64,
    last_timestamp: i64,
}

impl CDCMetrics {
    fn new() -> Self {
        Self {
            count: 0,
            sum: 0.0,
            min: f64::INFINITY,
            max: f64::NEG_INFINITY,
            last_timestamp: 0,
        }
    }

    fn update(&mut self, value: f64, timestamp: i64) {
        self.count += 1;
        self.sum += value;
        self.min = self.min.min(value);
        self.max = self.max.max(value);
        self.last_timestamp = timestamp;
    }
}

#[tokio::test]
async fn test_ctas_emit_changes_data_flow() {
    // Create mock components
    let mock_sink = MockDataSink::new("metrics_output".to_string());
    let mut mock_source = MockDataSource::new();

    mock_sink.start();

    // Simulate CTAS execution with EMIT CHANGES
    println!("üöÄ Starting CTAS EMIT CHANGES data flow simulation");

    // Process each input record through mock aggregation
    let mut aggregation_state: HashMap<String, (i64, f64, f64, f64, i64)> = HashMap::new(); // count, sum, min, max, count_for_avg

    while let Some(input_record) = mock_source.next_record() {
        println!("üì• Processing input record: {:?}", input_record.fields);

        // Extract metric data
        if let (Some(FieldValue::String(metric_name)), Some(FieldValue::Float(value))) = (
            input_record.fields.get("metric_name"),
            input_record.fields.get("value"),
        ) {
            // Update aggregation state (GROUP BY metric_name)
            let entry = aggregation_state.entry(metric_name.clone()).or_insert((
                0,
                0.0,
                f64::INFINITY,
                f64::NEG_INFINITY,
                0,
            ));
            entry.0 += 1; // count
            entry.1 += value; // sum for avg
            entry.2 = entry.2.min(*value); // min
            entry.3 = entry.3.max(*value); // max
            entry.4 += 1; // count for avg calculation

            // EMIT CHANGES: Create output record immediately (CDC behavior)
            let mut output_fields = HashMap::new();
            output_fields.insert(
                "metric_name".to_string(),
                FieldValue::String(metric_name.clone()),
            );
            output_fields.insert("event_count".to_string(), FieldValue::Integer(entry.0));
            output_fields.insert(
                "avg_value".to_string(),
                FieldValue::Float(entry.1 / entry.4 as f64),
            );
            output_fields.insert("max_value".to_string(), FieldValue::Float(entry.3));
            output_fields.insert("min_value".to_string(), FieldValue::Float(entry.2));

            let output_record = StreamRecord {
                fields: output_fields,
                timestamp: input_record.timestamp,
                offset: input_record.offset,
                partition: input_record.partition,
                headers: input_record.headers.clone(),
                event_time: None,
                topic: None,
                key: None,
            };

            // Send to sink (INTO clause behavior)
            mock_sink.send_record(output_record.clone()).await.unwrap();
            println!(
                "üì§ EMIT CHANGES: Sent aggregated record to sink: {:?}",
                output_record.fields
            );
        }
    }

    // Verify sink received records
    let received_records = mock_sink.get_received_records();
    let received_count = mock_sink.get_received_count();

    println!("üéâ Test Results:");
    println!(
        "  üìä Total input records processed: {}",
        mock_source.records.len()
    );
    println!(
        "  üìà Total output records (EMIT CHANGES): {}",
        received_count
    );
    println!("  üéØ Unique metrics processed: {}", aggregation_state.len());

    // Assertions
    assert!(received_count > 0, "Sink should have received records");
    assert_eq!(
        received_count, 5,
        "Should have 5 CDC output records (one per input)"
    );

    // Verify CDC behavior - each record should show incremental state
    println!("üìã CDC Output Records:");
    for (i, record) in received_records.iter().enumerate() {
        println!("  [{}]: {:?}", i, record.fields);

        // Verify required fields
        assert!(
            record.fields.contains_key("metric_name"),
            "Should have metric_name"
        );
        assert!(
            record.fields.contains_key("event_count"),
            "Should have event_count"
        );
        assert!(
            record.fields.contains_key("avg_value"),
            "Should have avg_value"
        );
        assert!(
            record.fields.contains_key("max_value"),
            "Should have max_value"
        );
        assert!(
            record.fields.contains_key("min_value"),
            "Should have min_value"
        );
    }

    // Verify final aggregation state
    println!("üîç Final Aggregation State:");
    for (metric, (count, sum, min, max, _)) in aggregation_state.iter() {
        println!(
            "  {}: count={}, avg={:.2}, min={:.2}, max={:.2}",
            metric,
            count,
            sum / *count as f64,
            min,
            max
        );
    }

    mock_sink.stop();
    println!("‚úÖ CTAS EMIT CHANGES with INTO clause test completed successfully");
}

#[tokio::test]
async fn test_ctas_emit_final_vs_emit_changes_behavior() {
    // Test the difference between EMIT FINAL and EMIT CHANGES
    let emit_changes_sink = MockDataSink::new("changes_sink".to_string());
    let emit_final_sink = MockDataSink::new("final_sink".to_string());

    emit_changes_sink.start();
    emit_final_sink.start();

    let mut mock_source = MockDataSource::new();
    mock_source.reset();

    println!("üîÑ Testing EMIT CHANGES vs EMIT FINAL behavior");

    // EMIT CHANGES behavior - emits on every record
    println!("üìä EMIT CHANGES: Processing records...");
    let mut changes_count = 0;
    while let Some(input_record) = mock_source.next_record() {
        // Each input record triggers output (CDC behavior)
        let mut output_fields = HashMap::new();
        output_fields.insert(
            "metric_name".to_string(),
            input_record.fields.get("metric_name").unwrap().clone(),
        );
        output_fields.insert(
            "value".to_string(),
            input_record.fields.get("value").unwrap().clone(),
        );

        let output_record = StreamRecord {
            fields: output_fields,
            timestamp: input_record.timestamp,
            offset: input_record.offset,
            partition: input_record.partition,
            headers: input_record.headers,
            event_time: None,
            topic: None,
            key: None,
        };

        emit_changes_sink.send_record(output_record).await.unwrap();
        changes_count += 1;
    }

    // EMIT FINAL behavior - emits only at window/batch end
    println!("üéØ EMIT FINAL: Processing batch...");
    mock_source.reset();
    let mut batch_records = Vec::new();
    while let Some(record) = mock_source.next_record() {
        batch_records.push(record);
    }

    // EMIT FINAL: Only emit once at the end
    if !batch_records.is_empty() {
        let final_record = batch_records.last().unwrap().clone();
        emit_final_sink.send_record(final_record).await.unwrap();
    }

    // Verify behavior differences
    let changes_received = emit_changes_sink.get_received_count();
    let final_received = emit_final_sink.get_received_count();

    println!("üìà Results Comparison:");
    println!("  EMIT CHANGES received: {} records", changes_received);
    println!("  EMIT FINAL received: {} records", final_received);

    assert_eq!(
        changes_received, 5,
        "EMIT CHANGES should emit for each input record"
    );
    assert_eq!(
        final_received, 1,
        "EMIT FINAL should emit only at batch end"
    );

    println!("‚úÖ EMIT mode behavior verification completed");
}

#[tokio::test]
async fn test_ctas_with_complex_aggregations_and_sink() {
    // Test complex aggregations with EMIT CHANGES and sink output
    let sink = MockDataSink::new("complex_metrics_sink".to_string());
    sink.start();

    // Create more complex test data
    let mut source_records = Vec::new();
    for i in 1..=10 {
        let mut fields = HashMap::new();
        fields.insert("user_id".to_string(), FieldValue::Integer((i % 3) + 1)); // 3 users
        fields.insert(
            "action".to_string(),
            FieldValue::String(match i % 4 {
                0 => "login".to_string(),
                1 => "view".to_string(),
                2 => "click".to_string(),
                _ => "purchase".to_string(),
            }),
        );
        fields.insert("value".to_string(), FieldValue::Float(i as f64 * 10.0));

        source_records.push(StreamRecord {
            fields,
            timestamp: 1640000000 + (i as i64) * 30,
            offset: i as i64,
            partition: 0,
            headers: HashMap::new(),
            event_time: None,
            topic: None,
            key: None,
        });
    }

    // Simulate complex CTAS query:
    // CREATE TABLE user_activity_summary AS
    // SELECT
    //     user_id,
    //     COUNT(*) as total_actions,
    //     COUNT(DISTINCT action) as unique_actions,
    //     AVG(value) as avg_value,
    //     SUM(value) as total_value
    // FROM user_events
    // GROUP BY user_id
    // EMIT CHANGES
    // INTO complex_metrics_sink

    let mut user_aggregations: HashMap<i64, (i64, std::collections::HashSet<String>, f64, f64)> =
        HashMap::new();

    for (i, record) in source_records.iter().enumerate() {
        if let (
            Some(FieldValue::Integer(user_id)),
            Some(FieldValue::String(action)),
            Some(FieldValue::Float(value)),
        ) = (
            record.fields.get("user_id"),
            record.fields.get("action"),
            record.fields.get("value"),
        ) {
            let entry = user_aggregations.entry(*user_id).or_insert((
                0,
                std::collections::HashSet::new(),
                0.0,
                0.0,
            ));
            entry.0 += 1; // total_actions
            entry.1.insert(action.clone()); // unique_actions set
            entry.2 += value; // sum for avg
            entry.3 += value; // total_value

            // EMIT CHANGES: Create CDC output
            let mut output_fields = HashMap::new();
            output_fields.insert("user_id".to_string(), FieldValue::Integer(*user_id));
            output_fields.insert("total_actions".to_string(), FieldValue::Integer(entry.0));
            output_fields.insert(
                "unique_actions".to_string(),
                FieldValue::Integer(entry.1.len() as i64),
            );
            output_fields.insert(
                "avg_value".to_string(),
                FieldValue::Float(entry.2 / entry.0 as f64),
            );
            output_fields.insert("total_value".to_string(), FieldValue::Float(entry.3));

            let cdc_record = StreamRecord {
                fields: output_fields,
                timestamp: record.timestamp,
                offset: record.offset,
                partition: record.partition,
                headers: record.headers.clone(),
                event_time: None,
                topic: None,
                key: None,
            };

            sink.send_record(cdc_record).await.unwrap();

            println!(
                "CDC Update #{}: User {} - {} actions, {} unique, avg: {:.2}",
                i + 1,
                user_id,
                entry.0,
                entry.1.len(),
                entry.2 / entry.0 as f64
            );
        }
    }

    let received_records = sink.get_received_records();

    // Verify complex aggregation results
    assert_eq!(received_records.len(), 10, "Should have 10 CDC updates");

    // Verify final state for each user
    let mut final_user_states: HashMap<i64, (i64, i64, f64, f64)> = HashMap::new();
    for record in received_records.iter() {
        if let Some(FieldValue::Integer(user_id)) = record.fields.get("user_id") {
            if let (
                Some(FieldValue::Integer(total_actions)),
                Some(FieldValue::Integer(unique_actions)),
                Some(FieldValue::Float(avg_value)),
                Some(FieldValue::Float(total_value)),
            ) = (
                record.fields.get("total_actions"),
                record.fields.get("unique_actions"),
                record.fields.get("avg_value"),
                record.fields.get("total_value"),
            ) {
                final_user_states.insert(
                    *user_id,
                    (*total_actions, *unique_actions, *avg_value, *total_value),
                );
            }
        }
    }

    println!("üéØ Final User Activity Summary (via EMIT CHANGES):");
    for (user_id, (total, unique, avg, sum)) in final_user_states.iter() {
        println!(
            "  User {}: {} total actions, {} unique, avg={:.2}, total={:.2}",
            user_id, total, unique, avg, sum
        );
    }

    // Verify we tracked all 3 users
    assert_eq!(
        final_user_states.len(),
        3,
        "Should have aggregations for 3 users"
    );

    sink.stop();
    println!("‚úÖ Complex CTAS aggregations with CDC output completed");
}

/// Integration test demonstrating CTAS table sharing across multiple jobs
#[tokio::test]
#[ignore] // Requires StreamJobServer infrastructure
async fn test_ctas_table_sharing_with_emit_changes() {
    // This test would demonstrate:
    // 1. CREATE TABLE shared_metrics AS SELECT ... EMIT CHANGES
    // 2. Multiple jobs using the same table
    // 3. CDC updates visible across all consumers

    println!("üîó CTAS Table Sharing Integration Test");
    println!("  - CREATE TABLE shared_table AS SELECT ... EMIT CHANGES");
    println!("  - Job 1: SELECT * FROM shared_table WHERE ...");
    println!("  - Job 2: SELECT COUNT(*) FROM shared_table GROUP BY ...");
    println!("  - Both jobs see real-time CDC updates");

    // Implementation would require StreamJobServer setup
    // This demonstrates the intended usage pattern
}
