/*!
# FR-082 Scenario 0: Pure SELECT (Passthrough) Baseline Test

**Purpose**: Measure baseline performance for pure SELECT queries (no GROUP BY, no WINDOW).

## Scenario Classification
- **Pattern**: Filter + Projection (passthrough)
- **State Management**: None (stateless)
- **Query Category**: Category 5 - Pure SELECT (17% of workload)
- **Phase 0 Target**: âŒ Not a target (no GROUP BY to optimize)
- **V2 Routing**: Round-robin across partitions (no hash key)

## Example Query
```sql
SELECT order_id, customer_id, order_date, total_amount
FROM orders
WHERE order_date > '2024-01-01'
  AND total_amount > 100;
```

## Performance Expectations
- **SQL Engine**: 1M+ rec/sec (trivial - just filter/project)
- **Job Server**: 400K+ rec/sec (I/O-bound, not CPU-bound)
- **Overhead**: ~60% (lower than aggregation scenarios)

## Key Insights
- **Bottleneck**: Serialization/I/O, not CPU
- **No State**: No hash tables, no window buffers
- **Not a Priority**: Phase 0 focuses on GROUP BY scenarios
- **Reference Baseline**: Shows job server overhead without aggregation
*/

use async_trait::async_trait;
use serial_test::serial;
use std::collections::HashMap;
use std::sync::Arc;
use std::time::Instant;
use tokio::sync::{Mutex, mpsc};
use velostream::velostream::datasource::types::SourceOffset;
use velostream::velostream::datasource::{DataReader, DataWriter};
use velostream::velostream::server::processors::{
    JobProcessor, JobProcessorConfig, JobProcessorFactory,
};
use velostream::velostream::sql::execution::StreamExecutionEngine;
use velostream::velostream::sql::execution::types::{FieldValue, StreamRecord};
use velostream::velostream::sql::parser::StreamingSqlParser;

// Import validation utilities
use super::super::validation::MetricsValidation;
// Import shared metrics helper
use super::test_helpers::JobServerMetrics;

const TEST_SQL: &str = r#"
    SELECT
        order_id,
        customer_id,
        order_date,
        total_amount
    FROM orders
    WHERE total_amount > 100
"#;

/// Simple mock data source for passthrough testing
struct PassthroughDataSource {
    records: Vec<StreamRecord>,
    read_count: usize,
}

impl PassthroughDataSource {
    fn new(num_records: usize) -> Self {
        let records = (0..num_records)
            .map(|i| {
                let mut fields = HashMap::new();
                fields.insert("order_id".to_string(), FieldValue::Integer(i as i64));
                fields.insert(
                    "customer_id".to_string(),
                    FieldValue::Integer((i % 1000) as i64),
                );
                fields.insert(
                    "order_date".to_string(),
                    FieldValue::String("2024-01-15".to_string()),
                );
                fields.insert(
                    "total_amount".to_string(),
                    FieldValue::Float(150.0 + (i % 100) as f64),
                );
                StreamRecord::new(fields)
            })
            .collect();

        Self {
            records,
            read_count: 0,
        }
    }
}

#[async_trait]
impl DataReader for PassthroughDataSource {
    async fn read(
        &mut self,
    ) -> Result<Vec<StreamRecord>, Box<dyn std::error::Error + Send + Sync>> {
        if self.read_count > 0 {
            return Ok(vec![]);
        }
        self.read_count += 1;
        Ok(self.records.clone())
    }

    async fn commit(&mut self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        Ok(())
    }

    async fn seek(
        &mut self,
        _offset: SourceOffset,
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        Ok(())
    }

    async fn has_more(&self) -> Result<bool, Box<dyn std::error::Error + Send + Sync>> {
        Ok(self.read_count == 0)
    }
}

/// Mock data writer
struct PassthroughDataWriter {
    records_written: usize,
}

impl PassthroughDataWriter {
    fn new() -> Self {
        Self { records_written: 0 }
    }
}

#[async_trait]
impl DataWriter for PassthroughDataWriter {
    async fn write(
        &mut self,
        _record: StreamRecord,
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        self.records_written += 1;
        Ok(())
    }

    async fn write_batch(
        &mut self,
        records: Vec<Arc<StreamRecord>>,
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        self.records_written += records.len();
        Ok(())
    }

    async fn update(
        &mut self,
        _key: &str,
        _record: StreamRecord,
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        Ok(())
    }

    async fn delete(&mut self, _key: &str) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        Ok(())
    }

    async fn flush(&mut self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        Ok(())
    }

    async fn commit(&mut self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        Ok(())
    }

    async fn rollback(&mut self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        Ok(())
    }
}

/// Measure pure SQL engine performance (without job server)
async fn measure_sql_engine_only(records: Vec<StreamRecord>, query: &str) -> (usize, u128) {
    let mut parser = StreamingSqlParser::new();
    let parsed_query = parser.parse(query).expect("Failed to parse SQL");
    let (tx, mut _rx) = mpsc::unbounded_channel();
    let mut engine = StreamExecutionEngine::new(tx);

    let start = Instant::now();
    for record in records.iter() {
        let _ = engine.execute_with_record(&parsed_query, &record).await;
    }
    let elapsed = start.elapsed();

    (records.len(), elapsed.as_micros())
}

#[tokio::test]
#[serial]
async fn scenario_0_pure_select_baseline() {
    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("ğŸ”¬ FR-082 Scenario 0: Pure SELECT (V1 vs V2)");
    println!("Testing through unified JobProcessor trait");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    let num_records = 5000;
    println!("Test Configuration: {} records\n", num_records);

    // Create test records for SQL engine measurement
    let records: Vec<StreamRecord> = (0..num_records)
        .map(|i| {
            let mut fields = HashMap::new();
            fields.insert("order_id".to_string(), FieldValue::Integer(i as i64));
            fields.insert(
                "customer_id".to_string(),
                FieldValue::Integer((i % 1000) as i64),
            );
            fields.insert(
                "order_date".to_string(),
                FieldValue::String("2024-01-15".to_string()),
            );
            fields.insert(
                "total_amount".to_string(),
                FieldValue::Float(150.0 + (i % 100) as f64),
            );
            StreamRecord::new(fields)
        })
        .collect();

    let parser = StreamingSqlParser::new();
    let query = Arc::new(parser.parse(TEST_SQL).expect("Parse failed"));

    // ========================================================================
    // TEST V1 (Single-threaded baseline)
    // ========================================================================
    println!("â”Œâ”€ Testing V1 (Single-threaded via JobProcessor trait)");
    let data_source_v1 = PassthroughDataSource::new(num_records);
    let data_writer_v1 = PassthroughDataWriter::new();

    let (tx_v1, _rx_v1) = mpsc::unbounded_channel();
    let engine_v1 = Arc::new(tokio::sync::RwLock::new(StreamExecutionEngine::new(tx_v1)));

    let processor_v1 = JobProcessorFactory::create(JobProcessorConfig::V2 {
        num_partitions: Some(1),
        enable_core_affinity: false,
    });
    let (_shutdown_tx_v1, shutdown_rx_v1) = mpsc::channel(1);

    let v1_start = Instant::now();
    let v1_result = processor_v1
        .process_job(
            Box::new(data_source_v1),
            Some(Box::new(data_writer_v1)),
            engine_v1.clone(),
            (*query).clone(),
            "v1_scenario0".to_string(),
            shutdown_rx_v1,
        )
        .await;
    let v1_duration = v1_start.elapsed();

    let v1_throughput = num_records as f64 / v1_duration.as_secs_f64();
    println!(
        "âœ“ V1 completed: {:.2?} ({:.0} rec/sec)\n",
        v1_duration, v1_throughput
    );

    // ========================================================================
    // TEST V2 (Multi-partition parallel via JobProcessor trait)
    // ========================================================================
    println!("â”Œâ”€ Testing V2 (Multi-partition via JobProcessor trait)");
    let num_v2_partitions = 4;
    let data_source_v2 = PassthroughDataSource::new(num_records);
    let data_writer_v2 = PassthroughDataWriter::new();

    let (tx_v2, _rx_v2) = mpsc::unbounded_channel();
    let engine_v2 = Arc::new(tokio::sync::RwLock::new(StreamExecutionEngine::new(tx_v2)));

    let processor_v2 = JobProcessorFactory::create(JobProcessorConfig::V2 {
        num_partitions: Some(num_v2_partitions),
        enable_core_affinity: false,
    });
    let (_shutdown_tx_v2, shutdown_rx_v2) = mpsc::channel(1);

    let v2_start = Instant::now();
    let v2_result = processor_v2
        .process_job(
            Box::new(data_source_v2),
            Some(Box::new(data_writer_v2)),
            engine_v2.clone(),
            (*query).clone(),
            "v2_scenario0".to_string(),
            shutdown_rx_v2,
        )
        .await;
    let v2_duration = v2_start.elapsed();

    let v2_throughput = num_records as f64 / v2_duration.as_secs_f64();
    println!(
        "âœ“ V2 ({} partitions) completed: {:.2?} ({:.0} rec/sec)\n",
        num_v2_partitions, v2_duration, v2_throughput
    );

    // ========================================================================
    // RESULTS COMPARISON
    // ========================================================================
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘ RESULTS: V1 vs V2 (Scenario 0: Pure SELECT)              â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    match (v1_result, v2_result) {
        (Ok(v1_stats), Ok(v2_stats)) => {
            // Create JobServerMetrics from stats
            let v1_metrics = JobServerMetrics::from_stats(&v1_stats, v1_duration.as_micros());
            let v2_metrics = JobServerMetrics::from_stats(&v2_stats, v2_duration.as_micros());

            let scaling_factor = v2_throughput / v1_throughput;
            let speedup = v1_duration.as_secs_f64() / v2_duration.as_secs_f64();
            let scaling_efficiency = (scaling_factor / num_v2_partitions as f64) * 100.0;

            println!("Throughput Comparison:");
            println!("  V1 (1 partition):          {:.0} rec/sec", v1_throughput);
            println!(
                "  V2 ({} partitions):        {:.0} rec/sec",
                num_v2_partitions, v2_throughput
            );
            println!("  Scaling factor:            {:.2}x", scaling_factor);
            println!("  Speedup:                   {:.2}x faster\n", speedup);

            println!("Scaling Efficiency:");
            println!(
                "  Per-core efficiency:       {:.1}% (ideal = 100%)\\n",
                scaling_efficiency
            );

            println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
            println!("â•‘ âœ… VALIDATION COMPLETE                                   â•‘");
            println!("â•‘ âœ“ V1 and V2 both tested through JobProcessor trait      â•‘");
            println!("â•‘ âœ“ Records flow through V2 partition pipeline            â•‘");
            println!("â•‘ âœ“ Scenario 0 validates both architectures               â•‘");
            println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

            // Display JobServer metrics from both processors
            println!("ğŸ“Š JOBSERVER METRICS FROM PROCESSORS");
            println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
            v1_metrics.print_table("V1");
            v2_metrics.print_table(&format!("V2 ({} partitions)", num_v2_partitions));

            // Use JobServerMetrics to validate V1 processor
            println!("âœ… VALIDATING JOBSERVER METRICS");
            println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

            // V1 Validation
            assert_eq!(
                v1_metrics.records_processed as usize, num_records,
                "V1: Should process exactly {} records (got {})",
                num_records, v1_metrics.records_processed
            );
            assert!(
                v1_metrics.throughput_rec_per_sec > 0.0,
                "V1: Should have valid throughput (got {:.0} rec/sec)",
                v1_metrics.throughput_rec_per_sec
            );
            assert_eq!(
                v1_metrics.records_failed, 0,
                "V1: Should not fail any records (failed: {})",
                v1_metrics.records_failed
            );
            assert_eq!(
                v1_metrics.batches_failed, 0,
                "V1: Should not fail any batches (failed: {})",
                v1_metrics.batches_failed
            );
            println!(
                "âœ“ V1 JobMetrics: {} records processed at {:.0} rec/sec, 0 failures, 0 batch failures\n",
                v1_metrics.records_processed, v1_metrics.throughput_rec_per_sec
            );

            // V2 Validation
            assert_eq!(
                v2_metrics.records_processed as usize, num_records,
                "V2: Should process exactly {} records (got {})",
                num_records, v2_metrics.records_processed
            );
            assert!(
                v2_metrics.throughput_rec_per_sec > 0.0,
                "V2: Should have valid throughput (got {:.0} rec/sec)",
                v2_metrics.throughput_rec_per_sec
            );
            assert_eq!(
                v2_metrics.records_failed, 0,
                "V2: Should not fail any records (failed: {})",
                v2_metrics.records_failed
            );
            assert_eq!(
                v2_metrics.batches_failed, 0,
                "V2: Should not fail any batches (failed: {})",
                v2_metrics.batches_failed
            );
            println!(
                "âœ“ V2 JobMetrics: {} records processed at {:.0} rec/sec, 0 failures, 0 batch failures",
                v2_metrics.records_processed, v2_metrics.throughput_rec_per_sec
            );
            println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

            // Validate metrics for both V1 and V2
            let v1_validation = MetricsValidation::validate_metrics(
                v1_stats.records_processed as usize,
                v1_stats.batches_processed as usize,
                v1_stats.records_failed as usize,
            );
            let v2_validation = MetricsValidation::validate_metrics(
                v2_stats.records_processed as usize,
                v2_stats.batches_processed as usize,
                v2_stats.records_failed as usize,
            );

            println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
            println!("â•‘ âœ… V1 METRICS VALIDATION                                 â•‘");
            println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
            v1_validation.print_results();

            println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
            println!("â•‘ âœ… V2 METRICS VALIDATION                                 â•‘");
            println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
            v2_validation.print_results();

            // Check that V2 processes records (V1 stats may be anomalous)
            assert!(v2_stats.records_processed > 0, "V2 should process records");

            // Report any anomalies
            if v1_stats.records_processed == 0 && v1_throughput > 0.0 {
                println!("\nâš ï¸  V1 STATS ANOMALY:");
                println!(
                    "  Throughput: {:.0} rec/sec (proves data flows)",
                    v1_throughput
                );
                println!("  Records processed: 0 (stat tracking issue)");
            }

            if v1_stats.records_failed > 0 {
                println!(
                    "\nâš ï¸  V1 marked {} records as failed",
                    v1_stats.records_failed
                );
            }

            if v2_stats.records_failed > 0 {
                println!(
                    "\nâš ï¸  V2 marked {} records as failed",
                    v2_stats.records_failed
                );
            }
        }
        _ => {
            eprintln!("âŒ One or both processors failed");
            panic!("Test failed");
        }
    }
}
