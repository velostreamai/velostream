# Kafka Data Sink Configuration  
# For producing processed financial transactions to Kafka topics

datasink:
  type: "kafka"
  name: "processed_transactions_producer"
  config:
    # Kafka connection settings
    bootstrap.servers: "localhost:9092"
    topic: "processed-transactions"
    
    # Producer configuration
    producer:
      # Delivery guarantees
      acks: "all"              # Wait for all in-sync replicas
      enable_idempotence: true # Exactly-once semantics
      retries: 3               # Retry failed sends
      
      # Performance settings
      batch.size: 16384        # 16KB batches
      linger.ms: 10            # Wait 10ms to batch records
      compression.type: "snappy" # Fast compression
      buffer_memory: 33554432  # 32MB total buffer
      
      # Timeout settings
      request_timeout_ms: 30000  # 30 seconds
      delivery_timeout_ms: 120000 # 2 minutes total
      
    # Partitioning strategy
    partitioning:
      strategy: "key_hash"  # Partition by key hash
      # Custom partitioner for customer-based partitioning:
      # strategy: "custom"
      # partitioner_class: "com.velostream.CustomerPartitioner"
      
    # Schema Registry (if using Avro)
    schema_registry:
      url: "http://localhost:8081"
      
# Serialization configuration
serialization:
  format: "avro"
  key.format: "string"  # Transaction ID as key
  value_schema_file: "./schemas/processed_transaction.avsc"
  
# Producer processing settings  
processing:
  # Async vs sync sending
  send_mode: "async"  # Options: "sync", "async"
  
  # Batch processing
  batch.size: 100
  flush_interval_ms: 5000  # Flush every 5 seconds
  
  # Error handling
  error_handling:
    strategy: "retry_then_dlq"  # Options: "fail_fast", "retry_then_dlq", "log_and_continue"
    max_retries: 3
    retry_delay_ms: 1000
    dead_letter_topic: "processed-transactions-errors"
    
  # Key extraction (for partitioning)
  key.field: "transaction_id"
  
  # Headers to add
  headers:
    pipeline_version: "1.0.0"
    processing_timestamp: "@timestamp"  # Current timestamp
    source_system: "velostream-demo"
    
  # Metrics and monitoring
  metrics:
    enabled: true
    report_interval_ms: 10000
    
# Topic management (optional - for auto-creation)
topic_config:
  num_partitions: 3
  replication.factor: 1
  config:
    cleanup.policy: "delete"
    retention.ms: 604800000  # 7 days
    segment_ms: 86400000     # 1 day segments