# Velostream Development Guide for Claude


## Behaviour
- Dont make assumptions about the behaviour of the project.
- Always run all of the tests, demos, examples and /bin' and check the documentation.
- Always check the code formatting.
- Always check the code for errors.
- Always check the code for bugs.
- Always check the code for security issues.
- Always check the code for performance issues.
- Always check the code for correctness.
- Never assume that it is ok to perform a 'workaround' with consent first
- Always look for opportunities to improve the code. (refactoring, code restructuring, etc.)
- NEVER use Mocks in the codebase - only in Tests where it is needed
- Always fix ALL compilation errors before marking any task as complete.
- Always fix ALL test failures before marking any task as complete.
- Always ensure that the code compiles and passes all tests before marking any task as complete.
- Always ensure that the code passes all pre-commit checks before marking any task as complete.
- Always ensure that the code passes all CI/CD checks before marking any task as complete.
- Always check CLIPPY errors and fix them before marking any task as complete.
- Look at run-commit.sh for pre-commit checks and ensure that all checks pass before marking any task as complete.

## Project Overview

Velostream is a high-performance streaming SQL engine written in Rust that provides real-time data processing capabilities with pluggable serialization formats (JSON, Avro, Protobuf). The project emphasizes performance, precision, and compatibility, particularly for financial analytics use cases.

## Key Components

### SQL Engine (`src/velostream/sql/`)
- **Parser**: Streaming SQL query parsing with support for windows, aggregations, joins
- **Execution Engine**: High-performance query execution with pluggable processors
- **Types System**: FieldValue-based type system for SQL execution and serialization
- **Aggregation**: Windowed and continuous aggregation processing
- **Windowing**: Tumbling, sliding, and session windows with emit modes

### Serialization (`src/velostream/serialization/`)
- **Pluggable Formats**: JSON, Avro, and Protobuf (all always available)
- **Type Conversion**: Direct FieldValue serialization with enhanced error chaining
- **Financial Precision**: ScaledInteger support for exact financial arithmetic

### Kafka Integration (`src/velostream/kafka/`)
- **Consumers/Producers**: High-performance Kafka integration with configurable serialization
- **Schema Support**: Avro schema registry integration
- **Performance Presets**: Optimized configurations for different use cases

## Recent Major Enhancements

### Advanced SQL Parser Features (Latest)

**Problem Solved**
- Limited SQL standard compliance for complex window functions
- Missing support for table aliases in PARTITION BY clauses
- No support for INTERVAL syntax in window frames
- EXTRACT function only supported non-standard syntax

**Solution Implemented**
- **Table Alias Support**: Full support for `table.column` syntax in PARTITION BY and ORDER BY clauses
- **INTERVAL Window Frames**: Native support for `RANGE BETWEEN INTERVAL '1' DAY PRECEDING AND CURRENT ROW`
- **Dual EXTRACT Syntax**: Both function call style `EXTRACT('YEAR', date)` and SQL standard `EXTRACT(YEAR FROM date)`
- **Enhanced Financial SQL**: 100% compatibility with complex financial trading queries

**Key Benefits**
```sql
-- Table aliases in complex window functions (NEW)
SELECT
    p.trader_id,
    LAG(m.price, 1) OVER (PARTITION BY p.trader_id ORDER BY m.event_time) as prev_price
FROM market_data m
JOIN positions p ON m.symbol = p.symbol;

-- INTERVAL-based window frames (NEW)
SELECT
    symbol, price,
    AVG(price) OVER (
        PARTITION BY symbol
        ORDER BY event_time
        RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW
    ) as hourly_moving_avg
FROM trades;

-- SQL standard EXTRACT syntax (NEW)
SELECT
    EXTRACT(EPOCH FROM (end_time - start_time)) as duration_seconds,
    EXTRACT(YEAR FROM order_date) as order_year
FROM orders;
```

**Financial Trading Compatibility**: Achieved 100% parser compatibility with complex financial SQL (improved from 30% to 100%).

See updated [SQL function documentation](docs/sql/functions/) for complete syntax reference.

### Compression Independence in Batch Configuration

**Problem Solved**
- Batch strategies were overriding explicit compression settings
- Users couldn't configure compression independently from batch optimizations
- Need for fine-grained control over compression vs automatic optimization

**Solution Implemented**
- **Suggestion vs Override Pattern**: Batch strategies suggest compression only when none is explicitly set
- **Full Independence**: Explicit compression settings are never overridden by batch configurations
- **Intelligent Defaults**: When no compression is specified, batch strategies provide optimal suggestions
- **Comprehensive Logging**: Detailed logging shows final applied compression settings

**Key Benefits**
```rust
// Explicit compression is always preserved
props.insert("compression.type".to_string(), "zstd".to_string());
let batch_config = BatchConfig {
    strategy: BatchStrategy::MemoryBased(1024 * 1024), // Would suggest gzip
    // ... 
};
// Result: compression.type remains "zstd" (user choice preserved)
```

See [docs/compression-independence.md](docs/developer/compression-independence.md) for complete documentation.

### SerializationError Enhancement: Comprehensive Error Chaining

**Problem Solved**
- Limited error diagnostics for serialization failures
- Loss of original error context in error chains
- Difficult debugging of cross-format serialization issues

**Solution Implemented**
- **Enhanced Error Variants**: 6 new structured error types with full source chain preservation
- **JSON/Avro/Protobuf Support**: All serialization formats now use enhanced error variants
- **Error Chain Traversal**: Full error source chain information for debugging
- **100% Backward Compatibility**: Existing error handling patterns continue to work

**Enhanced Error Types**
```rust
SerializationError::JsonError { message, source }          // JSON serialization with source
SerializationError::AvroError { message, source }          // Avro serialization with source  
SerializationError::ProtobufError { message, source }      // Protobuf serialization with source
SerializationError::TypeConversionError { message, from_type, to_type, source }
SerializationError::SchemaValidationError { message, source }
SerializationError::EncodingError { message, source }
```

### Financial Precision Enhancement

### Problem Solved
- f64 floating-point precision errors in financial calculations
- Need for exact arithmetic in financial analytics
- Performance bottlenecks in financial computations

### Solution Implemented
- **FieldValue::ScaledInteger(i64, u8)**: Stores scaled integer with decimal precision
- **42x Performance Improvement**: ScaledInteger operations are 42x faster than f64
- **Perfect Precision**: No floating-point rounding errors
- **Cross-System Compatibility**: Serializes as decimal strings for JSON/Avro

## Performance Benchmarks

Financial calculation patterns (price √ó quantity):
- **f64**: 83.458¬µs (with precision errors)
- **ScaledInteger**: 1.958¬µs (exact precision) ‚Üí **42x FASTER**
- **Decimal**: 53.583¬µs (exact precision) ‚Üí 1.5x faster than f64

## Development Commands



### Testing
```bash
# Run all tests
cargo test
2
# Unit test
cargo test --tests --verbose -- --skip integration:: --skip performance:: --skip comprehensive

# Run specific test module
cargo test unit::sql::execution::types -- --nocapture

# Run financial precision benchmarks
cargo test financial_precision_benchmark -- --nocapture

# Test specific SQL functionality
cargo test windowing_test -- --nocapture
```

### Building
```bash
# Build the project (all serialization formats included)
cargo build

# Build specific binaries
cargo build --bin velo-sql-multi
```

### Code Formatting
```bash
# Format all code (required for CI/CD)
cargo fmt --all

# Check formatting without making changes
cargo fmt --all -- --check

# CRITICAL: Always run formatting check before committing
cargo fmt --all -- --check

# Run complete pre-commit verification to ensure CI passes
cargo fmt --all -- --check && cargo check

# Run clippy to catch additional linting issues
cargo clippy --no-default-features

# Run clippy with strict warnings (for high code quality)
cargo clippy --no-default-features -- -D warnings
```

### Git Workflow
```bash
# Commit after completing significant work units
# Examples: 1 day's work, project milestone, or architectural phase
git add .
git commit -m "feat: implement Day X - [Feature Name]

Key achievements:
- [Achievement 1]
- [Achievement 2] 
- [Performance metrics if applicable]

ü§ñ Generated with Claude Code"

# Push to remote after successful local testing
git push origin branch-name
```

### Performance Testing

#### FR-082 Comprehensive Baseline Comparison
```bash
# All scenarios, release build (recommended for final benchmarks)
./run_baseline.sh

# Choose mode and scenarios flexibly
./run_baseline_flexible.sh release 1    # Release, scenario 1 only
./run_baseline_flexible.sh debug        # Debug, all scenarios
./run_baseline_flexible.sh profile 2    # Profile, scenario 2 only

# Quick iteration with minimal recompilation
./run_baseline_quick.sh

# Multiple compilation modes
./run_baseline_options.sh debug         # Fast compile
./run_baseline_options.sh release       # Optimized runtime
./run_baseline_options.sh profile       # With debug symbols
```

**Documentation:** See [`docs/benchmarks/`](docs/benchmarks/) for:
- [`SCRIPTS_README.md`](docs/benchmarks/SCRIPTS_README.md) - Complete reference guide
- [`BASELINE_TESTING.md`](docs/benchmarks/BASELINE_TESTING.md) - Detailed methodology
- [`BASELINE_QUICK_REFERENCE.md`](docs/benchmarks/BASELINE_QUICK_REFERENCE.md) - Quick cheat sheet

#### Financial Precision & Compatibility Tests
```bash
# Run financial precision tests
cargo run --bin test_financial_precision

# Test serialization compatibility
cargo run --bin test_serialization_compatibility
```


## Schema Configuration

### Kafka Schema Support
Velostream now supports comprehensive schema configuration for Kafka data sources:

**Avro Schema Configuration**:
```yaml
# Inline schema
avro.schema: |
  {
    "type": "record",
    "name": "ExampleRecord", 
    "fields": [{"name": "id", "type": "long"}]
  }

# Schema file  
avro.schema.file: "./schemas/example.avsc"
```

**Protobuf Schema Configuration**:
```yaml
# Inline schema
protobuf.schema: |
  syntax = "proto3";
  message ExampleRecord {
    int64 id = 1;
  }

# Schema file
protobuf.schema.file: "./schemas/example.proto"
```

**Key Benefits**:
- **Schema Enforcement**: Avro/Protobuf now require proper schemas (no more hardcoded fallbacks)
- **Multiple Config Keys**: Support for various naming conventions (`avro.schema`, `value.avro.schema`, etc.)
- **File Support**: Load schemas from external files for better maintainability
- **Financial Precision**: Built-in support for decimal logical types in schemas
- **Production Ready**: Schema Registry integration for centralized schema management

See [docs/kafka-schema-configuration.md](docs/developer/kafka-schema-configuration.md) for complete configuration guide.

## Code Organization

### Module Structure Guidelines
**IMPORTANT**: Use `mod.rs` files ONLY for module construction and re-exports, NOT for struct/class definitions.

‚úÖ **Correct mod.rs usage**:
```rust
// mod.rs should only contain:
pub mod error;          // Import submodules
pub mod data_source;
pub mod reader;

// Re-export types for convenience
pub use error::DataSourceError;
pub use data_source::KafkaDataSource;
```

‚ùå **Incorrect mod.rs usage**:
```rust
// DO NOT define structs/classes in mod.rs
pub struct MyStruct { /* ... */ }
impl MyStruct { /* ... */ }
```

**Best Practice**: Create dedicated files for each major struct/class and import them in mod.rs.

### Type System Architecture
```rust
// Unified FieldValue Types (for both execution and serialization)
FieldValue::ScaledInteger(i64, u8)  // 42x faster than f64, financial precision
FieldValue::Float(f64)              // Standard floating point
FieldValue::Integer(i64)            // Standard integer
FieldValue::String(String)          // Text data
FieldValue::Boolean(bool)           // Boolean values
FieldValue::Timestamp(NaiveDateTime) // Date/time values
```

### Serialization Patterns
```rust
// ScaledInteger serialization for compatibility:
// JSON: "123.4567" (decimal string)
// Avro: "123.4567" (string field)  
// Protobuf: Decimal{units: 1234567, scale: 4} (structured)
```

## Critical Implementation Details

### Financial Arithmetic
- **Internal Representation**: Scaled integers (e.g., $123.45 stored as 123450 with scale=3)
- **Arithmetic Operations**: Direct integer operations preserve exact precision
- **Display Formatting**: Converts back to decimal representation with trailing zero removal
- **Type Coercion**: Automatic scaling alignment for operations between different scales

### Pattern Matching Requirements
When adding new FieldValue variants, ensure all pattern matches are updated:
- `src/velostream/sql/execution/types.rs` - Core type operations
- `src/velostream/sql/execution/aggregation/` - Aggregation functions
- `src/velostream/serialization/mod.rs` - Serialization conversion
- Binary files: `src/bin/*.rs` - Server implementations

### Serialization Compatibility Strategy
- **JSON**: Decimal strings for universal parsing (`"123.4567"`)
- **Avro**: String fields with decimal logical type support
- **Protobuf**: Structured Decimal message with units/scale fields (industry standard)

## Common Tasks

### Adding New SQL Functions
1. Update `src/velostream/sql/execution/expression/functions.rs`
2. Add pattern matches for all FieldValue variants
3. Implement arithmetic preserving ScaledInteger precision
4. Add tests in `tests/unit/sql/functions/`

### Adding New Aggregation Functions
1. Update `src/velostream/sql/execution/aggregation/accumulator.rs`
2. Handle ScaledInteger accumulation with proper scaling
3. Add tests in `tests/unit/sql/execution/aggregation/`

### Adding New Serialization Support
1. Implement conversion functions in `src/velostream/serialization/mod.rs`
2. Handle ScaledInteger ‚Üí compatible format mapping
3. Add comprehensive tests in `tests/unit/serialization/`

### Adding New Configuration Features
1. Update appropriate module in `src/velostream/sql/config/`
2. Ensure public visibility for methods used in tests
3. Add comprehensive tests in `tests/unit/sql/config/`
4. Update imports in test files as needed

## Testing Strategy

### Test Organization
**CRITICAL**: Tests MUST be organized into dedicated test files outside of implementation modules:
- **NEVER add `#[cfg(test)]` blocks inside implementation files** - All tests must be in `tests/` directory
- **NEVER add `#[test]` functions inside src files** - They belong in `tests/unit/` or `tests/integration/`
- **Dedicated test files**: Tests are located in `tests/unit/` and `tests/integration/`
- **Test module structure**: Mirrors source structure for easy navigation

```
tests/
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ sql/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/                    # Configuration system tests
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ builder_test.rs        # Builder pattern tests
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ connection_string_test.rs  # URI parsing tests
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validation_test.rs     # Validation system tests
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ environment_test.rs    # Environment config tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ execution/                 # SQL execution tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ functions/                 # SQL function tests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ parser/                    # SQL parser tests
‚îÇ   ‚îî‚îÄ‚îÄ kafka/                         # Kafka integration tests
‚îî‚îÄ‚îÄ integration/                       # End-to-end tests
```

### Writing Tests - CORRECT Structure
When adding new functionality:
1. **Create test file** in `tests/unit/[module_path]/[feature]_test.rs`
2. **‚ö†Ô∏è CRITICAL: Register test in mod.rs** - Add `pub mod [feature]_test;` to the parent `mod.rs`
3. **Use proper imports** at the top of the test file
4. **Write comprehensive test cases** covering:
   - Happy path scenarios
   - Error conditions
   - Edge cases
   - Performance considerations (if applicable)

**‚ö†Ô∏è CRITICAL: Always Register New Test Files in mod.rs**

When you create a new test file, you MUST add it to the parent module's `mod.rs` file, otherwise Cargo won't discover or run your tests!

Example workflow:
```bash
# 1. Create new test file
touch tests/unit/table/new_feature_test.rs

# 2. IMMEDIATELY add to mod.rs (DO NOT SKIP THIS!)
echo "pub mod new_feature_test;" >> tests/unit/table/mod.rs

# 3. Verify test is discovered
cargo test new_feature_test --no-default-features -- --list
```

Example test file structure:
```rust
// tests/unit/sql/query_analyzer_test.rs
use velostream::velostream::sql::{
    query_analyzer::{QueryAnalyzer, QueryAnalysis},
    ast::{StreamingQuery, StreamSource, SelectField},
    SqlError,
};
use std::collections::HashMap;

#[test]
fn test_query_analyzer_select() {
    // Test implementation
}

#[test]
fn test_query_analyzer_error_handling() {
    // Test implementation
}
```

Example mod.rs registration:
```rust
// tests/unit/sql/mod.rs
pub mod query_analyzer_test;  // ‚Üê ADD THIS LINE for each new test file
pub mod parser_test;
pub mod execution_test;
```

### Unit Tests
- **Type Operations**: All arithmetic, casting, formatting
- **SQL Functions**: Builtin functions with all type combinations  
- **Aggregation**: Window functions, GROUP BY, HAVING clauses
- **Serialization**: Round-trip compatibility tests
- **Configuration**: URI parsing, validation, environment config

### Integration Tests
- **End-to-End SQL**: Complete query processing
- **Performance**: Benchmark critical paths
- **Compatibility**: Cross-system serialization verification

### Performance Tests
- **Financial Benchmarks**: ScaledInteger vs f64 vs Decimal
- **Aggregation Performance**: Large dataset processing
- **Serialization Speed**: Format comparison benchmarks

## Debugging Tips

### Common Issues
1. **Pattern Match Exhaustiveness**: New FieldValue variants need matches everywhere
2. **Scale Alignment**: Different scales in ScaledInteger arithmetic
3. **Serialization Round-trips**: Ensure exact precision preservation
4. **Performance Regressions**: Monitor financial arithmetic benchmarks
5. **CI/CD Formatting Failures**: Always run `cargo fmt --all -- --check` before committing

### Critical Development Rule
**NEVER mark tasks as completed when code doesn't compile.** Always verify compilation and basic functionality before marking work as done. This is essential for maintaining code quality and avoiding wasted time.

### Useful Debug Commands
```bash
# Debug specific test with full output
RUST_BACKTRACE=1 cargo test test_name --no-default-features -- --nocapture

# Performance debugging
cargo test financial_precision_benchmark::performance_benchmarks -- --nocapture

# Check for missing pattern matches
cargo check
```

## Pre-Commit Checks

### üöÄ Quick Pre-Commit Runner

When asked to run pre-commit checks, execute this comprehensive verification sequence:

```bash
echo "üßπ Running Velostream pre-commit checks..."
echo "‚ö° Stage 1: Fast Feedback Checks"

echo "1Ô∏è‚É£ Checking code formatting..."
cargo fmt --all -- --check || {
    echo "‚ùå Formatting check failed. Run 'cargo fmt --all' to fix."
    exit 1
}
echo "‚úÖ Code formatting passed"

echo "2Ô∏è‚É£ Checking compilation..."
cargo check --all-targets --no-default-features || {
    echo "‚ùå Compilation failed."
    exit 1
}
echo "‚úÖ Compilation passed"

echo "3Ô∏è‚É£ Running clippy linting..."
cargo clippy --all-targets --no-default-features || {
    echo "‚ùå Clippy linting failed."
    exit 1
}
echo "‚úÖ Clippy linting passed"

echo "4Ô∏è‚É£ Verifying test registration..."
# Check if any *_test.rs files exist that aren't registered in mod.rs
UNREGISTERED=$(find tests/unit -name "*_test.rs" -type f | while read test_file; do
    test_name=$(basename "$test_file" .rs)
    mod_file=$(dirname "$test_file")/mod.rs
    if [ -f "$mod_file" ] && ! grep -q "pub mod $test_name" "$mod_file"; then
        echo "‚ö†Ô∏è  $test_file not registered in $mod_file"
    fi
done)
if [ -n "$UNREGISTERED" ]; then
    echo "‚ùå Found unregistered test files:"
    echo "$UNREGISTERED"
    echo "Run: echo 'pub mod <test_name>;' >> <mod_file>"
    exit 1
fi
echo "‚úÖ All test files registered"

echo "5Ô∏è‚É£ Running unit tests..."
cargo test --lib --no-default-features --quiet || {
    echo "‚ùå Unit tests failed."
    exit 1
}
echo "‚úÖ Unit tests passed"

echo "üîÑ Stage 2: Comprehensive Validation"

echo "6Ô∏è‚É£ Testing example compilation..."
cargo build --examples --no-default-features || {
    echo "‚ùå Example compilation failed."
    exit 1
}
echo "‚úÖ Examples compiled successfully"

echo "7Ô∏è‚É£ Testing binary compilation..."
cargo build --bins --no-default-features || {
    echo "‚ùå Binary compilation failed."
    exit 1
}
echo "‚úÖ Binaries compiled successfully"

echo "8Ô∏è‚É£ Running comprehensive test suite..."
cargo test --tests --no-default-features --quiet -- --skip integration:: --skip performance:: || {
    echo "‚ùå Comprehensive tests failed."
    exit 1
}
echo "‚úÖ Comprehensive tests passed"

echo "9Ô∏è‚É£ Running documentation tests..."
cargo test --doc --no-default-features --quiet || {
    echo "‚ùå Documentation tests failed."
    exit 1
}
echo "‚úÖ Documentation tests passed"

echo ""
echo "üéâ ALL PRE-COMMIT CHECKS PASSED!"
echo "‚úÖ Code is ready for commit and push"
echo "üìä Summary:"
echo "   ‚Ä¢ Code formatting: ‚úÖ"
echo "   ‚Ä¢ Compilation: ‚úÖ"
echo "   ‚Ä¢ Clippy linting: ‚úÖ"
echo "   ‚Ä¢ Test registration: ‚úÖ"
echo "   ‚Ä¢ Unit tests: ‚úÖ"
echo "   ‚Ä¢ Examples: ‚úÖ"
echo "   ‚Ä¢ Binaries: ‚úÖ"
echo "   ‚Ä¢ Comprehensive tests: ‚úÖ"
echo "   ‚Ä¢ Documentation tests: ‚úÖ"
```

### üîß Individual Check Commands

#### Code Formatting (CRITICAL - GitHub Actions requirement)
```bash
# Check formatting
cargo fmt --all -- --check

# Fix formatting if needed
cargo fmt --all
```

#### Compilation and Linting
```bash
# Check compilation
cargo check --all-targets --no-default-features

# Run clippy (with strict warnings)
cargo clippy --all-targets --no-default-features -- -D warnings
```

#### Testing
```bash
# Unit tests only
cargo test --lib --no-default-features

# Comprehensive tests (excludes performance/integration)
cargo test --tests --no-default-features -- --skip integration:: --skip performance::

# Documentation tests
cargo test --doc --no-default-features
```

#### Build Verification
```bash
# Examples
cargo build --examples --no-default-features

# Binaries
cargo build --bins --no-default-features
```

### üìã Pre-Commit Checklist

Before every commit, ensure:
- [ ] **Code formatting** passes (`cargo fmt --all -- --check`)
- [ ] **Compilation** succeeds (`cargo check --all-targets --no-default-features`)
- [ ] **Clippy linting** passes (`cargo clippy --all-targets --no-default-features`)
- [ ] **New test files registered** in `mod.rs` (verify with `cargo test --list`)
- [ ] **Unit tests** pass (`cargo test --lib --no-default-features`)
- [ ] **Examples compile** (`cargo build --examples --no-default-features`)
- [ ] **Binaries compile** (`cargo build --bins --no-default-features`)
- [ ] **Comprehensive tests** pass (excluding performance/integration)
- [ ] **Documentation tests** pass (`cargo test --doc --no-default-features`)

### üéØ One-Line Complete Check

For quick verification, run this single command:
```bash
cargo fmt --all -- --check && cargo check && cargo test --no-default-features && cargo build --examples --no-default-features && cargo build --bins --no-default-features
```

### üö® Critical Rules

1. **NEVER commit when code doesn't compile**
2. **ALWAYS run formatting check** - GitHub Actions will fail without it
3. **Run comprehensive checks** before pushing to prevent CI failures
4. **Fix all clippy warnings** for code quality
5. **Ensure all tests pass** before marking work complete

### üîÑ CI/CD Pipeline Match

These checks mirror the GitHub Actions pipeline:
- **Stage 1**: Fast feedback (formatting, compilation, clippy, unit tests)
- **Stage 2**: Comprehensive validation (full test suite, examples, binaries)

Running these locally ensures CI/CD success and maintains code quality standards.

### Useful Debug Commands
```bash
# Debug specific test with full output

## Architecture Principles

### Performance First
- **Zero-Copy Where Possible**: Minimize allocations in hot paths
- **Integer Arithmetic**: ScaledInteger for financial calculations
- **Efficient Serialization**: Direct binary formats over text when possible

### Precision Over Speed (for Financial Data)
- **Exact Arithmetic**: Never compromise precision for performance
- **Deterministic Results**: Same inputs always produce identical outputs
- **Regulatory Compliance**: Meet financial industry precision requirements

### Compatibility
- **Standard Formats**: Use industry-standard serialization patterns
- **Cross-Language**: Ensure other systems can consume data
- **Schema Evolution**: Support backward-compatible changes

## Current Status

‚úÖ **Completed**: Enhanced SerializationError system with comprehensive error chaining
‚úÖ **Completed**: Financial precision implementation with 42x performance improvement
‚úÖ **Completed**: Cross-compatible JSON/Avro/Protobuf serialization with enhanced errors
‚úÖ **Completed**: Comprehensive test coverage (255+ tests passing)
‚úÖ **Completed**: All demos, examples, and doctests verified compliant
‚úÖ **Completed**: High-performance Protobuf implementation with Decimal message
‚úÖ **Completed**: Performance test compilation issues resolved (Phase 3 benchmarks)
‚úÖ **Completed**: Complete pre-commit validation pipeline passing
‚úÖ **Completed**: Type system conflicts resolved (WatermarkStrategy, CircuitBreakerConfig)
‚úÖ **Completed**: Circuit breaker pattern fixes with proper closure handling
‚úÖ **Completed**: Production-ready CI/CD compliance validation

### Latest Achievement: Performance Test Infrastructure Completion

**Problem Solved**: Critical compilation failures in performance testing infrastructure
- Fixed complex type conflicts between `config::WatermarkStrategy` and `watermarks::WatermarkStrategy`
- Resolved CircuitBreakerConfig field mismatches and missing properties
- Fixed Rust closure borrowing issues in circuit breaker patterns
- Updated SystemTime to DateTime<Utc> conversions for proper StreamRecord compatibility

**Technical Implementation**:
- **Type System Fixes**: Proper module imports with aliases for conflicting types
- **Circuit Breaker Enhancement**: Added missing fields (failure_rate_window, min_calls_in_window, failure_rate_threshold)
- **Closure Pattern Fixes**: Used `move` keyword and variable extraction to resolve borrowing conflicts
- **Stream Processing**: Fixed enum variant usage and struct field access patterns

**Key Benefits**:
```rust
// Fixed type conflicts with proper module imports
use config::{WatermarkStrategy as ConfigWatermarkStrategy};
use watermarks::{WatermarkStrategy, WatermarkManager};

// Enhanced circuit breaker configuration
CircuitBreakerConfig {
    failure_threshold: 5,
    recovery_timeout: Duration::from_secs(60),
    failure_rate_window: Duration::from_secs(60),    // Added
    min_calls_in_window: 10,                         // Added
    failure_rate_threshold: 50.0,                    // Added
}

// Fixed closure borrowing patterns
let has_field = record.fields.get("id").is_some();
let result = circuit_breaker.execute(move || {
    let _processing = has_field;  // No borrowing conflict
    Ok(())
}).await;
```

### Latest Achievement: Reserved Keyword Fixes for Common Field Names

**Problem Solved**: Reserved keywords conflicting with common field names in data streams
- Fixed `STATUS`, `METRICS`, and `PROPERTIES` being globally reserved, preventing their use as field names
- Enhanced `OptimizedTableImpl` to support SUM aggregation functions alongside existing COUNT support
- Updated test expectations to reflect current parser capabilities (BETWEEN now supported)

**Technical Implementation**:
- **Contextual Keywords**: Converted global reserved keywords to contextual-only parsing
- **Enhanced Aggregation**: Added SUM support to `sql_scalar` method with proper type handling
- **Parser Compatibility**: Updated SQL parser to allow common field names while preserving command functionality

**Key Benefits**:
```sql
-- Now works perfectly! ‚úÖ (Previously failed due to reserved keywords)
SELECT
    order_id,
    status,              -- No longer globally reserved
    metrics,             -- No longer globally reserved
    properties,          -- No longer globally reserved
    COUNT(*) OVER (PARTITION BY status) as status_count,
    SUM(metrics) as total_metrics  -- SUM now fully supported
FROM data_stream
WHERE status = 'active'
  AND metrics > 100
  AND properties IS NOT NULL;

-- Command functionality preserved ‚úÖ
SHOW STATUS;           -- Still works via contextual parsing
SHOW METRICS;          -- Still works via contextual parsing
SHOW PROPERTIES;       -- Still works via contextual parsing
```

**Reserved Keywords Fixed**:
- **`STATUS`**: Most common status field (order status, job status, system status)
- **`METRICS`**: Performance metrics, business metrics, system metrics
- **`PROPERTIES`**: Configuration properties, object properties, metadata

**Parser Improvements**: BETWEEN operator now fully supported, enhancing SQL standard compliance.

The codebase is now **production-ready** for financial analytics use cases requiring exact precision and high performance. All performance testing infrastructure is operational and validated for continuous integration.
- Always run clippy checks#