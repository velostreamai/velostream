# FR-082: Job Server V2 Unified Schedule & Roadmap

**Project**: Per-Partition Streaming Engine Architecture
**Last Updated**: November 9, 2025
**Status**: ‚úÖ **PHASE 6.4 COMPLETE** - Per-partition engine optimization delivered ~2x throughput improvement
**Achievement**: **Comprehensive multi-partition optimization** with 694K rec/sec (Scenario 0)

---

## üìä Progress Status Table

| Phase | Name | Status | Effort | Result | Documentation |
|-------|------|--------|--------|--------|-----------------|
| **Phases 0-5** | Architecture & Baseline | ‚úÖ COMPLETE | (Done) | V2 foundation | Various |
| **Phase 6.0-6.2** | V2 Architecture Foundation | ‚úÖ COMPLETE | (Done) | Per-partition execution | Various |
| **Phase 6.3** | Comprehensive Benchmarking | ‚úÖ COMPLETE | (Done) | All 5 scenarios measured | FR-082-PHASE6-3-COMPLETE-BENCHMARKS.md |
| **Phase 6.4** | Per-Partition Engine Migration | ‚úÖ COMPLETE | **S** | ~2x improvement | FR-082-PHASE6-4-EXECUTIVE-SUMMARY.md |
| **Phase 6.4C** | Eliminate State Duplication | üìã PLANNED | **M** | +5-10% improvement | FR-082-STATE-SHARING-ANALYSIS.md |
| **Phase 6.5** | Window State Optimization | üìã PLANNED | **M** | +5-15% improvement | - |
| **Phase 6.6** | Zero-Copy Record Processing | üìã PLANNED | **L** | +10-20% improvement | - |
| **Phase 6.7** | Lock-Free Metrics Optimization | üìã PLANNED | **S** | +1-5% improvement | - |
| **Phase 7** | Vectorization & SIMD | üìã FUTURE | **XL** | 2-3M rec/sec | - |
| **Phase 8** | Distributed Processing | üìã FUTURE | **XXL** | Multi-machine scaling | - |

### Key Metrics (Phase 6.4 Final - November 9, 2025)

**Scenario 0 (Pure SELECT)**:
- V1 Baseline: 23,584 rec/sec
- V2 Phase 6.3: 353,210 rec/sec (+15x)
- V2 Phase 6.4: **693,838 rec/sec** (+1.96x Phase 6.3) ‚úÖ

**Scenario 2 (GROUP BY)**:
- V1 Baseline: 23,355 rec/sec
- V2 Phase 6.3: 290,322 rec/sec (+12.4x)
- V2 Phase 6.4: **570,934 rec/sec** (+1.97x Phase 6.3) ‚úÖ

**Scenario 3a (Tumbling + GROUP BY)**:
- SQL Engine: 441,306 rec/sec
- V2 Phase 6.4: **1,041,883 rec/sec** (+2.36x vs SQL!) ‚úÖ

**Scenario 3b (EMIT CHANGES)**:
- SQL Engine: 487 rec/sec
- Job Server: **2,277 rec/sec** (+4.7x vs SQL!) ‚úÖ

---

## Phase 6.4: ‚úÖ COMPLETE - Per-Partition Engine Architecture Optimization

**Status**: ‚úÖ COMPLETE (November 9, 2025)
**Impact**: ~2x throughput improvement across all scenarios
**Effort**: **S** (Small - straightforward ownership transfer)

### What Was Done

**Problem Identified**:
- Shared `Arc<RwLock<StreamExecutionEngine>>` forced all partitions to contend for locks
- Mandatory state clones could not cross lock boundaries
- Write lock serialized updates across partitions
- ~1-2ms overhead per batch round with 8 partitions

**Solution Implemented**:
- Each partition now has its **OWN StreamExecutionEngine** (not shared)
- Eliminated RwLock wrapper entirely for owned engines
- Direct owned access to state without lock guards
- True single-threaded per-partition execution

**Code Changes**:
- `src/velostream/server/v2/coordinator.rs:881-883` - Per-partition engine creation
- `src/velostream/server/v2/coordinator.rs:977` - Owned mutable engine in partition_pipeline
- `src/velostream/server/v2/coordinator.rs:1101` - Direct engine access in execute_batch_for_partition
- `src/velostream/server/v2/coordinator.rs:1114` - State access without locks
- `src/velostream/server/v2/coordinator.rs:1140-1141` - Direct state updates without write lock

**Testing**:
- ‚úÖ All 530 unit tests pass (no behavioral changes)
- ‚úÖ All 5 benchmark scenarios complete with improved metrics
- ‚úÖ Code compiles without errors
- ‚úÖ All pre-commit checks pass

### Documentation

- **FR-082-PHASE6-4-PER-PARTITION-ENGINE-MIGRATION.md** - Detailed implementation guide
- **FR-082-PHASE6-4-EXECUTIVE-SUMMARY.md** - Results and performance analysis
- **FR-082-PHASE6-4-V2-LOCK-CONTENTION-ANALYSIS.md** - Problem analysis that informed the solution

---

## Phase 6.4C: Eliminate State Duplication (PLANNED)

**Status**: üìã PLANNED
**Effort**: **M** (Medium - 2-3 days)
**Expected Improvement**: +5-10% additional throughput
**Target**: ~730-750K rec/sec (Scenario 0)

### The Problem Solved

Currently, state lives in **TWO places**:
- **StreamExecutionEngine** holds `group_states` (master copy)
- **ProcessorContext** holds `group_by_states` (working copy)
- **Manual synchronization** copies state back and forth (~20 locations)

This violates the distributed streaming principle: **state should attach to the processing unit (partition), not the global engine**.

### Solution: ProcessorContext as Single Source of Truth

**Change**: Move state from StreamExecutionEngine to PartitionStateManager
- StreamExecutionEngine becomes stateless (just executes)
- PartitionStateManager owns state (lives with partition)
- ProcessorContext accesses partition state (no copying)
- Processors continue to use context (no changes needed)

### Implementation Path

1. **Remove state from StreamExecutionEngine** (4 hours)
   - Delete `group_states` field
   - Delete `get_group_states()` and `set_group_states()` methods
   - Remove all synchronization code (~20 locations)

2. **Add state to PartitionStateManager** (8 hours)
   - Add `group_by_states: Arc<Mutex<HashMap<...>>>`
   - Initialize state on first record
   - Persist across records
   - Update ProcessorContext initialization

3. **Testing & Validation** (4 hours)
   - Run comprehensive baseline test
   - Verify state persistence
   - Check performance improvement

### Expected Benefits

- ‚úÖ **Single source of truth** (no duplication)
- ‚úÖ **Distributed-ready** (state attached to partition)
- ‚úÖ **Performance** (5-10% from eliminating HashMap clones)
- ‚úÖ **Cleaner code** (remove 20+ synchronization lines)
- ‚úÖ **Better testability** (easier to mock state)

### Dependency: MUST BE DONE BEFORE Phase 6.5

‚ö†Ô∏è **CRITICAL**: Phase 6.4C enables Phase 6.5 optimization.

**Why**: Phase 6.5 also needs to move window state from engine to partition manager.
- **6.4C** removes `group_states` from engine
- **6.5** can then remove `window_v2_states` from engine
- Same pattern: engine becomes stateless, partitions own all state
- If 6.5 done first: would move window state, then 6.4C moves group state (messy)
- If 6.4C done first: clears the path for 6.5 (clean architecture)

### Documentation

See **FR-082-STATE-SHARING-ANALYSIS.md** for detailed analysis, architectural comparison with Flink/Kafka Streams, and complete implementation plan.

---

## Phase 6.5: Window State Optimization (PLANNED)

**Status**: üìã PLANNED (after 6.4C)
**Effort**: **M** (Medium - 2-3 days, REDUCED because 6.4C already refactored state handling)
**Expected Improvement**: +5-15% additional throughput
**Target**: ~850K-900K rec/sec (Scenario 0)

### Dependency on Phase 6.4C

Phase 6.4C completes the pattern for moving state from engine to partitions:
- 6.4C moves `group_states` to PartitionStateManager
- 6.5 applies same pattern to `window_v2_states`
- No need to refactor the architecture twice - just extend it

### Optimization Opportunities

1. **Per-Partition Window Managers**
   - Move window state from StreamExecutionEngine to PartitionStateManager
   - Each partition manages its own windows independently
   - Eliminates window state synchronization overhead (same pattern as 6.4C)

2. **Lazy Window Initialization**
   - Don't pre-allocate window state
   - Initialize only when first record arrives for that window
   - Reduces memory footprint and initialization time

3. **Memory Pooling for Window Accumulators**
   - Reuse window accumulator objects across batches
   - Avoid allocation/deallocation overhead
   - Particularly effective for high-cardinality windows

### Implementation Path

1. Extract window management from StreamExecutionEngine
2. Create WindowManager trait per partition
3. Implement lazy initialization pattern
4. Add memory pooling for common window types
5. Benchmark and validate improvements

---

## Phase 6.6: Zero-Copy Record Processing (PLANNED)

**Status**: üìã PLANNED
**Effort**: **L** (Large - 1-2 weeks)
**Expected Improvement**: +10-20% additional throughput
**Target**: ~900K-1.0M rec/sec (Scenario 0)

### Optimization Opportunities

1. **Record Streaming Instead of Batching**
   - Process records as they arrive instead of batch-at-a-time
   - Reduce latency and memory buffering
   - Improve cache locality

2. **Direct Transformation Pipelines**
   - Chain transformations without intermediate allocations
   - Iterator-style processing
   - Lazy evaluation where possible

3. **Reduce Allocations in Hot Path**
   - Output record allocation patterns
   - Context object pooling
   - Arc<StreamRecord> optimization

### Implementation Path

1. Profile current allocation patterns
2. Implement streaming record processor
3. Refactor batch coordination layer
4. Add context pooling mechanism
5. Comprehensive testing and benchmarking

---

## Phase 6.7: Lock-Free Metrics Optimization (PLANNED)

**Status**: üìã PLANNED
**Effort**: **S** (Small - 1-2 days)
**Expected Improvement**: +1-5% additional throughput
**Target**: ~900K-1.05M rec/sec (Scenario 0)

### Optimization Opportunities

1. **Atomic Operations for All Metrics**
   - Currently using some atomics, can expand
   - Replace any remaining lock-based metrics
   - Use proper memory ordering

2. **Thread-Local Accumulation**
   - Accumulate metrics locally per partition
   - Publish to global metrics periodically
   - Reduces atomic contention

3. **Batch-Level Metric Updates**
   - Update metrics once per batch instead of per-record
   - Reduces atomic operation frequency
   - Still maintains accuracy

### Implementation Path

1. Audit current metrics collection
2. Convert remaining locks to atomics
3. Implement thread-local accumulation
4. Batch metric updates
5. Validate with metrics benchmarks

---

## Phase 7: Vectorization & SIMD (FUTURE)

**Status**: üìã FUTURE PLANNING
**Effort**: **XL** (Extra Large - 2-4 weeks)
**Expected Improvement**: +2-3x additional throughput
**Target**: **2-3M rec/sec** (multi-scenario average)

### Optimization Opportunities

1. **SIMD Aggregations**
   - Vectorize COUNT, SUM operations
   - Process multiple records in parallel with vector instructions
   - Particularly effective for GROUP BY

2. **Batch-Level Vectorization**
   - Process entire batches with vector operations
   - Predicate pushdown for filtering
   - Early termination for aggregations

3. **Memory Layout Optimization**
   - Column-oriented storage for batch processing
   - Cache-friendly data layout
   - Reduce memory bandwidth requirements

### Implementation Path

1. Profile hot aggregation paths
2. Identify vectorizable patterns
3. Implement SIMD versions of common operations
4. Benchmark and validate
5. Extend to additional scenarios

---

## Phase 8: Distributed Processing (FUTURE)

**Status**: üìã FUTURE PLANNING
**Effort**: **XXL** (Extra Extra Large - 4-8 weeks)
**Expected Improvement**: **Multi-machine scaling**
**Target**: **2-3M+ rec/sec** (distributed across multiple machines)

### Optimization Opportunities

1. **Multi-Machine Partitioning**
   - Distribute partitions across network-connected machines
   - Reduce single-machine resource limits
   - Enable horizontal scaling

2. **Efficient State Synchronization**
   - Distributed window state management
   - Efficient GROUP BY key distribution
   - Minimal network overhead

3. **Fault Tolerance**
   - Checkpoint mechanisms for state recovery
   - Replication for high availability
   - Stream rebalancing on failures

### Implementation Path

1. Design distributed architecture
2. Implement network communication layer
3. Add checkpointing mechanism
4. Implement replication protocol
5. Comprehensive testing and validation

---

## Summary: Current State & Trajectory

### ‚úÖ Delivered (Phases 0-6.4)

- Per-partition independent execution
- Lock contention elimination
- ~2x Phase 6.3 ‚Üí 6.4 improvement
- 693K rec/sec throughput (Scenario 0)
- All 5 benchmark scenarios validated
- Production-ready code quality

### üìä Performance Trajectory

```
Phase 6.3:   353K rec/sec (pure SELECT, 4 cores)
Phase 6.4:   694K rec/sec (+1.96x)
Phase 6.4C:  ~730-750K rec/sec (+5-10%, target - state duplication elimination)
Phase 6.5:   ~850K rec/sec (+12-16%, target - window state optimization)
Phase 6.6:   ~1.0M rec/sec (+18%, target - zero-copy processing)
Phase 6.7:   ~1.05M rec/sec (+5%, target - lock-free metrics)
Phase 7:     ~2-3M rec/sec (+2-3x, target - vectorization & SIMD)
Phase 8:     2-3M+ rec/sec (distributed, target - multi-machine scaling)
```

### üéØ Architecture Principles

1. **Single-threaded per-partition** - No cross-partition contention
2. **Owned engine instances** - Direct access without synchronization
3. **Lock-free when possible** - Atomics for metrics and lightweight ops
4. **Minimal allocations** - Reference-based processing in hot paths
5. **Deterministic routing** - Hash-based partition assignment

---

## References to Phase 6.4 Documentation

**For Phase 6.4 details, see**:

1. **FR-082-PHASE6-4-EXECUTIVE-SUMMARY.md**
   - High-level overview of Phase 6.4
   - Performance results and metrics
   - Architectural comparison with alternatives

2. **FR-082-PHASE6-4-PER-PARTITION-ENGINE-MIGRATION.md**
   - Detailed implementation guide
   - Code changes and explanations
   - Before/after comparisons

3. **FR-082-PHASE6-4-V2-LOCK-CONTENTION-ANALYSIS.md**
   - Lock contention analysis
   - Bottleneck identification
   - Why the solution works

4. **FR-082-PHASE6-3-COMPLETE-BENCHMARKS.md**
   - Phase 6.3 baseline measurements
   - All 5 scenarios benchmarked
   - Comparison data for Phase 6.4

---

**Last Updated**: November 9, 2025
**Next Review**: After Phase 6.5 completion
**Status**: On track - Phase 6.4 delivered ahead of schedule with exceptional results
