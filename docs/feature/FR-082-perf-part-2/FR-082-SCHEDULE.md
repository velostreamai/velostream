# FR-082: Job Server V2 Implementation Schedule

**Project**: Hash-Partitioned Pipeline Architecture for "Better than Flink" Performance
**Total Duration**: 8 weeks (2 weeks Phase 0 + 6 weeks V2 core)
**Start Date**: November 1, 2025
**Target Completion**: December 27, 2025
**Last Updated**: November 6, 2025

---

## ðŸ“Š Overall Progress

| Phase | Status | Duration | Start | End | Progress |
|-------|--------|----------|-------|-----|----------|
| Phase 0 | âœ… **COMPLETED** | 2 weeks | Nov 1 | Nov 15 | 100% |
| Phase 1 | âœ… **COMPLETED** | 1 week | Nov 16 | Nov 22 | 100% |
| Phase 2 | ðŸ“… Planned | 1 week | Nov 23 | Nov 29 | 0% |
| Phase 3 | ðŸ“… Planned | 1 week | Nov 30 | Dec 6 | 0% |
| Phase 4 | ðŸ“… Planned | 1 week | Dec 7 | Dec 13 | 0% |
| Phase 5 | ðŸ“… Planned | 2 weeks | Dec 14 | Dec 27 | 0% |

**Overall Completion**: 38% (Phases 0-1 complete)

---

## Phase 0: SQL Engine Optimization (PREREQUISITE)

**Status**: âœ… **COMPLETED**
**Duration**: 2 weeks (November 1-15, 2025)
**Goal**: Achieve 200K rec/sec baseline for GROUP BY queries (56x improvement from 3.58K rec/sec)

### Week 1: Phase 4B - Hash Table Optimization
**Dates**: November 1-8, 2025
**Status**: âœ… **COMPLETED**

#### Tasks Completed
- âœ… Implemented `GroupKey` struct with pre-computed hash
- âœ… Replaced `HashMap` with `FxHashMap` (faster hash function)
- âœ… Added hash caching to avoid recomputation
- âœ… Benchmarked performance improvements

#### Results
- **Baseline**: 3.58K rec/sec (HashMap with runtime hashing)
- **After Phase 4B**: 15-20K rec/sec (FxHashMap + cached hash)
- **Improvement**: 4-6x speedup
- **Test**: `cargo test profile_pure_group_by_complex -- --nocapture`

#### Files Changed
- `src/velostream/sql/execution/aggregation/group_key.rs` - GroupKey implementation
- `tests/performance/phase4b_group_by_optimization.rs` - Benchmark tests

---

### Week 2: Phase 4C - Arc-based State Sharing
**Dates**: November 9-15, 2025
**Status**: âœ… **COMPLETED**

#### Tasks Completed
- âœ… Wrapped group states in `Arc<FxHashMap>` for zero-copy sharing
- âœ… Implemented copy-on-write merge pattern
- âœ… Added string interning for GROUP BY keys
- âœ… Optimized key caching strategy
- âœ… Comprehensive benchmarks

#### Results
- **Before**: 15-20K rec/sec (Phase 4B)
- **After Phase 4C**: 200K rec/sec (Arc-based zero-copy)
- **Improvement**: 10-13x speedup (56x total from baseline)
- **Test**: `cargo test comprehensive_sql_benchmarks -- --nocapture`

#### Files Changed
- `src/velostream/sql/execution/aggregation/group_state.rs` - Arc wrapper
- `src/velostream/sql/execution/aggregation/string_cache.rs` - String interning
- `tests/performance/phase4c_arc_optimization.rs` - Benchmark tests

---

### Phase 0 Summary

**ðŸŽ‰ MILESTONE ACHIEVED**

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Throughput** | 3.58K rec/sec | 200K rec/sec | **56x faster** |
| **Hash Operations** | Runtime | Pre-computed | Eliminated overhead |
| **State Cloning** | Full clone | Arc (zero-copy) | **92% reduction** |
| **Memory Efficiency** | High duplication | Shared state | 10x less allocation |

**Unblocks**: Phase 1-5 implementation (V2 requires 200K rec/sec per-partition baseline)

---

## Phase 1: Hash Routing Foundation

**Status**: âœ… **COMPLETED** (Week 3)
**Duration**: 1 week (November 16-22, 2025)
**Goal**: Implement hash-based routing and partition state management

### Week 3: Hash Router + Partition Manager
**Dates**: November 16-22, 2025
**Status**: âœ… **COMPLETED**

#### Tasks Completed
- âœ… Implemented `HashRouter` with GROUP BY key extraction
- âœ… Implemented `PartitionStateManager` with lock-free state foundation
- âœ… Added `PartitionMetrics` for throughput/latency/backpressure monitoring
- âœ… Created comprehensive unit tests (23 tests passing)
- âœ… Created Phase 1 benchmark suite (5 benchmark tests)
- âœ… CPU core affinity placeholder (deferred to Phase 3)
- ðŸ”„ Router task for record distribution (deferred to Phase 2)

#### Actual Results
- **Foundation**: Hash routing infrastructure validated
- **Unit Tests**: 23/23 tests passing (HashRouter, PartitionMetrics, PartitionStateManager)
- **Benchmarks**: 5 comprehensive tests created (determinism, metrics, throughput, batch, end-to-end)
- **Test Files**:
  - Library tests: `src/velostream/server/v2/*.rs` (inline unit tests)
  - Benchmarks: `tests/performance/fr082_phase1_partitioned_routing_benchmark.rs`

#### Key Components
```rust
// HashRouter: Extracts GROUP BY key and determines partition
pub struct HashRouter {
    num_partitions: usize,
    partition_strategy: PartitionStrategy,
}

// PartitionStateManager: Manages state for single partition
pub struct PartitionStateManager {
    partition_id: usize,
    query_states: HashMap<String, QueryState>,
    metrics: PartitionMetrics,
}
```

#### Files Changed
- `src/velostream/server/v2/mod.rs` - Module hub and re-exports
- `src/velostream/server/v2/hash_router.rs` - Hash-based record routing
- `src/velostream/server/v2/partition_manager.rs` - Per-partition state manager
- `src/velostream/server/v2/metrics.rs` - Partition performance metrics
- `src/velostream/server/mod.rs` - Added v2 module
- `tests/performance/fr082_phase1_partitioned_routing_benchmark.rs` - Benchmark suite
- `tests/performance/mod.rs` - Registered Phase 1 benchmarks

#### Acceptance Criteria
- âœ… Same GROUP BY key always routes to same partition (deterministic) - Verified in tests
- âœ… HashRouter integrates with GroupByStateManager's GROUP BY key extraction
- âœ… Partition metrics track throughput, queue depth, and latency
- âœ… All unit tests passing (23/23)
- âœ… Benchmark suite created and compiling
- ðŸ”„ Performance target (400K rec/sec) - Deferred to Phase 2 with full SQL integration

---

### Phase 1 Summary

**ðŸŽ‰ FOUNDATION ESTABLISHED**

Phase 1 successfully established the core infrastructure for hash-partitioned query execution:

| Component | Status | Details |
|-----------|--------|---------|
| **HashRouter** | âœ… Complete | Deterministic GROUP BY key-based routing |
| **PartitionStateManager** | âœ… Complete | Lock-free foundation with metrics integration |
| **PartitionMetrics** | âœ… Complete | Thread-safe throughput/latency/backpressure tracking |
| **Unit Tests** | âœ… 23/23 passing | Full coverage of routing, metrics, state manager |
| **Benchmarks** | âœ… 5 tests created | Determinism, metrics, throughput, batch, E2E |

**Key Achievements:**
- Reuses existing `GroupByStateManager::generate_group_key()` for consistency
- Uses pre-computed hash from `GroupKey` for fast routing
- Atomic counters in metrics for lock-free updates
- Comprehensive test suite validates foundation
- Ready for Phase 2 SQL engine integration

**Unblocks**: Phase 2 coordinator implementation and full SQL execution integration

---

## Phase 2: Partitioned Coordinator

**Status**: ðŸ“… **PLANNED** (Week 4)
**Duration**: 1 week (November 23-29, 2025)
**Goal**: Multi-partition orchestration and output merging

### Week 4: Job Coordinator + Multi-Partition Orchestration
**Dates**: November 23-29, 2025
**Status**: ðŸ“… **PLANNED**

#### Planned Tasks
- [ ] Implement `PartitionedJobCoordinator`
- [ ] Create router task with record distribution logic
- [ ] Implement output merger (if ordering required)
- [ ] Add partition lifecycle management (start/stop/restart)
- [ ] Handle partition failures and recovery

#### Expected Results
- **Target**: 400K â†’ 800K rec/sec (4 partitions)
- **Scaling Efficiency**: 85-90% on 4 cores
- **Test**: `tests/unit/server/v2/coordinator_test.rs`

#### Key Components
```rust
pub struct PartitionedJobCoordinator {
    partitions: Vec<PartitionStateManager>,
    router: HashRouter,
    output_merger: Option<OutputMerger>,
}
```

#### Acceptance Criteria
- âœ… 4 partitions achieve 800K rec/sec
- âœ… Output records maintain correct ordering (if required)
- âœ… Partition failures don't crash entire job
- âœ… Coordinator can restart failed partitions

---

## Phase 3: Backpressure & Observability

**Status**: ðŸ“… **PLANNED** (Week 5)
**Duration**: 1 week (November 30 - December 6, 2025)
**Goal**: Production-grade monitoring and flow control

### Week 5: Per-Partition Backpressure + Prometheus Metrics
**Dates**: November 30 - December 6, 2025
**Status**: ðŸ“… **PLANNED**

#### Planned Tasks
- [ ] Implement `PartitionMetrics` with atomic counters
- [ ] Add backpressure detection per partition
- [ ] Implement automatic throttling
- [ ] Expose Prometheus metrics endpoint
- [ ] Create Grafana dashboard template

#### Expected Results
- **Target**: Maintain 800K rec/sec under load
- **Backpressure Threshold**: Detect within 100ms
- **Test**: `tests/integration/v2/backpressure_test.rs`

#### Metrics to Track
- Records processed per second (per partition)
- State size (per partition)
- Backpressure events
- Processing latency (p50, p95, p99)
- Memory usage per partition

#### Acceptance Criteria
- âœ… System remains stable at 800K rec/sec for 10+ minutes
- âœ… Backpressure prevents OOM errors
- âœ… Grafana dashboard shows real-time metrics
- âœ… Slow partitions don't block fast ones

---

## Phase 4: System Fields & Watermarks

**Status**: ðŸ“… **PLANNED** (Week 6)
**Duration**: 1 week (December 7-13, 2025)
**Goal**: Full system field support and watermark management

### Week 6: System Fields Integration + Watermark Management
**Dates**: December 7-13, 2025
**Status**: ðŸ“… **PLANNED**

#### Planned Tasks
- [ ] Update `HashRouter` to handle system fields in GROUP BY
- [ ] Implement per-partition watermark managers
- [ ] Add `_WINDOW_START` / `_WINDOW_END` field injection
- [ ] Implement watermark propagation across partitions
- [ ] Add late record detection and handling

#### Expected Results
- **Target**: Full system field support at 800K rec/sec
- **Watermark Lag**: <100ms across partitions
- **Test**: `tests/unit/server/v2/watermark_test.rs`

#### System Fields
- `_TIMESTAMP` - Processing time
- `_PARTITION` - Kafka partition
- `_OFFSET` - Kafka offset
- `_WINDOW_START` - Window start time
- `_WINDOW_END` - Window end time

#### Acceptance Criteria
- âœ… System fields accessible in GROUP BY and WHERE clauses
- âœ… Watermarks advance correctly across all partitions
- âœ… Late records handled according to configured strategy
- âœ… No performance degradation from system field access

---

## Phase 5: Advanced Features

**Status**: ðŸ“… **PLANNED** (Weeks 7-8)
**Duration**: 2 weeks (December 14-27, 2025)
**Goal**: Production-ready features (ROWS WINDOW, EMIT CHANGES, state management)

### Week 7: ROWS WINDOW + Late Record Handling
**Dates**: December 14-20, 2025
**Status**: ðŸ“… **PLANNED**

#### Planned Tasks
- [ ] Implement per-partition ROWS WINDOW buffers
- [ ] Add late record detection and strategy
- [ ] **âš ï¸ FIX EMIT CHANGES SUPPORT** (architectural limitation discovered Nov 6, 2025)
- [ ] Add window emission logic per partition
- [ ] Implement windowed aggregations (LEAD, LAG, RANK)

#### Critical Issue to Resolve: EMIT CHANGES

**Problem Discovered**: November 6, 2025
- Job Server's `QueryProcessor::process_query()` bypasses engine's `output_sender` channel
- EMIT CHANGES queries emit through channel â†’ 0 emissions in Job Server
- **Evidence**: Scenario 3b test shows 99,810 emissions from SQL Engine, 0 from Job Server

**Solution Options**:
1. **Option A**: Use `engine.execute_with_record()` for EMIT CHANGES queries
2. **Option B**: Actively drain `output_sender` channel in `process_batch_with_output()`
3. **Option C**: Refactor batch processing to support streaming emissions

**Reference**: `tests/performance/analysis/scenario_3b_tumbling_emit_changes_baseline.rs`

#### Expected Results
- **Target**: ROWS WINDOW support at 500K rec/sec
- **EMIT CHANGES**: Proper streaming emission (fix required)
- **Test**: `tests/unit/server/v2/rows_window_test.rs`

#### Acceptance Criteria
- âœ… ROWS WINDOW functions work correctly
- âœ… Late records handled without data loss
- âœ… **EMIT CHANGES emits results properly** (currently broken!)
- âœ… Window state cleaned up correctly

---

### Week 8: State TTL + Recovery
**Dates**: December 21-27, 2025
**Status**: ðŸ“… **PLANNED**

#### Planned Tasks
- [ ] Add per-partition state TTL cleanup
- [ ] Implement state snapshot/restore
- [ ] Add partition rebalancing (for dynamic scaling)
- [ ] Implement checkpoint/savepoint mechanism
- [ ] Add state size monitoring and alerting

#### Expected Results
- **Target**: Production-ready state management
- **State Cleanup**: Automatic TTL-based cleanup
- **Recovery**: <5 second recovery from checkpoint
- **Test**: `tests/integration/v2/state_management_test.rs`

#### Acceptance Criteria
- âœ… State cleaned up automatically based on TTL
- âœ… Checkpoints taken every N seconds (configurable)
- âœ… Recovery from checkpoint restores full state
- âœ… Partition rebalancing works without data loss

---

## ðŸŽ¯ Performance Targets Summary

| Metric | Phase 0 | Phase 1 | Phase 2 | Phase 3-5 | Target |
|--------|---------|---------|---------|-----------|--------|
| **Throughput** | 200K | 400K | 800K | 1.5M | 1.5M rec/sec |
| **Cores Used** | 1 | 2 | 4 | 8 | 8 |
| **Scaling Efficiency** | - | 95% | 90% | 85-90% | 85%+ |
| **Improvement over V1** | 8.5x | 17x | 34x | **65x** | **65x** |

**V1 Baseline**: 23K rec/sec (single-threaded Job Server)
**V2 Target**: 1.5M rec/sec (hash-partitioned on 8 cores)

---

## ðŸ“‹ Deliverables Checklist

### Core Implementation
- [x] Phase 0: SQL Engine optimization (200K rec/sec baseline)
- [ ] Phase 1: Hash routing + partition manager
- [ ] Phase 2: Partitioned coordinator
- [ ] Phase 3: Backpressure + observability
- [ ] Phase 4: System fields + watermarks
- [ ] Phase 5: ROWS WINDOW + state management

### Documentation
- [x] Architecture blueprint (PARTITIONED-PIPELINE.md)
- [x] Query partitionability analysis
- [x] Baseline measurements
- [x] Implementation schedule (this document)
- [ ] API documentation
- [ ] Deployment guide
- [ ] Operations runbook

### Testing
- [x] Phase 0 benchmarks
- [ ] Unit tests for all components
- [ ] Integration tests for multi-partition scenarios
- [ ] Performance regression tests
- [ ] Stress tests (10M+ records)

### Monitoring
- [ ] Prometheus metrics integration
- [ ] Grafana dashboard templates
- [ ] Alerting rules
- [ ] Performance profiling tools

---

## âš ï¸ Risks & Mitigation

### Risk 1: EMIT CHANGES Architectural Limitation
**Status**: ðŸ”´ **CRITICAL** - Discovered November 6, 2025
**Impact**: EMIT CHANGES queries don't work with Job Server
**Mitigation**: Fix in Phase 5 Week 7 (see options above)
**Owner**: TBD

### Risk 2: Scaling Efficiency Below 85%
**Status**: ðŸŸ¡ **MEDIUM**
**Impact**: May not achieve 1.5M rec/sec on 8 cores
**Mitigation**: Profile hot paths, optimize lock contention
**Owner**: TBD

### Risk 3: Partition Rebalancing Complexity
**Status**: ðŸŸ¡ **MEDIUM**
**Impact**: Dynamic scaling may cause data loss
**Mitigation**: Implement careful state transfer protocol
**Owner**: TBD

---

## ðŸš€ Next Actions

### Immediate (Week 3)
1. âœ… Create FR-082-SCHEDULE.md (this document)
2. âœ… Update QUERY-PARTITIONABILITY-ANALYSIS.md with EMIT CHANGES finding
3. ðŸ”§ Start Phase 1: Implement `HashRouter`
4. ðŸ”§ Implement `PartitionStateManager`
5. ðŸ”§ Write unit tests for hash routing

### Short Term (Week 4-5)
1. Complete Phase 2: Partitioned coordinator
2. Add backpressure and monitoring (Phase 3)
3. Validate 800K rec/sec on 4 cores

### Medium Term (Week 6-8)
1. Add system fields and watermarks (Phase 4)
2. **FIX EMIT CHANGES SUPPORT** (Phase 5)
3. Implement state management features
4. Final performance validation

---

## ðŸ“Š Success Criteria

**Phase 0 (COMPLETED)**: âœ…
- âœ… 200K rec/sec GROUP BY baseline achieved
- âœ… 56x improvement from 3.58K rec/sec
- âœ… Zero-copy state sharing implemented

**Phase 1 (Week 3)**:
- [ ] 400K rec/sec on 2 cores
- [ ] Deterministic hash routing
- [ ] CPU affinity implementation

**Phase 2 (Week 4)**:
- [ ] 800K rec/sec on 4 cores
- [ ] Multi-partition orchestration
- [ ] Output merging (if required)

**Phase 3-5 (Weeks 5-8)**:
- [ ] 1.5M rec/sec on 8 cores
- [ ] 85%+ scaling efficiency
- [ ] **EMIT CHANGES working correctly**
- [ ] Production-ready state management

**Final Success Metric**: **65x improvement over V1** (23K â†’ 1.5M rec/sec)

---

## ðŸ“š Related Documents

- **Architecture**: `FR-082-job-server-v2-PARTITIONED-PIPELINE.md`
- **Query Analysis**: `FR-082-QUERY-PARTITIONABILITY-ANALYSIS.md`
- **Baseline Data**: `FR-082-BASELINE-MEASUREMENTS.md`
- **Scenario Guide**: `FR-082-SCENARIO-CLARIFICATION.md`
- **Overhead Analysis**: `FR-082-overhead-analysis.md`

---

**Document Owner**: FR-082 Implementation Team
**Review Frequency**: Weekly
**Last Review**: November 6, 2025
**Next Review**: November 13, 2025 (end of Phase 1 Week 3)
