# Job Server V2: Clean-Slate Architecture Blueprint

## Executive Summary

**Current Performance**: 23K rec/sec (23x slower than pure SQL)
**Root Cause**: Arc<Mutex> + HashMap cloning overhead (204ms in 213ms total)
**Decision**: Should we fix incrementally (Option 5: 8x improvement) or redesign from scratch?

This document provides a **greenfield architecture** designed from requirements, then compares it to incremental fixes.

---

## Part 1: Requirements Analysis

### Functional Requirements (Extracted from Current Code)

#### 1. Data Source Abstraction
```rust
// From datasource/traits.rs
trait DataReader {
    async fn read() -> Vec<StreamRecord>;      // Batch reads
    async fn commit();                          // Offset management
    async fn seek(SourceOffset);                // Replay support
    async fn has_more() -> bool;                // End-of-stream detection
    async fn begin_transaction();               // Transactional support
    async fn commit_transaction();
    async fn rollback_transaction();
}
```

**Capabilities needed**:
- Kafka, File, S3, Database sources
- Batch size configuration
- Transaction support (optional)
- Offset tracking for replay
- Multiple concurrent sources

#### 2. Data Sink Abstraction
```rust
trait DataWriter {
    async fn write(StreamRecord);               // Single writes
    async fn write_batch(Vec<Arc<StreamRecord>>); // Batch writes
    async fn flush();                           // Buffer management
    async fn commit();                          // Sink commit
    async fn update(key, record);               // Upserts
    async fn delete(key);                       // Deletes
    async fn rollback();                        // Transaction rollback
}
```

**Capabilities needed**:
- Stdout, Kafka, File, Database sinks
- Batch write optimization
- Transaction support (optional)
- Multiple concurrent sinks

#### 3. Job Processing Configuration
```rust
struct JobProcessingConfig {
    max_batch_size: usize,
    batch_timeout: Duration,
    use_transactions: bool,
    failure_strategy: FailureStrategy,
    max_retries: u32,
    retry_backoff: Duration,
    log_progress: bool,
    progress_interval: u64,
}
```

**Capabilities needed**:
- Configurable batch sizes
- Timeout-based batch flushing
- Error handling strategies (LogAndContinue, FailBatch, RetryWithBackoff)
- Retry logic with exponential backoff
- Progress monitoring

#### 4. SQL Execution
```rust
trait QueryProcessor {
    fn process_query(
        query: &StreamingQuery,
        record: &StreamRecord,
        context: &mut ProcessorContext,
    ) -> Result<QueryResult>;
}

struct ProcessorContext {
    group_by_states: HashMap<GroupKey, AggregateState>,
    persistent_window_states: Vec<WindowState>,
    // ... other state
}
```

**Capabilities needed**:
- GROUP BY with aggregation
- WINDOW functions (Tumbling, Sliding, Session)
- JOIN operations
- WHERE/HAVING filtering
- State management across batches

#### 5. Observability & Metrics
```rust
// From processors/observability_helper.rs
- Batch span creation (distributed tracing)
- Deserialization metrics
- SQL processing metrics
- Serialization metrics
- Error tracking
- Throughput metrics
```

**Capabilities needed**:
- OpenTelemetry integration
- Prometheus metrics
- Custom SQL @metric annotations
- Counter, Gauge, Histogram metrics
- Distributed tracing context

#### 6. Multi-Source/Multi-Sink Processing
```rust
async fn process_multi_job(
    readers: HashMap<String, Box<dyn DataReader>>,
    writers: HashMap<String, Box<dyn DataWriter>>,
    engine: Arc<Mutex<StreamExecutionEngine>>,
    query: StreamingQuery,
) -> JobExecutionStats;
```

**Capabilities needed**:
- Multiple sources (JOINs, UNIONs)
- Multiple sinks (fanout)
- Coordinated batch processing
- Shutdown signaling

### Non-Functional Requirements

#### Performance Goals
- **Target**: 500K+ rec/sec (close to pure SQL's 790K)
- **Acceptable**: 200K+ rec/sec (8x improvement)
- **Current**: 23K rec/sec (unacceptable)

#### Concurrency Requirements
- Multiple jobs processing concurrently
- Multiple batches from same job in parallel
- Lock-free hot paths where possible

#### Reliability Requirements
- Exactly-once processing (with transactions)
- At-least-once processing (without transactions)
- Graceful error handling
- Retry logic for transient failures

#### Operational Requirements
- Progress monitoring
- Health checks
- Metrics export
- Dynamic configuration

---

## Part 2: Current Architecture (V1) Analysis

### Component Structure
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               SimpleJobProcessor                        â”‚
â”‚  (Coordinates everything, owns Arc<Mutex<Engine>>)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”œâ”€â–º DataReader (Kafka/File/S3)
                   â”‚
                   â”œâ”€â–º process_batch_with_output()
                   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   â”‚ LOCK #1: Clone state       â”‚ â† 80ms overhead
                   â”‚   â”‚ Process batch locally      â”‚
                   â”‚   â”‚ LOCK #2: Sync state back   â”‚ â† 80ms overhead
                   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”œâ”€â–º ObservabilityHelper (metrics) â† 30ms overhead
                   â”‚
                   â””â”€â–º DataWriter (Kafka/File/Stdout) â† 10ms overhead
```

### Critical Bottlenecks

#### 1. Centralized State with Coarse Locking
```rust
// PROBLEM: Single Arc<Mutex<Engine>> for ALL state
Arc<Mutex<StreamExecutionEngine>> {
    group_by_states: HashMap<GroupKey, AggregateState>,  // Cloned EVERY batch
    window_states: Vec<WindowState>,                     // Cloned EVERY batch
    // ... other state
}
```

**Cost per batch** (5 batches total):
- Lock acquisition: 2 Ã— 8ms = 16ms
- HashMap clone (200 keys): 16ms
- **Total per batch**: 32ms Ã— 5 = **160ms wasted**

#### 2. Observability Always On
```rust
// PROBLEM: Always pays observability cost, even when disabled
ObservabilityHelper::start_batch_span()      // ~5ms
ObservabilityHelper::record_deserialization() // ~3ms
ObservabilityHelper::record_sql_processing()  // ~5ms
ObservabilityHelper::inject_trace_context()   // ~2ms
// Total per batch: ~15ms Ã— 5 = 75ms (when not needed)
```

#### 3. Sequential Batch Processing
```rust
// PROBLEM: Loop processes batches sequentially
loop {
    let batch = reader.read().await;     // Wait for batch
    process_batch(...).await;            // Wait for processing
    writer.write_batch(...).await;       // Wait for write
}
// Concurrency opportunity missed!
```

---

## Part 3: Clean-Slate Architecture (V2)

### Design Principles

1. **Zero-Copy Where Possible**: Avoid cloning large structures
2. **Lock-Free Execution Paths**: Use message passing over shared memory
3. **Optional Observability**: Pay cost only when needed
4. **Pipeline Parallelism**: Overlap I/O and compute
5. **Stateful Stream Processing**: Match Flink/Kafka Streams patterns

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Job Coordinator                             â”‚
â”‚  (Lightweight orchestrator, no shared state)                    â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                               â”‚
    â”‚                               â”‚
    â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Source Pipeline  â”‚     â”‚    State Manager Actor     â”‚
â”‚   (async stream)  â”‚     â”‚   (single-threaded actor)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ DataReader      â”‚     â”‚ â€¢ GROUP BY states          â”‚
â”‚ â€¢ Batching        â”‚â”€â”€â”€â”€â–ºâ”‚ â€¢ Window states            â”‚
â”‚ â€¢ Backpressure    â”‚     â”‚ â€¢ Message-based updates    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                          â”‚
         â”‚                          â”‚
         â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Processing Pipelineâ”‚     â”‚   Sink Pipeline            â”‚
â”‚  (parallel workers)â”‚â”€â”€â”€â”€â”€â–º  (async stream)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ SQL execution    â”‚     â”‚ â€¢ DataWriter               â”‚
â”‚ â€¢ Local state      â”‚     â”‚ â€¢ Batching                 â”‚
â”‚ â€¢ Parallel batches â”‚     â”‚ â€¢ Async writes             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Optional Observability Layer        â”‚
â”‚  (enabled via feature flag/config)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

#### 1. State Manager Actor (Lock-Free)

**Purpose**: Single-threaded state owner, no locks needed

```rust
/// Actor that owns ALL query state (no locks!)
struct StateManagerActor {
    receiver: mpsc::UnboundedReceiver<StateMessage>,
    states: HashMap<String, QueryState>,  // query_id -> state
}

enum StateMessage {
    /// Update local batch state into global state
    MergeBatchState {
        query_id: String,
        batch_state: HashMap<GroupKey, AggregateState>,
        response: oneshot::Sender<()>,
    },

    /// Get current state snapshot (for recovery)
    GetSnapshot {
        query_id: String,
        response: oneshot::Sender<QueryState>,
    },

    /// Initialize query state
    Initialize {
        query_id: String,
        initial_state: QueryState,
    },
}

impl StateManagerActor {
    async fn run(mut self) {
        while let Some(msg) = self.receiver.recv().await {
            match msg {
                StateMessage::MergeBatchState { query_id, batch_state, response } => {
                    // Fast HashMap merge (no cloning!)
                    let state = self.states.entry(query_id).or_default();
                    for (key, agg) in batch_state {
                        state.group_by.entry(key)
                            .and_modify(|existing| existing.merge(&agg))
                            .or_insert(agg);
                    }
                    response.send(()).ok();
                }
                StateMessage::GetSnapshot { query_id, response } => {
                    let snapshot = self.states.get(&query_id).cloned();
                    response.send(snapshot).ok();
                }
                StateMessage::Initialize { query_id, initial_state } => {
                    self.states.insert(query_id, initial_state);
                }
            }
        }
    }
}
```

**Benefits**:
- âœ… Zero lock contention (single-threaded actor)
- âœ… Zero cloning during merges (in-place updates)
- âœ… Natural backpressure (channel buffering)
- âœ… Easy to make persistent (snapshot to disk)

**Performance**:
- Message passing: ~500ns per message
- HashMap merge: ~50ns per key
- **Total cost per batch**: ~10ms (vs 160ms current)
- **Improvement**: 16x faster state management

#### 2. Processing Pipeline (Parallel Workers)

**Purpose**: CPU-bound SQL execution, no I/O

```rust
/// Worker that processes batches in parallel
struct ProcessingWorker {
    worker_id: usize,
    query: StreamingQuery,
    state_tx: mpsc::UnboundedSender<StateMessage>,
}

impl ProcessingWorker {
    async fn process_batch(&self, batch: Vec<StreamRecord>) -> ProcessedBatch {
        // Build LOCAL state for THIS batch (no locks!)
        let mut local_context = ProcessorContext::new(&self.query);
        let mut output_records = Vec::with_capacity(batch.len());

        for record in batch {
            match QueryProcessor::process_query(&self.query, &record, &mut local_context) {
                Ok(result) => {
                    if let Some(output) = result.record {
                        output_records.push(Arc::new(output));
                    }
                }
                Err(e) => {
                    // Record error
                }
            }
        }

        // Send local state to actor for merging (async, non-blocking)
        let (tx, rx) = oneshot::channel();
        self.state_tx.send(StateMessage::MergeBatchState {
            query_id: self.query.id.clone(),
            batch_state: local_context.group_by_states,
            response: tx,
        }).ok();

        // Don't wait for merge to complete - return output immediately
        ProcessedBatch { output_records }
    }
}
```

**Benefits**:
- âœ… True parallelism (N workers process N batches concurrently)
- âœ… Zero locking in hot path
- âœ… Local state building (fast allocations)
- âœ… Async state merging (doesn't block worker)

**Performance**:
- Batch processing: ~9ms (same as pure SQL)
- State message send: ~500ns (non-blocking)
- **Total cost per batch**: ~9ms (vs 213ms current)
- **Improvement**: 23x faster!

#### 3. Source/Sink Pipelines (Async Streams)

**Purpose**: Overlap I/O with compute

```rust
/// Source pipeline using tokio streams
async fn source_pipeline(
    mut reader: Box<dyn DataReader>,
    batch_tx: mpsc::Sender<Vec<StreamRecord>>,
) {
    loop {
        match reader.read().await {
            Ok(batch) if !batch.is_empty() => {
                // Send to processing pipeline (with backpressure)
                if batch_tx.send(batch).await.is_err() {
                    break; // Receiver dropped
                }
            }
            Ok(_) => {
                // Empty batch - check for more data
                if !reader.has_more().await.unwrap_or(false) {
                    break;
                }
            }
            Err(e) => {
                // Handle error
                break;
            }
        }
    }
}

/// Sink pipeline using tokio streams
async fn sink_pipeline(
    mut writer: Box<dyn DataWriter>,
    mut output_rx: mpsc::Receiver<Vec<Arc<StreamRecord>>>,
) {
    while let Some(records) = output_rx.recv().await {
        if let Err(e) = writer.write_batch(records).await {
            // Handle error
        }
    }
    writer.flush().await.ok();
}
```

**Benefits**:
- âœ… Pipeline parallelism (read â†’ process â†’ write simultaneously)
- âœ… Automatic backpressure (bounded channels)
- âœ… Zero wasted cycles (async I/O)

#### 4. Job Coordinator (Lightweight Orchestrator)

```rust
/// Main job coordinator - wires everything together
pub struct JobCoordinatorV2 {
    config: JobProcessingConfig,
    observability: Option<ObservabilityConfig>,
}

impl JobCoordinatorV2 {
    pub async fn run_job(
        &self,
        reader: Box<dyn DataReader>,
        writer: Box<dyn DataWriter>,
        query: StreamingQuery,
    ) -> JobExecutionStats {
        // 1. Spawn state manager actor
        let (state_tx, state_rx) = mpsc::unbounded_channel();
        let state_actor = StateManagerActor::new(state_rx);
        tokio::spawn(state_actor.run());

        // 2. Create processing pipeline
        let (batch_tx, batch_rx) = mpsc::channel(self.config.max_batch_size);
        let (output_tx, output_rx) = mpsc::channel(self.config.max_batch_size);

        // 3. Spawn worker pool
        let num_workers = num_cpus::get();
        for i in 0..num_workers {
            let worker = ProcessingWorker::new(i, query.clone(), state_tx.clone());
            let batch_rx = batch_rx.clone();
            let output_tx = output_tx.clone();

            tokio::spawn(async move {
                while let Some(batch) = batch_rx.recv().await {
                    let result = worker.process_batch(batch).await;
                    output_tx.send(result.output_records).await.ok();
                }
            });
        }

        // 4. Spawn source and sink pipelines
        tokio::spawn(source_pipeline(reader, batch_tx));
        tokio::spawn(sink_pipeline(writer, output_rx));

        // 5. Wait for completion and collect stats
        // ... shutdown coordination ...

        JobExecutionStats::default()
    }
}
```

**Benefits**:
- âœ… Simple coordination (no shared state)
- âœ… Easy to scale (add more workers)
- âœ… Clean shutdown (drop channels)

---

## Part 4: Performance Comparison

### V1 (Current) Timeline - 5K records, 5 batches

```
Total time: 213ms

Timeline per batch (~43ms each):
â”œâ”€ Read batch (I/O): 5ms
â”œâ”€ LOCK #1: 8ms âŒ
â”œâ”€ Clone HashMap: 16ms âŒ
â”œâ”€ UNLOCK #1: 2ms
â”œâ”€ Process batch (CPU): 9ms
â”œâ”€ Observability: 5ms âš ï¸
â”œâ”€ LOCK #2: 8ms âŒ
â”œâ”€ Sync state: 4ms
â”œâ”€ UNLOCK #2: 2ms
â””â”€ Write batch (I/O): 4ms

Batches processed: SEQUENTIALLY (no parallelism)
```

### V2 (Proposed) Timeline - 5K records, 5 batches

```
Total time: ~25ms (8.5x faster!)

Pipeline (fully parallel):
â”Œâ”€ Source pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Read batch: 5ms           â”‚
â”‚  Send to worker: 0.5ms     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€ Worker pool (4 workers) â”€â”€â”
â”‚  Process batch: 9ms        â”‚
â”‚  Build local state: 0ms    â”‚
â”‚  Send merge msg: 0.5ms     â”‚
â”‚  Emit output: 0.5ms        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â–º State actor (async)
         â”‚   Merge state: 1ms
         â”‚
         â””â”€â–º Sink pipeline
             Write batch: 4ms

Batches processed: 4 IN PARALLEL
Observability: OPTIONAL (disabled in test)
```

**Key differences**:
- âŒ No locks in hot path
- âŒ No HashMap cloning
- âœ… 4 batches process simultaneously (4 CPU cores)
- âœ… I/O overlapped with compute
- âš ï¸ Observability opt-in (not always-on)

### Expected Throughput

| Scenario | V1 (Current) | V2 (Proposed) | Improvement |
|----------|--------------|---------------|-------------|
| **Single source, no obs** | 23K rec/sec | 200K rec/sec | 8.7x faster |
| **Single source, with obs** | 23K rec/sec | 150K rec/sec | 6.5x faster |
| **Multi-source (4 sources)** | 20K rec/sec | 600K rec/sec | 30x faster! |
| **Pure SQL baseline** | 790K rec/sec | 790K rec/sec | No change |

**Why multi-source scales better**:
- V1: Sequential batch processing from all sources
- V2: Parallel workers process batches from all sources concurrently

---

## Part 5: Migration Complexity Analysis

### Option A: Incremental Fix (Option 5 - Local Merge)

**Code Changes Required**:
```
Modified files: 1
Lines changed: ~50
New traits/types: 0
Breaking changes: 0
Testing effort: Low (existing tests work)
```

**Implementation**:
```rust
// BEFORE (common.rs:228-234)
let (group_states, window_states) = {
    let engine_lock = engine.lock().await;
    (
        engine_lock.get_group_states().clone(),  // âŒ Clone
        engine_lock.get_window_states(),
    )
};
// ... process batch ...
{
    let mut engine_lock = engine.lock().await;
    engine_lock.set_group_states(group_states);
}

// AFTER (Option 5)
// Build local state (no lock)
let mut local_context = ProcessorContext::new(&query_id);
for record in batch {
    QueryProcessor::process_query(&query, &record, &mut local_context)?;
}
// Single lock for merge
{
    let mut engine_lock = engine.lock().await;
    engine_lock.merge_batch_state(local_context.group_by_states);
}
```

**Timeline**: 2-4 hours to implement + test

### Option B: Complete Redesign (V2 Architecture)

**Code Changes Required**:
```
New files: 6-8
Modified files: 10-15
Lines changed: ~2000
New traits/types: 4-6 (StateMessage, ProcessingWorker, etc.)
Breaking changes: YES (API changes)
Testing effort: HIGH (new integration tests needed)
```

**Implementation phases**:
1. **Phase 1**: State manager actor (2 days)
2. **Phase 2**: Worker pool (2 days)
3. **Phase 3**: Pipeline integration (2 days)
4. **Phase 4**: Migration of existing features (3 days)
5. **Phase 5**: Testing & validation (3 days)

**Timeline**: 2-3 weeks

---

## Part 6: Recommendation Matrix

| Criterion | Option 5 (Incremental) | V2 (Redesign) | Winner |
|-----------|------------------------|---------------|--------|
| **Performance (single source)** | 200K rec/sec (8x) | 200K rec/sec (8x) | TIE âœ… |
| **Performance (multi-source)** | 80K rec/sec (3x) | 600K rec/sec (30x) | V2 ğŸ† |
| **Implementation time** | 4 hours | 2-3 weeks | Option 5 ğŸ† |
| **Code complexity** | +50 lines | +2000 lines | Option 5 ğŸ† |
| **Breaking changes** | None | Moderate | Option 5 ğŸ† |
| **Testing effort** | Low | High | Option 5 ğŸ† |
| **Scalability** | Limited | Excellent | V2 ğŸ† |
| **Future maintenance** | Technical debt | Clean slate | V2 ğŸ† |
| **Observability opt-out** | No | Yes | V2 ğŸ† |
| **Pipeline parallelism** | No | Yes | V2 ğŸ† |

---

## Part 7: Final Recommendation

### For Immediate Performance Gains: **Option 5 (Incremental)**

**When to choose**:
- âœ… Need performance improvement ASAP (hours, not weeks)
- âœ… Primarily single-source workloads
- âœ… 8x improvement is acceptable
- âœ… Want minimal risk/testing
- âœ… Can't afford breaking changes

**Implementation path**:
1. Implement local merge in `process_batch_with_output` (~4 hours)
2. Run overhead_breakdown test (validate 8x improvement)
3. Run comprehensive benchmarks (ensure no regressions)
4. Deploy to production

### For Long-Term Architecture: **V2 (Redesign)**

**When to choose**:
- âœ… Multi-source/multi-sink workloads are critical
- âœ… Want near-pure-SQL performance (26x improvement for multi-source)
- âœ… Can invest 2-3 weeks for migration
- âœ… Value clean architecture over quick wins
- âœ… Want optional observability overhead

**Implementation path**:
1. Develop V2 in parallel (new module: `server::processors::v2`)
2. Run performance tests comparing V1 vs V2
3. Migrate one job at a time (gradual rollout)
4. Deprecate V1 after 6 months

### **HYBRID APPROACH (Recommended)**

**Best of both worlds**:

1. **Short term** (Week 1): Implement Option 5 local merge
   - Get 8x improvement immediately
   - Ship to production quickly
   - Buy time for proper redesign

2. **Medium term** (Weeks 2-5): Develop V2 architecture
   - Build actor-based state manager
   - Implement worker pools
   - Create pipeline coordinators
   - Thorough testing

3. **Long term** (Month 2): Gradual V2 migration
   - New jobs use V2
   - Existing jobs stay on V1 (with local merge)
   - Monitor performance & stability
   - Migrate when confident

**Why this works**:
- âœ… Immediate performance gains (Option 5)
- âœ… No rush/pressure for V2 development
- âœ… Can validate V2 in production gradually
- âœ… Fallback to V1 if V2 has issues
- âœ… Best long-term architecture

---

## Part 8: Next Steps

### If choosing Option 5 (Incremental):
1. Implement merge_batch_state() in StreamExecutionEngine
2. Refactor process_batch_with_output() to use local state
3. Run job_server_overhead_breakdown test
4. Verify 8x improvement (23K â†’ 200K rec/sec)
5. Update documentation

### If choosing V2 (Redesign):
1. Create new module: `server::processors::v2`
2. Implement StateManagerActor
3. Implement ProcessingWorker
4. Create pipeline coordinators
5. Write integration tests
6. Performance validation

### If choosing HYBRID (Recommended):
1. **Week 1**: Option 5 implementation
2. **Week 2**: V2 design review & prototyping
3. **Weeks 3-4**: V2 core implementation
4. **Week 5**: V2 testing & validation
5. **Week 6+**: Gradual production migration

**Question for you**: Which path do you want to take?
