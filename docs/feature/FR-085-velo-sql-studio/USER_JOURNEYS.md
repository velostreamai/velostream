# FR-085: Velostream SQL Studio - User Journeys

> **Document Purpose**: User-centric workflows showing how people actually use the Studio
> **Last Updated**: February 2026

---

## Overview

This document maps out the primary user journeys through Velostream Studio. Each journey
describes a persona, their goal, the entry point, and a step-by-step flow with concrete
chat messages, tool calls, and artifacts produced.

The Studio supports three fundamental starting points:
1. **Data-first** ("I have data, help me query it") â€” connect to any source (Kafka, files, S3, databases), explore data, build SQL from real schemas
2. **Intent-first** ("I know what I want, generate it") â€” describe a pipeline in English, generate SQL and test data
3. **Template-first** ("Give me a starting point") â€” browse pre-built application templates, customize for your data

The Studio works with **all Velostream data sources**:
- **Kafka** â€” streaming topics with partition-level exploration
- **File / FileMmap** â€” CSV, JSON, JSON Lines, Parquet, Avro, ORC (FileMmap for high-throughput batch)
- **S3** â€” Object storage with format and compression support
- **ClickHouse** â€” Analytical database integration
- **Database (CDC)** â€” Change data capture from relational databases
- **URI-based** â€” Extensible via `kafka://`, `file://`, `s3://`, `clickhouse://` schemes

This document covers **10 user journeys**:

| # | Journey | Starting Point |
|---|---------|---------------|
| 1 | Explore First | Data-first (Kafka) |
| 2 | Greenfield | Intent-first |
| 3 | Import Existing SQL | Intent-first |
| 4 | Debug & Iterate | Data-first |
| 5 | Observe & Monitor | Data-first |
| 6 | Team Onboarding | Intent-first |
| 7 | Build Me an App | Intent-first (NL â†’ full app) |
| 8 | Start from a Template | Template-first |
| 9 | AI Proactive Intelligence | Cross-cutting |
| 10 | Files, S3, & Databases | Data-first (non-Kafka) |

Across all journeys, the AI acts as a **proactive collaborator** â€” not just responding to
requests but actively suggesting optimizations, detecting issues, recommending metrics,
and identifying opportunities the user may not have considered.

---

## Journey 1: Explore First

> **Persona**: Data engineer with a running Kafka cluster
> **Goal**: Discover what data is available and build a query from real schemas
> **Entry point**: Studio chat thread

### Flow

```
User: "Connect to kafka at broker1:9092"
  â†’ Tool: connect_source(uri: "kafka://broker1:9092")
  â†’ Stores connection in ThreadContext.connection
  â†’ Response: "Connected to broker1:9092. What would you like to explore?"

User: "What topics do I have?"
  â†’ Tool: list_topics()
  â†’ API: GET /api/topics
  â†’ Infra: TestHarnessInfra::fetch_topic_info(None)
  â†’ Artifact: TopicListArtifact
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ trades           â”‚ 6 partitions â”‚ 1.2M messages         â”‚
    â”‚ orders           â”‚ 3 partitions â”‚ 450K messages         â”‚
    â”‚ customer-events  â”‚ 1 partition  â”‚ 89K messages          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User: "What does the trades topic look like?"
  â†’ Tool: inspect_topic(topic: "trades")
  â†’ API: GET /api/topics/trades/schema
  â†’ Infra: TestHarnessInfra::fetch_topic_schema("trades", 10)
  â†’ Artifact: SchemaViewerArtifact
    â”‚ Field     â”‚ Type     â”‚ Example          â”‚
    â”‚ symbol    â”‚ String   â”‚ "AAPL"           â”‚
    â”‚ price     â”‚ Float    â”‚ 152.34           â”‚
    â”‚ quantity  â”‚ Integer  â”‚ 5000             â”‚
    â”‚ timestamp â”‚ DateTime â”‚ 2026-02-16T...   â”‚

User: "Show me the last 10 messages"
  â†’ Tool: peek_messages(topic: "trades", limit: 10, from_end: true)
  â†’ API: GET /api/topics/trades/messages?limit=10&from_end=true
  â†’ Infra: TestHarnessInfra::peek_topic_messages("trades", 10, true, None, None)
  â†’ Artifact: DataPreviewArtifact
    [Formatted JSON messages with partition, offset, timestamp metadata]

User: "Write a query to get average price per symbol in 5-minute windows"
  â†’ Tool: generate_sql(prompt, context: { schemas: { trades: inferred_schema } })
  â†’ AI uses the real discovered schema as context
  â†’ Artifact: SqlEditorArtifact

User: [clicks Test]
  â†’ Tool: test_query(sql, schema: inferred_schema)
  â†’ Artifact: TestResultsArtifact

User: [clicks Deploy]
  â†’ Tool: deploy_pipeline(notebook_id, config)
  â†’ Artifact: DeploySummaryArtifact
```

### Backend Requirements

| API Endpoint | Infra Method | Description |
|-------------|--------------|-------------|
| `POST /api/connect` | â€” | Store Kafka connection in session |
| `GET /api/topics` | `fetch_topic_info(None)` | List all topics |
| `GET /api/topics/{name}/schema` | `fetch_topic_schema(name, 10)` | Infer schema from samples |
| `GET /api/topics/{name}/messages` | `peek_topic_messages(...)` | Peek at messages |

### Key Insight

The real schema discovered from Kafka becomes the context for SQL generation. Instead of
the AI guessing field names, it uses the actual fields from `TopicSchema.fields`.

---

## Journey 2: Greenfield

> **Persona**: Developer starting a new project
> **Goal**: Design and deploy a streaming pipeline from scratch
> **Entry point**: Studio chat thread (no existing data)

### Flow

```
User: "I want to monitor trading activity in real-time"
  â†’ AI suggests schema fields based on domain knowledge
  â†’ Tool: generate_sql(prompt: "monitor trading activity in real-time")
  â†’ Artifact: SqlEditorArtifact (with @metric annotations)
    -- @metric: trade_volume
    -- @metric_type: counter
    -- @metric_labels: symbol
    SELECT symbol, SUM(quantity) as volume, AVG(price) as avg_price
    FROM trades
    GROUP BY symbol
    WINDOW TUMBLING(INTERVAL '5' MINUTE)
    EMIT CHANGES

User: "Generate some test data for this"
  â†’ Tool: generate_data(schema: inferred_from_sql, records: 1000)
  â†’ API: POST /api/generate-data
  â†’ Artifact: DataPreviewArtifact (synthetic data sample)

User: [clicks Test]
  â†’ Tool: test_query(sql, schema: inferred, records: 1000)
  â†’ Artifact: TestResultsArtifact
    âœ… Passed (3/3 assertions)
    â€¢ record_count: 5 (expected: > 0)
    â€¢ schema_contains: [symbol, volume, avg_price]
    â€¢ no_nulls: [symbol, volume, avg_price]

User: "Add alerting when volume exceeds 1M"
  â†’ AI modifies SQL with @alert annotation
  â†’ Tool: generate_sql(prompt: "add alert when volume > 1M", context: { previousSql })
  â†’ Artifact: SqlEditorArtifact (updated with @alert)

User: [clicks Deploy]
  â†’ Tool: deploy_pipeline(notebook_id, config: { deploy_dashboard: true })
  â†’ Artifact: DeploySummaryArtifact (jobs + metrics + alerts + Grafana link)
```

### Backend Requirements

Existing tools only â€” no new endpoints needed:
- `POST /api/nl-to-sql` â€” SQL generation
- `POST /api/generate-data` â€” Synthetic data via `SchemaDataGenerator`
- `POST /api/test` â€” Test harness execution
- `POST /api/deploy` â€” Pipeline deployment

---

## Journey 3: Import Existing SQL

> **Persona**: Developer with existing streaming SQL files
> **Goal**: Validate, test, and deploy existing SQL through the Studio
> **Entry point**: Paste or upload SQL into chat

### Flow

```
User: [pastes SQL into chat]
    SELECT symbol, AVG(price) as avg_price, COUNT(*) as trade_count
    FROM trades
    GROUP BY symbol
    WINDOW TUMBLING(INTERVAL '5' MINUTE)
    EMIT CHANGES

  â†’ Tool: validate_sql(sql)
  â†’ API: POST /api/validate
  â†’ Artifact: SqlEditorArtifact (validated, with syntax highlighting)
  â†’ Response: "Valid streaming SQL. Groups by symbol with 5-minute tumbling windows."

User: "Test this query"
  â†’ AI infers schema from SQL (fields: symbol STRING, price DECIMAL, quantity INTEGER)
  â†’ Tool: test_query(sql, schema: inferred, records: 1000)
  â†’ Generates synthetic data matching inferred schema
  â†’ Artifact: TestResultsArtifact

User: "Add monitoring to this query"
  â†’ AI analyzes query and suggests appropriate @metric annotations
  â†’ Tool: generate_sql(prompt: "add monitoring", context: { previousSql })
  â†’ Artifact: SqlEditorArtifact (annotated version)
    -- @metric: avg_price_per_symbol
    -- @metric_type: gauge
    -- @metric_labels: symbol
    SELECT symbol, AVG(price) as avg_price, COUNT(*) as trade_count
    FROM trades
    GROUP BY symbol
    WINDOW TUMBLING(INTERVAL '5' MINUTE)
    EMIT CHANGES

User: [clicks Deploy]
  â†’ Tool: deploy_pipeline(notebook_id, config: { deploy_dashboard: true })
  â†’ Artifact: DeploySummaryArtifact
```

### Backend Requirements

Uses existing endpoints:
- `POST /api/validate` â€” SQL validation via `SqlValidator`
- `POST /api/test` â€” Schema inference via `SchemaInferencer::infer_from_sql()`
- `POST /api/nl-to-sql` â€” Annotation via `Annotator::analyze()`
- `POST /api/deploy` â€” Pipeline deployment

---

## Journey 4: Debug & Iterate

> **Persona**: Developer debugging a running pipeline
> **Goal**: Compare input and output topics to find data issues
> **Entry point**: Studio chat with Kafka connection

### Flow

```
User: "Connect to broker1:9092 and show me the output topic 'trade-alerts'"
  â†’ Tool: connect_source(uri: "kafka://broker1:9092")
  â†’ Tool: inspect_topic(topic: "trade-alerts")
  â†’ Artifact: SchemaViewerArtifact (inferred schema of output)

User: "Show me the last 5 messages from trade-alerts"
  â†’ Tool: peek_messages(topic: "trade-alerts", limit: 5, from_end: true)
  â†’ Artifact: DataPreviewArtifact
    { "symbol": "AAPL", "alert_type": "volume_spike", "price": 0.00, ... }
    { "symbol": "TSLA", "alert_type": "volume_spike", "price": 0.00, ... }

User: "These prices look wrong â€” they're all zero. The source data is on 'raw-trades'"
  â†’ Tool: peek_messages(topic: "raw-trades", limit: 5, from_end: true)
  â†’ Artifact: DataPreviewArtifact (side-by-side comparison)
    { "symbol": "AAPL", "price": 185.42, "quantity": 5000, ... }
    { "symbol": "TSLA", "price": 242.10, "quantity": 3200, ... }
  â†’ AI: "The source has valid prices. The issue is likely in the SQL query.
         Can you share the query that populates trade-alerts?"

User: [pastes SQL â€” reveals a missing field in SELECT]
  â†’ Tool: validate_sql(sql)
  â†’ AI: "Found it â€” the query selects `amount` but the source field is `price`.
         Here's the fix:"
  â†’ Artifact: SqlEditorArtifact (corrected SQL)

User: [clicks Test] â†’ verifies fix â†’ [clicks Deploy] â†’ re-deploys
```

### Backend Requirements

| API Endpoint | Infra Method | Description |
|-------------|--------------|-------------|
| `POST /api/connect` | â€” | Store connection |
| `GET /api/topics/{name}/schema` | `fetch_topic_schema(...)` | Inspect output topic |
| `GET /api/topics/{name}/messages` | `peek_topic_messages(...)` | Peek at both topics |
| `POST /api/validate` | â€” | Validate corrected SQL |

---

## Journey 5: Observe & Monitor

> **Persona**: DevOps/SRE adding observability to existing pipelines
> **Goal**: Add metrics, alerts, and dashboards to deployed SQL
> **Entry point**: Paste existing SQL into Studio

### Flow

```
User: [pastes existing production SQL]
    SELECT region, COUNT(*) as order_count, SUM(total) as revenue
    FROM orders
    GROUP BY region
    WINDOW TUMBLING(INTERVAL '1' MINUTE)
    EMIT CHANGES

User: "Add Prometheus metrics and alerting to this"
  â†’ Tool: generate_sql(prompt: "add metrics and alerting", context: { previousSql })
  â†’ Artifact: SqlEditorArtifact (annotated version)
    -- @metric: order_count_by_region
    -- @metric_type: counter
    -- @metric_labels: region
    -- @metric: revenue_by_region
    -- @metric_type: gauge
    -- @metric_labels: region
    -- @alert: revenue > 100000
    -- @alert_severity: warning
    SELECT region, COUNT(*) as order_count, SUM(total) as revenue
    FROM orders
    GROUP BY region
    WINDOW TUMBLING(INTERVAL '1' MINUTE)
    EMIT CHANGES

User: "Generate a Grafana dashboard for this"
  â†’ Tool: generate_dashboard(notebook_id)
  â†’ API: POST /api/dashboards/generate
  â†’ Artifact: GrafanaEmbed (dashboard preview)

User: [clicks Deploy]
  â†’ Tool: deploy_pipeline(notebook_id, config: { deploy_dashboard: true })
  â†’ Artifact: DeploySummaryArtifact
    â€¢ 1 streaming SQL job
    â€¢ 2 Prometheus metrics
    â€¢ 1 alert rule (revenue > 100K)
    â€¢ Grafana dashboard: http://grafana:3000/d/xyz/orders
```

### Backend Requirements

Existing endpoints only:
- `POST /api/nl-to-sql` â€” Annotation generation
- `POST /api/dashboards/generate` â€” Grafana dashboard JSON
- `POST /api/deploy` â€” Pipeline + dashboard deployment

---

## Journey 6: Team Onboarding

> **Persona**: New team member learning Velostream
> **Goal**: Learn streaming SQL concepts through guided lessons
> **Entry point**: "Start the tutorial" in Studio chat

### Flow

```
User: "Start the tutorial"
  â†’ AI begins interactive lesson sequence matching quickstart guides

  Lesson 1: Passthrough
  â†’ "Let's start with the simplest query â€” passing data through unchanged."
  â†’ Artifact: SqlEditorArtifact (pre-filled example)
    SELECT * FROM trades EMIT CHANGES
  â†’ "Click [Test] to see it work with synthetic data."
  â†’ User clicks [Test] â†’ TestResultsArtifact (all passing)
  â†’ "All records pass through. Now try adding a WHERE clause..."

  Lesson 2: Filtering
  â†’ "Filter trades to only high-value ones."
  â†’ User types: "Show me trades where price > 1000"
  â†’ AI generates SQL â†’ User tests it â†’ Learns filtering

  Lesson 3: Aggregation
  â†’ "Now let's count trades per symbol using GROUP BY."
  â†’ Guided walk-through of GROUP BY + EMIT CHANGES

  Lesson 4: Windowed Aggregation
  â†’ "Time-based windows bucket data into intervals."
  â†’ Introduces WINDOW TUMBLING(INTERVAL '5' MINUTE)

  Lesson 5: Row Windows
  â†’ "Row windows compute running statistics over the last N records."
  â†’ Introduces ROWS WINDOW BUFFER 100 ROWS

  Lesson 6: Joins
  â†’ "Enrich streaming data with reference data."
  â†’ Introduces JOIN syntax with two input topics

  Lesson 7: Metrics & Alerts
  â†’ "Add @metric annotations to make SQL self-monitoring."
  â†’ Introduces @metric, @alert annotations

  Lesson 8: Deploy
  â†’ "Deploy your query as a production pipeline."
  â†’ Walks through the deploy flow with dashboard
```

### Backend Requirements

No new endpoints â€” uses existing tools with curated prompts:
- AI maintains lesson state in thread context
- Each lesson uses `generate_sql`, `test_query`, and `deploy_pipeline`

---

## Journey 7: Build Me an App

> **Persona**: Product manager or data engineer who wants a complete application, not individual queries
> **Goal**: Go from a single English sentence to a fully deployed, multi-query application with dashboards
> **Entry point**: Studio chat thread â€” a single high-level request
> **Key difference from other journeys**: The AI generates **multiple coordinated queries**, not one at a time

### What Makes This Different

In Journeys 1-6, the user builds one query at a time. Journey 7 is the **"build the whole thing"**
experience. The user describes a business outcome and the AI designs an entire application:
multiple queries, input/output topic wiring, metrics, alerts, and a unified Grafana dashboard.

### Flow

```
User: "Build me a real-time trading analytics dashboard"

  â†’ AI analyzes the request and produces an application plan
  â†’ Tool: generate_app(prompt: "real-time trading analytics dashboard")
  â†’ API: POST /api/generate-app
  â†’ AI determines: this needs 4 coordinated queries + dashboard

  â†’ Artifact: AppPreviewArtifact
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ðŸ“‹ Application: Trading Analytics                                â”‚
    â”‚                                                                  â”‚
    â”‚ Queries (4):                                                     â”‚
    â”‚                                                                  â”‚
    â”‚ 1. VWAP Calculator                                               â”‚
    â”‚    trades â†’ trade-vwap                                           â”‚
    â”‚    SELECT symbol,                                                â”‚
    â”‚           SUM(price * quantity) / SUM(quantity) as vwap,         â”‚
    â”‚           SUM(quantity) as volume                                â”‚
    â”‚    FROM trades                                                   â”‚
    â”‚    GROUP BY symbol                                               â”‚
    â”‚    WINDOW TUMBLING(INTERVAL '5' MINUTE)                          â”‚
    â”‚    EMIT CHANGES                                                  â”‚
    â”‚                                                                  â”‚
    â”‚ 2. Volume Spike Detector                                         â”‚
    â”‚    trades â†’ trade-alerts                                         â”‚
    â”‚    -- @alert: volume > 1000000                                   â”‚
    â”‚    -- @alert_severity: warning                                   â”‚
    â”‚    SELECT symbol, SUM(quantity) as volume                        â”‚
    â”‚    FROM trades                                                   â”‚
    â”‚    GROUP BY symbol                                               â”‚
    â”‚    WINDOW TUMBLING(INTERVAL '1' MINUTE)                          â”‚
    â”‚    HAVING SUM(quantity) > 1000000                                â”‚
    â”‚    EMIT CHANGES                                                  â”‚
    â”‚                                                                  â”‚
    â”‚ 3. Price Movement Tracker                                        â”‚
    â”‚    trades â†’ price-movements                                      â”‚
    â”‚    SELECT symbol,                                                â”‚
    â”‚           AVG(price) as avg_price,                               â”‚
    â”‚           STDDEV(price) / AVG(price) as volatility               â”‚
    â”‚    FROM trades                                                   â”‚
    â”‚    GROUP BY symbol                                               â”‚
    â”‚    WINDOW TUMBLING(INTERVAL '5' MINUTE)                          â”‚
    â”‚    EMIT CHANGES                                                  â”‚
    â”‚                                                                  â”‚
    â”‚ 4. Top Movers (Cross-Query)                                      â”‚
    â”‚    price-movements â†’ top-movers                                  â”‚
    â”‚    SELECT symbol, volatility                                     â”‚
    â”‚    FROM price-movements                                          â”‚
    â”‚    WHERE volatility > 0.02                                       â”‚
    â”‚    ORDER BY volatility DESC                                      â”‚
    â”‚    LIMIT 10                                                      â”‚
    â”‚    EMIT CHANGES                                                  â”‚
    â”‚                                                                  â”‚
    â”‚ Metrics (6):                                                     â”‚
    â”‚  â€¢ vwap_per_symbol (gauge, labels: symbol)                       â”‚
    â”‚  â€¢ trade_volume (counter, labels: symbol)                        â”‚
    â”‚  â€¢ volume_spike_count (counter)                                  â”‚
    â”‚  â€¢ avg_price (gauge, labels: symbol)                             â”‚
    â”‚  â€¢ volatility (gauge, labels: symbol)                            â”‚
    â”‚  â€¢ top_movers_count (gauge)                                      â”‚
    â”‚                                                                  â”‚
    â”‚ Alerts (2):                                                      â”‚
    â”‚  â€¢ volume_spike: volume > 1M per symbol per minute               â”‚
    â”‚  â€¢ high_volatility: volatility > 5% per symbol                   â”‚
    â”‚                                                                  â”‚
    â”‚ Dashboard: 6-panel Grafana layout                                â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
    â”‚  â”‚ VWAP/Symbolâ”‚ Volume/Sym â”‚                                     â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                     â”‚
    â”‚  â”‚ Volatility â”‚ Top Movers â”‚                                     â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                     â”‚
    â”‚  â”‚ Alerts     â”‚ Throughput â”‚                                     â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
    â”‚                                                                  â”‚
    â”‚ Pipeline Topology:                                               â”‚
    â”‚  trades â”€â”€â”¬â”€â”€ VWAP Calculator â”€â”€â”€â”€ trade-vwap                    â”‚
    â”‚           â”œâ”€â”€ Volume Spike â”€â”€â”€â”€â”€â”€â”€â”€ trade-alerts                 â”‚
    â”‚           â””â”€â”€ Price Movement â”€â”€â”¬â”€â”€ price-movements               â”‚
    â”‚                                â””â”€â”€ Top Movers â”€â”€ top-movers      â”‚
    â”‚                                                                  â”‚
    â”‚              [Edit Queries] [Test All] [Deploy App â†’]            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User: "Looks good, but I also want to track order flow â€” can you add a query
       that joins trades with the orders topic?"

  â†’ AI inspects the orders topic schema (if connected) or infers it
  â†’ Adds a 5th query: Trade-Order Enrichment (JOIN)
  â†’ Updates the AppPreviewArtifact with the new query + wiring
  â†’ Updates the dashboard layout to include the new panel

User: [clicks Test All]
  â†’ Tool: test_app(app_id) â€” tests ALL queries with coordinated synthetic data
  â†’ Generates data for `trades` and `orders` with matching foreign keys
  â†’ Runs all 5 queries, validates each independently
  â†’ Artifact: TestResultsArtifact (consolidated)
    âœ… 5/5 queries passed
    â€¢ VWAP Calculator: 3 output records, 45ms
    â€¢ Volume Spike Detector: 1 alert, 52ms
    â€¢ Price Movement Tracker: 3 records, 38ms
    â€¢ Top Movers: 2 records, 22ms
    â€¢ Trade-Order Enrichment: 4 records, 89ms (JOIN)

User: [clicks Deploy App]
  â†’ Tool: deploy_app(app_id, config)
  â†’ Deploys all 5 queries as coordinated Velostream jobs
  â†’ Registers all 6 metrics with Prometheus
  â†’ Configures both alert rules
  â†’ Generates and deploys unified 6-panel Grafana dashboard
  â†’ Artifact: DeploySummaryArtifact
    â€¢ Pipeline: trading-analytics (5 jobs)
    â€¢ Dashboard: http://grafana:3000/d/trading-analytics
    â€¢ Alerts: 2 rules active
    â€¢ Status: All jobs running
```

### How the AI Designs an App

When the user says "build me an app", the AI doesn't just generate one query. It:

1. **Decomposes the request** into logical processing stages
2. **Designs the data flow** â€” which topics feed which queries, where outputs go
3. **Adds observability by default** â€” every query gets @metric annotations
4. **Creates alert rules** for business-relevant thresholds
5. **Designs the dashboard layout** â€” panels arranged by logical grouping
6. **Wires the topology** â€” multi-stage pipelines where one query's output feeds the next

If a Kafka connection is active, the AI uses real schemas to ensure field names match.
If not, it infers schemas from domain knowledge and generates synthetic data for testing.

### Backend Requirements

| API Endpoint | Description |
|-------------|-------------|
| `POST /api/generate-app` | AI generates multi-query application from NL description |
| `POST /api/apps/{id}/test` | Test all queries in an app with coordinated data |
| `POST /api/apps/{id}/deploy` | Deploy all queries + metrics + alerts + dashboard |
| `GET /api/apps/{id}` | Get app definition (queries, metrics, topology) |
| `PUT /api/apps/{id}` | Update app (add/remove/modify queries) |

---

## Journey 8: Start from a Template

> **Persona**: Developer who wants a proven starting point, not a blank canvas
> **Goal**: Select a pre-built application template, customize it for their data, and deploy
> **Entry point**: Template browser or "show me templates" in chat

### Template Library

Templates are **complete, tested application blueprints** for common streaming use cases.
Each template includes multiple SQL queries, metrics, alerts, a Grafana dashboard layout,
and a test spec. Templates are maintained as part of the Velostream distribution.

```
Available Templates:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ðŸ“¦ Trading Analytics          â”‚ 4 queries â”‚ Finance              â”‚
â”‚ VWAP, volume monitoring, price alerts, top movers                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ðŸ“¦ Fraud Detection            â”‚ 5 queries â”‚ FinTech              â”‚
â”‚ Velocity checks, amount anomalies, geo-fencing, pattern matching  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ðŸ“¦ IoT Device Monitoring      â”‚ 3 queries â”‚ IoT / Manufacturing  â”‚
â”‚ Sensor anomaly detection, fleet health, predictive maintenance    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ðŸ“¦ API Observability          â”‚ 4 queries â”‚ Platform / SRE       â”‚
â”‚ Request rate, error rate, latency percentiles, SLO tracking       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ðŸ“¦ E-Commerce Analytics       â”‚ 5 queries â”‚ Retail               â”‚
â”‚ Cart tracking, conversion funnel, revenue by region, inventory    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ðŸ“¦ Clickstream Analytics      â”‚ 3 queries â”‚ AdTech / Product     â”‚
â”‚ Session tracking, page flow, engagement scoring                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ðŸ“¦ Log Analytics              â”‚ 3 queries â”‚ DevOps               â”‚
â”‚ Error rate tracking, pattern detection, alert aggregation         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ðŸ“¦ AI Agent Monitoring        â”‚ 4 queries â”‚ AI / MLOps           â”‚
â”‚ Decision audit, latency tracking, confidence scoring, drift       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flow

```
User: "Show me templates"
  â†’ Tool: list_templates()
  â†’ API: GET /api/templates
  â†’ Artifact: TemplateBrowserArtifact
    [Grid of template cards with descriptions, query counts, categories]

User: "I want the fraud detection template"
  â†’ Tool: get_template(template_id: "fraud-detection")
  â†’ API: GET /api/templates/fraud-detection
  â†’ Artifact: AppPreviewArtifact (read-only template preview)
    Shows all 5 queries, metrics, alerts, dashboard layout, expected input schemas

User: "Customize this for my data â€” I have a 'transactions' topic on broker1:9092"
  â†’ Tool: connect_source(uri: "kafka://broker1:9092")
  â†’ Tool: inspect_topic(topic: "transactions")
  â†’ Tool: customize_template(template_id: "fraud-detection", mappings: {
      source_topic: "transactions",
      field_mappings: {
        "amount" â†’ "transaction_amount",
        "user_id" â†’ "customer_id",
        "timestamp" â†’ "event_time"
      }
    })
  â†’ API: POST /api/templates/fraud-detection/customize
  â†’ AI maps template's expected fields to actual topic fields
  â†’ AI detects extra fields in real schema and suggests additional queries
  â†’ Artifact: AppPreviewArtifact (customized version)

  â†’ AI proactive suggestion:
    "I notice your transactions topic also has a `merchant_category` field
     and a `device_fingerprint` field that aren't in the standard template.
     Would you like me to add:
     â€¢ A merchant category anomaly detector?
     â€¢ A device fingerprint velocity check?
     These are common fraud signals."

User: "Yes, add both"
  â†’ AI adds 2 more queries to the app, updates dashboard
  â†’ Artifact: AppPreviewArtifact (7 queries now)

User: [clicks Test All] â†’ Tests with real schema + synthetic data
User: [clicks Deploy App] â†’ Full deployment
```

### Template Customization Process

When a user selects a template and connects it to real data, the AI performs:

1. **Schema mapping** â€” maps template's expected fields to actual topic fields
   - Exact name matches are auto-mapped
   - Similar names are suggested (e.g., `user_id` â†’ `customer_id`)
   - Missing fields are flagged with alternatives
2. **Field type adaptation** â€” adjusts SQL types if the real data differs
   (e.g., template expects Integer but data has Float)
3. **Threshold calibration** â€” AI samples real data to suggest appropriate alert
   thresholds instead of using template defaults
   (e.g., "Your average transaction is $47, template default alert at $10K seems right,
    but I'd suggest a velocity alert at 5 transactions/minute based on your data patterns")
4. **Extra field discovery** â€” identifies fields in the real schema that aren't in
   the template and suggests additional queries that could use them
5. **Test data alignment** â€” generates synthetic data that matches the real schema
   for testing before deployment

### Template Definition Format

```yaml
# templates/fraud-detection.yaml
id: fraud-detection
name: Fraud Detection
description: Real-time fraud pattern detection for financial transactions
category: FinTech
version: "1.0"
tags: [fraud, finance, security, alerting]

# Expected input schema (mapped to actual fields during customization)
input:
  topic: transactions          # User maps this to their actual topic
  fields:
    - name: transaction_id
      type: String
      required: true
    - name: user_id
      type: String
      required: true
    - name: amount
      type: Float
      required: true
    - name: merchant_id
      type: String
      required: false
    - name: timestamp
      type: DateTime
      required: true
    - name: location
      type: String
      required: false

# Queries in execution order
queries:
  - name: velocity-check
    description: Flag users with too many transactions in a short window
    sql: |
      -- @metric: transaction_velocity
      -- @metric_type: gauge
      -- @metric_labels: user_id
      -- @alert: tx_count > 10
      -- @alert_severity: warning
      SELECT user_id, COUNT(*) as tx_count, SUM(amount) as total_amount
      FROM {input_topic}
      GROUP BY user_id
      WINDOW TUMBLING(INTERVAL '1' MINUTE)
      HAVING COUNT(*) > 5
      EMIT CHANGES
    output_topic: fraud-velocity-alerts

  - name: amount-anomaly
    description: Detect unusually large transactions
    sql: |
      -- @metric: large_transaction_count
      -- @metric_type: counter
      -- @alert: amount > {threshold_large_amount}
      SELECT transaction_id, user_id, amount, timestamp
      FROM {input_topic}
      WHERE amount > {threshold_large_amount}
      EMIT CHANGES
    output_topic: fraud-amount-alerts
    parameters:
      threshold_large_amount:
        default: 10000
        description: Transactions above this amount trigger alerts
        auto_calibrate: true   # AI adjusts based on real data sampling

  - name: geo-velocity
    description: Flag impossible travel (transactions from distant locations in short time)
    sql: |
      -- @metric: geo_anomaly_count
      -- @metric_type: counter
      SELECT t1.user_id, t1.location as loc1, t2.location as loc2,
             t1.timestamp as time1, t2.timestamp as time2
      FROM {input_topic} t1
      JOIN {input_topic} t2
        ON t1.user_id = t2.user_id
      WHERE t1.location != t2.location
      EMIT CHANGES
    output_topic: fraud-geo-alerts
    requires_fields: [location]  # Only included if location field exists

  # ... more queries

# Dashboard layout
dashboard:
  title: "Fraud Detection"
  panels:
    - title: Transaction Velocity
      query: transaction_velocity
      type: line
      position: { x: 0, y: 0, w: 12, h: 8 }
    - title: Large Transactions
      query: large_transaction_count
      type: bar
      position: { x: 12, y: 0, w: 12, h: 8 }
    - title: Geo Anomalies
      query: geo_anomaly_count
      type: gauge
      position: { x: 0, y: 8, w: 8, h: 6 }
    - title: Alert Feed
      type: table
      source: fraud-velocity-alerts
      position: { x: 8, y: 8, w: 16, h: 6 }

# Test specification
test:
  records: 5000
  seed: 42
  assertions:
    - type: record_count
      query: velocity-check
      greater_than: 0
    - type: record_count
      query: amount-anomaly
      greater_than: 0
```

### Backend Requirements

| API Endpoint | Description |
|-------------|-------------|
| `GET /api/templates` | List all available templates with metadata |
| `GET /api/templates/{id}` | Get full template definition |
| `POST /api/templates/{id}/customize` | Customize template for real data (field mapping, threshold calibration) |
| `POST /api/templates/{id}/preview` | Preview customized template as AppPreviewArtifact |

---

## Journey 9: AI Proactive Intelligence

> **Persona**: Any user, at any point in any journey
> **Goal**: The AI doesn't wait to be asked â€” it actively identifies opportunities, risks, and improvements
> **Entry point**: Happens automatically throughout all other journeys

This is not a standalone journey but a **cross-cutting capability** that enriches every
other journey. The AI monitors the conversation context and proactively intervenes when
it identifies something valuable.

### 9.1: Proactive Suggestions After Data Exploration

When the user explores a Kafka cluster (Journey 1), the AI analyzes what it finds and
suggests queries, applications, or templates before being asked.

```
User: "Connect to broker1:9092 and show me all topics"
  â†’ [Normal exploration flow â€” TopicListArtifact shows 12 topics]

ðŸ¤– AI Proactive Analysis:
  "Looking at your topics, I notice some patterns:

   ðŸ“Š You have a classic trading data pipeline:
   â€¢ trades (1.2M messages) â€” raw trade events
   â€¢ orders (450K messages) â€” order submissions
   â€¢ positions (89K messages) â€” portfolio positions

   I could build you a complete trading analytics app that:
   â€¢ Calculates real-time VWAP from trades
   â€¢ Joins orders with trades for fill rates
   â€¢ Monitors position risk from positions

   ðŸ’¡ Would you like me to build this? Or would you prefer to start
      with the 'Trading Analytics' template?

   I also see these topics that might need attention:
   âš ï¸ dead-letter-queue has 12,400 messages â€” want me to inspect them?
   âš ï¸ trades-v1 hasn't received messages in 3 days â€” possibly deprecated?"
```

### 9.2: Schema-Aware Query Optimization

After the user writes or generates SQL, the AI analyzes it against the actual data and
suggests improvements.

```
User: "Write a query to count orders per customer"
  â†’ [Normal flow â€” generates SQL]

ðŸ¤– AI Proactive Analysis:
  "Query looks correct. A few observations from your data:

   âš¡ Performance: Your orders topic has 450K messages across 3 partitions.
     The GROUP BY customer_id will create ~15,000 groups. Consider adding
     HAVING COUNT(*) > 1 to reduce output volume by ~60%.

   ðŸ“Š Suggestion: I notice orders has a `region` field. Adding it to
     GROUP BY would give you geographic breakdown at no extra cost:
     GROUP BY customer_id, region

   ðŸ”” Monitoring: This is a natural place for an alert. Want me to add:
     -- @alert: order_count > 100
     -- @alert_severity: warning
     to flag unusually active customers?

   [Apply All] [Apply Performance Only] [Ignore]"
```

### 9.3: Auto-Annotation Recommendations

When the user writes SQL without @metric annotations, the AI suggests which metrics
would be valuable and why.

```
User: [pastes bare SQL]
    SELECT symbol, AVG(price) as avg_price, SUM(quantity) as volume
    FROM trades
    GROUP BY symbol
    WINDOW TUMBLING(INTERVAL '5' MINUTE)
    EMIT CHANGES

ðŸ¤– AI Proactive Analysis:
  "This query has no observability annotations. Based on the query structure,
   I recommend adding:

   Metrics:
   â€¢ avg_price (gauge, labels: symbol) â€” track price movements per symbol
   â€¢ volume (counter, labels: symbol) â€” total volume over time
   â€¢ window_output_count (counter) â€” records emitted per window
   â€¢ processing_latency_ms (histogram) â€” window processing time

   Alerts:
   â€¢ avg_price change > 5% in 1 minute â€” sudden price movement
   â€¢ volume spike > 2x rolling average â€” unusual activity

   Dashboard:
   â€¢ 4-panel Grafana layout: price chart, volume bars, alerts feed, latency

   Would you like me to add these annotations? I can:
   1. Add just the metrics (lightweight monitoring)
   2. Add metrics + alerts (active monitoring)
   3. Add metrics + alerts + dashboard (full observability)

   [Option 1] [Option 2] [Option 3 (Recommended)]"
```

### 9.4: Pipeline Health Monitoring

When the user deploys a pipeline, the AI continues to monitor its health and
proactively surfaces issues.

```
[After deployment, user returns to the Studio thread]

ðŸ¤– AI Proactive Update:
  "Your trading-analytics pipeline has been running for 2 hours. Status update:

   âœ… VWAP Calculator: 12,500 records/sec, p99 latency 8ms
   âœ… Volume Spike Detector: 3 alerts triggered (normal range)
   âš ï¸ Price Movement Tracker: p99 latency increased from 5ms to 45ms
      â””â”€ Root cause: symbol group count grew from 50 to 800.
         The WINDOW is accumulating too many groups.
      â””â”€ Suggestion: Add WHERE symbol IN (top 100 by volume)
         or increase window interval to 15 minutes.
   âŒ Top Movers: Consumer lag growing (1,200 messages behind)
      â””â”€ Root cause: Output topic partition count (1) is bottlenecking.
         The upstream produces to 6 partitions.
      â””â”€ Suggestion: Increase top-movers topic to 6 partitions.

   [Apply Fix: Price Tracker] [Apply Fix: Top Movers] [Ignore]"
```

### 9.5: Schema Evolution Detection

When the AI detects that a source topic's schema has changed since the query was written,
it proactively alerts the user.

```
[User opens an existing notebook/thread]

ðŸ¤– AI Proactive Alert:
  "I checked the schemas of your source topics and found a change:

   ðŸ“‹ Topic 'orders' schema has evolved since your query was deployed:

   Added fields:
   + discount_code (String) â€” new since Feb 14
   + shipping_priority (String) â€” new since Feb 14

   Removed fields:
   - legacy_status (String) â€” no longer present

   Impact on your queries:
   â€¢ order-analytics: No impact (doesn't use removed field)
   â€¢ order-enrichment: âš ï¸ Uses 'legacy_status' which no longer exists
     â””â”€ This query will produce null for the legacy_status field
     â””â”€ Suggested fix: Replace with new 'shipping_priority' field

   [View Diff] [Apply Suggested Fix] [Ignore]"
```

### 9.6: NL-to-Dashboard (Skip the SQL)

For non-technical users who don't even want to see SQL â€” go directly from English to
a live dashboard.

```
User: "I just want a dashboard showing trade volume and price by symbol,
       updating every minute. I don't need to see the SQL."

  â†’ AI generates the complete app internally (queries + metrics + dashboard)
  â†’ Skips showing SqlEditorArtifact â€” goes straight to dashboard preview
  â†’ Tool: generate_app(prompt, options: { skip_sql_preview: true })

  â†’ Artifact: GrafanaEmbed (live dashboard preview)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Trading Dashboard (Preview)                                   â”‚
    â”‚                                                              â”‚
    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
    â”‚ â”‚ Volume by Symbol     â”‚ Avg Price by Symbol    â”‚            â”‚
    â”‚ â”‚ AAPL â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 125K   â”‚ AAPL  $185.42          â”‚            â”‚
    â”‚ â”‚ TSLA â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 89K      â”‚ TSLA  $242.10          â”‚            â”‚
    â”‚ â”‚ MSFT â–ˆâ–ˆâ–ˆâ–ˆ 67K        â”‚ MSFT  $378.91          â”‚            â”‚
    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
    â”‚                                                              â”‚
    â”‚ Updating every 1 minute                                      â”‚
    â”‚                [View SQL] [Edit] [Deploy â†’]                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â†’ "Here's your dashboard. I created 2 queries behind the scenes
     with @metric annotations. Click [View SQL] if you want to see
     or edit the underlying queries. Otherwise, click [Deploy] to
     go live."

User: [clicks Deploy]
  â†’ Full deployment without ever seeing SQL
```

### How Proactive Intelligence Works

The AI proactive features are powered by **context analysis at every step**:

| Trigger | What the AI Analyzes | What It Surfaces |
|---------|---------------------|-----------------|
| After `connect_source` | Source data (topics, files, tables), sizes, patterns | Suggested apps, templates, stale topics |
| After `inspect_topic` | Schema fields and types | Related queries, joins with other topics, potential issues |
| After `generate_sql` | Query structure vs data | Performance tips, missing metrics, optimization opportunities |
| After `test_query` | Test results and data patterns | Threshold suggestions, edge cases, additional assertions |
| After `deploy_pipeline` | Job metrics over time | Latency issues, consumer lag, scaling recommendations |
| On thread open | Source schemas vs deployed queries | Schema evolution alerts, deprecated field warnings |
| After `list_topics` | Topic naming patterns | Pipeline topology suggestions, DLQ inspection |

### Backend Requirements

| API Endpoint | Description |
|-------------|-------------|
| `POST /api/analyze` | Analyze thread context and return proactive suggestions |
| `POST /api/analyze/schema-diff` | Compare current topic schemas against deployed queries |
| `POST /api/analyze/performance` | Analyze running job metrics and suggest optimizations |
| `POST /api/analyze/annotations` | Suggest @metric and @alert annotations for bare SQL |

---

## Journey 10: Work with Files, S3, and Databases

> **Persona**: Data engineer or analyst with data in files, object storage, or databases
> **Goal**: Use Velostream Studio with non-Kafka data sources
> **Entry point**: Studio chat thread

Velostream supports URI-based data sources beyond Kafka. The Studio generalizes the
exploration model to work with any supported source type.

### Flow: File-Based Streaming

```
User: "I have CSV trade data at /data/trades/*.csv, show me the schema"
  â†’ Tool: connect_source(uri: "file:///data/trades/*.csv", format: "csv")
  â†’ Studio infers schema from first file
  â†’ Artifact: SchemaViewerArtifact
    â”‚ Field     â”‚ Type     â”‚ Example          â”‚
    â”‚ symbol    â”‚ String   â”‚ "AAPL"           â”‚
    â”‚ price     â”‚ Float    â”‚ 152.34           â”‚
    â”‚ qty       â”‚ Integer  â”‚ 5000             â”‚
    â”‚ ts        â”‚ String   â”‚ "2026-02-16..."  â”‚

User: "Preview the first 10 rows"
  â†’ Tool: peek_source(uri: "file:///data/trades/*.csv", limit: 10)
  â†’ Artifact: DataPreviewArtifact (tabular rows instead of JSON messages)

User: "Calculate VWAP per symbol"
  â†’ Tool: generate_sql(prompt, context: { schemas: { source: inferred_schema } })
  â†’ AI generates:
    SELECT symbol, SUM(price * qty) / SUM(qty) as vwap
    FROM file:///data/trades/*.csv
    GROUP BY symbol
    EMIT CHANGES
  â†’ Artifact: SqlEditorArtifact

User: [clicks Run]
  â†’ Executes against local file data
  â†’ Artifact: QueryResultsArtifact with live bar chart
```

### Flow: High-Throughput Batch with FileMmap

```
User: "I need to process a 50GB JSON file at /data/large_events.json as fast as possible"
  â†’ AI recognizes large file use case
  â†’ Response: "For large files, I recommend memory-mapped I/O (FileMmap) for
    maximum throughput. This uses OS virtual memory to stream through the file
    without loading it all into RAM."

User: "Yes, use mmap"
  â†’ Tool: connect_source(uri: "file:///data/large_events.json", options: { use_mmap: true, format: "json" })
  â†’ Schema inferred from first records
  â†’ Artifact: SchemaViewerArtifact

User: "Aggregate events by type and count per hour"
  â†’ Tool: generate_sql(prompt, context: { source_type: "file_mmap" })
  â†’ AI generates SQL optimized for batch processing:
    SELECT event_type, COUNT(*) as event_count
    FROM file:///data/large_events.json
    GROUP BY event_type
    WINDOW TUMBLING(INTERVAL '1' HOUR)
    EMIT CHANGES
  â†’ Artifact: SqlEditorArtifact
```

### Flow: S3 Object Storage

```
User: "Connect to my S3 bucket s3://analytics-data/events/ â€” it has Parquet files"
  â†’ Tool: connect_source(uri: "s3://analytics-data/events/", format: "parquet", options: { region: "us-west-2" })
  â†’ Lists objects, infers schema from Parquet metadata
  â†’ Artifact: SchemaViewerArtifact (Parquet schema is exact â€” no inference needed)

User: "Show me the data"
  â†’ Tool: peek_source(uri: "s3://analytics-data/events/", limit: 10)
  â†’ Artifact: DataPreviewArtifact

User: "Build me an anomaly detection app"
  â†’ Tool: generate_app(prompt, context: { source_uri: "s3://analytics-data/events/", schema: ... })
  â†’ Artifact: AppPreviewArtifact (multi-query app reading from S3)
```

### Flow: Database CDC

```
User: "Connect to my Postgres at postgres://db:5432/orders_db"
  â†’ Tool: connect_source(uri: "postgres://db:5432/orders_db")
  â†’ Lists tables, shows schemas
  â†’ Artifact: SchemaViewerArtifact (multiple tables)

User: "Stream changes from the orders table"
  â†’ AI generates CDC-aware SQL:
    SELECT * FROM postgres://db:5432/orders_db?table=orders
    EMIT CHANGES
  â†’ Artifact: SqlEditorArtifact

User: "Replicate to Kafka"
  â†’ AI generates a replication pipeline:
    CREATE STREAM order_replication AS
    SELECT * FROM postgres://db:5432/orders_db?table=orders
    INTO kafka://broker:9092/orders-replicated
    EMIT CHANGES
```

### Supported Source URIs

| URI Scheme | Source Type | Streaming | Batch (Mmap) | Formats |
|-----------|-------------|:---------:|:------------:|---------|
| `kafka://host:port/topic` | Kafka | Yes | â€” | JSON, Avro, Protobuf |
| `file:///path/to/data` | File | Yes (watch) | Yes | CSV, JSON, JSONL, Parquet, Avro, ORC |
| `s3://bucket/prefix` | S3 | â€” | Yes | CSV, JSON, JSONL, Parquet, Avro, ORC |
| `clickhouse://host:port/db` | ClickHouse | â€” | Yes | Native |
| `postgres://host:port/db` | Database (CDC) | Yes | â€” | Row-level changes |

### Backend Requirements

| API Endpoint | Description |
|-------------|-------------|
| `POST /api/connect` | Generalized source connection (any URI scheme) |
| `GET /api/sources/{id}/schema` | Infer or read schema from connected source |
| `GET /api/sources/{id}/preview` | Preview data from source (rows or messages) |
| `GET /api/sources/{id}/stats` | Source statistics (record count, size, partitions) |

### Key Insight

The Studio's exploration model is **URI-driven**. Whether data is in Kafka topics,
local CSV files, S3 Parquet datasets, or Postgres tables, the user experience is
consistent: connect â†’ explore schema â†’ preview data â†’ generate SQL â†’ test â†’ deploy.
The AI adapts its SQL generation based on the source type (e.g., using FileMmap for
large local files, suggesting batch strategies for S3, or CDC semantics for databases).

---

## Cross-Cutting Concerns

### Session & Connection Model

The Studio maintains a **session** that persists across messages in a thread. Data source
connections are stored in the `ThreadContext` and passed to backend API calls. The connection
model is **URI-driven** â€” any supported data source (Kafka, File, FileMmap, S3, ClickHouse,
Database) uses the same connect â†’ explore â†’ query pattern.

```typescript
interface ThreadContext {
  // Core fields
  sources: Map<string, SourceConfig>;
  sinks: Map<string, SinkConfig>;
  metrics: MetricAnnotation[];
  alerts: AlertAnnotation[];
  schemas: Map<string, DataSchema>;

  // Data source connections (one or more, keyed by alias or URI)
  // Supports: kafka://, file://, s3://, clickhouse://, postgres://
  connections: Map<string, DataSourceConnection>;

  // App generation state (set by generate_app / customize_template tools)
  app?: {
    id: string;
    name: string;
    queries: AppQuery[];
    dashboard?: DashboardLayout;
    templateId?: string;         // if created from a template
  };
}

interface DataSourceConnection {
  uri: string;                   // e.g. "kafka://broker1:9092", "file:///data/trades.csv"
  type: 'kafka' | 'file' | 'file_mmap' | 's3' | 'clickhouse' | 'database';
  format?: 'json' | 'csv' | 'jsonl' | 'parquet' | 'avro' | 'orc' | 'protobuf';
  options?: Record<string, string>;  // source-specific options (region, delimiter, etc.)
  schemaRegistryUrl?: string;    // Kafka-specific
  connected_at: string;
}
```

When connections are active, exploration tools automatically use them. Multiple connections
can coexist (e.g., Kafka for streaming input + S3 for historical lookups). The AI uses the
source type to tailor SQL generation â€” for example, suggesting FileMmap for large local
files, batch strategies for S3, or CDC semantics for database sources.

When an app is being built (Journey 7 or 8), the `app` field tracks the multi-query
application state, including all generated queries, dashboard layout, and template origin.

### Complete Tool Registry

| Tool | Description | API Endpoint | Phase | Artifact |
|------|-------------|--------------|-------|----------|
| **Exploration Tools** | | | | |
| `connect_source` | Connect to any data source (Kafka, File, S3, DB) | `POST /api/connect` | 1.6 | â€” (confirmation text) |
| `list_sources` | List topics/files/tables on connected source | `GET /api/sources` | 1.6 | `TopicListArtifact` / `DataPreviewArtifact` |
| `inspect_source` | Infer schema from source samples | `GET /api/sources/{id}/schema` | 1.6 | `SchemaViewerArtifact` |
| `peek_source` | Preview data from source (messages/rows) | `GET /api/sources/{id}/preview` | 1.6 | `DataPreviewArtifact` |
| **SQL Tools** | | | | |
| `generate_sql` | Generate SQL from natural language | `POST /api/nl-to-sql` | 2 | `SqlEditorArtifact` |
| `validate_sql` | Validate SQL syntax | `POST /api/validate` | 2 | `SqlEditorArtifact` |
| `execute_query` | Execute SQL and return results | `POST /api/execute` | 2 | `QueryResultsArtifact` |
| **Test Tools** | | | | |
| `test_query` | Test SQL with synthetic data | `POST /api/test` | 3 | `TestResultsArtifact` |
| `generate_data` | Generate synthetic test data | `POST /api/generate-data` | 3 | `DataPreviewArtifact` |
| **Observability Tools** | | | | |
| `generate_dashboard` | Generate Grafana dashboard | `POST /api/dashboards/generate` | 4 | `GrafanaEmbed` |
| **Deployment Tools** | | | | |
| `deploy_pipeline` | Deploy notebook as pipeline | `POST /api/deploy` | 5 | `DeploySummaryArtifact` |
| **App Generation Tools** | | | | |
| `generate_app` | Generate multi-query app from NL | `POST /api/generate-app` | 1.7 | `AppPreviewArtifact` |
| `test_app` | Test all queries in an app | `POST /api/apps/{id}/test` | 1.7 | `TestResultsArtifact` |
| `deploy_app` | Deploy all app queries + dashboard | `POST /api/apps/{id}/deploy` | 1.7 | `DeploySummaryArtifact` |
| **Template Tools** | | | | |
| `list_templates` | Browse available templates | `GET /api/templates` | 1.7 | `TemplateBrowserArtifact` |
| `get_template` | Get template detail and preview | `GET /api/templates/{id}` | 1.7 | `AppPreviewArtifact` |
| `customize_template` | Customize template for user's data | `POST /api/templates/{id}/customize` | 1.7 | `AppPreviewArtifact` |
| **AI Analysis Tools** | | | | |
| `analyze` | Proactive AI suggestions for thread | `POST /api/analyze` | 6 | â€” (inline suggestions) |
| `analyze_schema_diff` | Detect schema changes vs deployed | `POST /api/analyze/schema-diff` | 6 | `SchemaViewerArtifact` |
| `analyze_performance` | Suggest performance optimizations | `POST /api/analyze/performance` | 6 | â€” (inline suggestions) |
| `analyze_annotations` | Suggest @metric/@alert annotations | `POST /api/analyze/annotations` | 6 | `SqlEditorArtifact` |

### Artifact Types

| Artifact Type | Description | Rendered By | Introduced |
|---------------|-------------|-------------|------------|
| `sql-editor` | Editable Monaco SQL with Run/Test/Deploy buttons | `SqlEditorArtifact.tsx` | Phase 2 |
| `query-results` | Auto-selected chart or table with live data | `QueryResultsArtifact.tsx` | Phase 2 |
| `schema-viewer` | Field names, types, and sample values | `SchemaViewerArtifact.tsx` | Phase 2 |
| `test-results` | Pass/fail assertions with AI failure analysis | `TestResultsArtifact.tsx` | Phase 3 |
| `deploy-summary` | Jobs, metrics, alerts, Grafana dashboard link | `DeploySummaryArtifact.tsx` | Phase 5 |
| `topology` | React Flow pipeline DAG with live metrics | `TopologyArtifact.tsx` | Phase 4 |
| `grafana-embed` | Embedded Grafana dashboard iframe | `GrafanaEmbed.tsx` | Phase 4 |
| `topic-list` | Topic grid with partition counts and message counts | `TopicListArtifact.tsx` | Phase 2.8 |
| `data-preview` | Formatted JSON messages with offset/partition metadata | `DataPreviewArtifact.tsx` | Phase 2.8 |
| `app-preview` | Multi-query app with queries, metrics, alerts, dashboard layout, topology | `AppPreviewArtifact.tsx` | Phase 2.9 |
| `template-browser` | Template library grid with categories, descriptions, preview | `TemplateBrowserArtifact.tsx` | Phase 2.9 |

### Journey-to-Phase Mapping

| Journey | Phase 1 (Backend) | Phase 1.6 (Explore API) | Phase 1.7 (App/Template API) | Phase 2 (Frontend) | Phase 2.8 (Explore UI) | Phase 2.9 (App/Template UI) | Phase 3 (Test) | Phase 4 (Observe) | Phase 5 (Deploy) | Phase 6 (AI Intelligence) |
|---------|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| 1. Explore First | | x | | x | x | | x | | x | x |
| 2. Greenfield | x | | | x | | | x | | x | x |
| 3. Import SQL | x | | | x | | | x | | x | x |
| 4. Debug & Iterate | | x | | x | x | | x | | x | x |
| 5. Observe & Monitor | x | | | x | | | | x | x | x |
| 6. Team Onboarding | x | | | x | | | x | | x | |
| 7. Build Me an App | | x | x | x | x | x | x | x | x | x |
| 8. Start from Template | | | x | x | | x | x | | x | x |
| 9. AI Proactive Intelligence | | x | | x | | | | x | | x |
| 10. Files, S3, & Databases | x | x | x | x | x | x | x | | x | x |

---

## Infra Method Reference

The exploration endpoints delegate to existing `TestHarnessInfra` methods
in `src/velostream/test_harness/infra.rs`:

| Method | Signature | Returns |
|--------|-----------|---------|
| `fetch_topic_info` | `(&self, topic_filter: Option<&str>) -> Result<Vec<TopicInfo>>` | Topics with partition counts, message counts, watermarks |
| `fetch_topic_schema` | `(&self, topic: &str, max_records: usize) -> Result<TopicSchema>` | Inferred field names/types, sample JSON, has_keys flag |
| `peek_topic_messages` | `(&self, topic: &str, limit: usize, from_end: bool, start_offset: Option<i64>, partition_filter: Option<i32>) -> Result<Vec<TopicMessage>>` | Messages with partition, offset, key, value, timestamp, headers |
| `get_consumer_info` | `(&self) -> Result<Vec<ConsumerInfo>>` | Consumer groups with subscriptions, positions, state |

### Return Types (from `statement_executor.rs`)

```rust
pub struct TopicInfo {
    pub name: String,
    pub partitions: Vec<PartitionInfo>,
    pub total_messages: i64,
    pub is_test_topic: bool,
}

pub struct TopicSchema {
    pub topic: String,
    pub fields: Vec<(String, String)>,        // (name, type)
    pub sample_value: Option<String>,          // first record as JSON
    pub has_keys: bool,
    pub records_sampled: usize,
}

pub struct TopicMessage {
    pub partition: i32,
    pub offset: i64,
    pub key: Option<String>,
    pub value: String,                         // JSON string
    pub timestamp_ms: Option<i64>,
    pub headers: Vec<(String, String)>,
}

pub struct ConsumerInfo {
    pub group_id: String,
    pub subscribed_topics: Vec<String>,
    pub positions: Vec<ConsumerPosition>,
    pub state: ConsumerState,
}
```
