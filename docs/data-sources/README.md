# Data Sources Documentation

This directory contains all documentation related to the pluggable data sources architecture for FerrisStreams.

## ğŸ“š Documentation Structure

### Planning & Design
- **[FEATURE_REQUEST_PLUGGABLE_DATASOURCES.md](./FEATURE_REQUEST_PLUGGABLE_DATASOURCES.md)** - Original feature request and requirements
- **[ARCHITECTURAL_DECOUPLING_PLAN.md](./ARCHITECTURAL_DECOUPLING_PLAN.md)** - 10-day implementation plan (currently on Day 2)
- **[KAFKA_COUPLING_AUDIT.md](./KAFKA_COUPLING_AUDIT.md)** - Audit of existing Kafka dependencies

### Implementation Status

#### âœ… Completed (Days 1-2)
- Day 1: Kafka dependency audit and mapping
- Day 2: Core trait definitions and configuration system
  - Created `DataSource`, `DataSink`, `DataReader`, `DataWriter` traits
  - Implemented URI-based configuration system
  - Built factory registry pattern

#### ğŸš§ In Progress (Day 3)
- Kafka adapter implementation
- Backward compatibility layer

#### ğŸ“… Upcoming (Days 4-10)
- Day 4: ProcessorContext refactoring
- Day 5: Integration testing
- Day 6: Schema management
- Day 7: Error handling
- Day 8: Configuration & URI parsing
- Day 9: Documentation
- Day 10: Performance optimization

## ğŸ—ï¸ Architecture Overview

The pluggable data sources architecture enables FerrisStreams to:
- **Core Data Sources**: PostgreSQL, S3, File, Iceberg, ClickHouse, and Kafka
- **Heterogeneous data flow**: Read from one source, write to another
- **Single Binary, Scale Out**: K8s native autoscaling with horizontal pod scaling
- **Maintain backward compatibility**: Existing Kafka code continues to work
- **Support multiple protocols**: Through a unified trait-based interface

### Single Binary, Scale Out Model

FerrisStreams is designed as a **single binary** that can **scale out horizontally** using Kubernetes native autoscaling:

- **ğŸ“¦ Single Binary**: One executable handles all data source types (Kafka, ClickHouse, PostgreSQL, S3, etc.)
- **âš¡ K8s Native Autoscaling**: Automatic horizontal pod scaling based on CPU, memory, or custom metrics
- **ğŸ”„ Stateless Design**: Each instance is stateless, enabling seamless scaling
- **ğŸ¯ Resource Efficiency**: Scale specific workloads independently using K8s deployments
- **ğŸ’¡ Zero Configuration**: Auto-discovery and dynamic resource allocation

### Core Supported Data Sources

1. **PostgreSQL** - Database source/sink with CDC support
2. **S3** - Object storage for batch processing
3. **File** - Local/network file systems
4. **Iceberg** - Table format for analytics
5. **ClickHouse** - Columnar OLAP database
6. **Kafka** - Streaming message broker (existing)

### Core Components

```
src/ferris/sql/datasource/
â”œâ”€â”€ mod.rs          # Core traits (DataSource, DataSink, DataReader, DataWriter)
â”œâ”€â”€ config.rs       # Configuration and URI parsing
â””â”€â”€ registry.rs     # Factory registry for dynamic source/sink creation
```

### Example Usage

```rust
// PostgreSQL CDC to Kafka
let source = create_source("postgresql://localhost/db?table=orders&cdc=true")?;
let sink = create_sink("kafka://localhost:9092/orders-stream")?;

// Kafka to ClickHouse 
let source = create_source("kafka://localhost:9092/events")?;
let sink = create_sink("clickhouse://localhost:8123/analytics?table=events")?;

// File to Iceberg
let source = create_source("file:///data/input/*.csv")?;
let sink = create_sink("iceberg://catalog/namespace/table")?;

// S3 to ClickHouse Analytics
let source = create_source("s3://bucket/data/*.parquet")?;
let sink = create_sink("clickhouse://localhost:8123/warehouse?table=facts")?;
```

## ğŸ”— Quick Links

- [Current Implementation](../../src/ferris/sql/datasource/)
- [SQL Engine Core](../../src/ferris/sql/)
- [Kafka Module](../../src/ferris/kafka/) (being decoupled)

## ğŸ“Š Progress Tracking

```
Week 1: Core Decoupling
[â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘] 40% - Day 2/5 Complete âœ…

Week 2: Advanced Features  
[â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0% - Not Started

Overall Progress
[â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 20% - 2/10 Days Complete âœ…
```

## ğŸ¯ Next Steps

1. **Day 3**: Implement Kafka adapter with new traits
2. **Day 4**: Refactor ProcessorContext for heterogeneous sources
3. **Day 5**: Integration testing and validation

## ğŸ“ Notes

- Architecture is already 90% ready for pluggable sources
- SQL engine core has minimal Kafka coupling
- Focus on additive changes to maintain backward compatibility