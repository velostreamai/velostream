# Data Sources Documentation

This directory contains all documentation related to the pluggable data sources architecture for FerrisStreams.

## ğŸ“š Documentation Structure

### Planning & Design
- **[FEATURE_REQUEST_PLUGGABLE_DATASOURCES.md](./FEATURE_REQUEST_PLUGGABLE_DATASOURCES.md)** - Original feature request and requirements
- **[ARCHITECTURAL_DECOUPLING_PLAN.md](./ARCHITECTURAL_DECOUPLING_PLAN.md)** - 10-day implementation plan (currently on Day 2)
- **[KAFKA_COUPLING_AUDIT.md](./KAFKA_COUPLING_AUDIT.md)** - Audit of existing Kafka dependencies

### Implementation Status

#### âœ… Completed (Days 1-8)
- **Day 1**: Kafka dependency audit and mapping
- **Day 2**: Core trait definitions and configuration system
  - Created `DataSource`, `DataSink`, `DataReader`, `DataWriter` traits
  - Implemented URI-based configuration system
  - Built factory registry pattern
- **Day 3**: Kafka adapter implementation
  - Full adapter with backward compatibility
  - Stream-based async reading
- **Day 4**: ProcessorContext refactoring
  - Extracted 450+ lines to dedicated files
  - Clean separation of concerns
- **Day 5**: Integration testing framework
  - Comprehensive test coverage
  - Performance benchmarks
- **Day 6**: Schema management system
  - Provider-based architecture
  - Schema evolution and caching
- **Day 7**: Error handling and recovery
  - Circuit breakers and retry logic
  - Dead letter queues
- **Day 8**: Configuration & URI parsing âœ…
  - Complete URI parser with multi-host support
  - Validation framework with detailed errors
  - Environment-based configuration
  - Builder pattern with fluent API

#### ğŸš§ In Progress (Day 9)
- Documentation updates
- Module organization cleanup

#### ğŸ“… Upcoming (Day 10)
- Day 10: Performance optimization and final testing

## ğŸ—ï¸ Architecture Overview

The pluggable data sources architecture enables FerrisStreams to:
- **Core Data Sources**: PostgreSQL, S3, File, Iceberg, ClickHouse, and Kafka
- **Heterogeneous data flow**: Read from one source, write to another
- **Single Binary, Scale Out**: K8s native autoscaling with horizontal pod scaling
- **Maintain backward compatibility**: Existing Kafka code continues to work
- **Support multiple protocols**: Through a unified trait-based interface

### Single Binary, Scale Out Model

FerrisStreams is designed as a **single binary** that can **scale out horizontally** using Kubernetes native autoscaling:

- **ğŸ“¦ Single Binary**: One executable handles all data source types (Kafka, ClickHouse, PostgreSQL, S3, etc.)
- **âš¡ K8s Native Autoscaling**: Automatic horizontal pod scaling based on CPU, memory, or custom metrics
- **ğŸ”„ Stateless Design**: Each instance is stateless, enabling seamless scaling
- **ğŸ¯ Resource Efficiency**: Scale specific workloads independently using K8s deployments
- **ğŸ’¡ Zero Configuration**: Auto-discovery and dynamic resource allocation

### Core Supported Data Sources

1. **PostgreSQL** - Database source/sink with CDC support
2. **S3** - Object storage for batch processing
3. **File** - Local/network file systems
4. **Iceberg** - Table format for analytics
5. **ClickHouse** - Columnar OLAP database
6. **Kafka** - Streaming message broker (existing)

### Core Components

```
src/ferris/sql/datasource/
â”œâ”€â”€ mod.rs          # Module exports only (following Rust best practices)
â”œâ”€â”€ traits.rs       # Core traits (DataSource, DataSink, DataReader, DataWriter)
â”œâ”€â”€ types.rs        # Type definitions (SourceOffset, Metadata, Errors)
â”œâ”€â”€ config/         # Configuration subsystem
â”‚   â”œâ”€â”€ mod.rs      # Module exports
â”‚   â”œâ”€â”€ types.rs    # Config types (DataSourceConfig, ConfigError)
â”‚   â””â”€â”€ ...         # URI parsing, validation, environment
â”œâ”€â”€ kafka/          # Kafka adapter implementation
â”‚   â”œâ”€â”€ mod.rs      # Module exports
â”‚   â”œâ”€â”€ data_source.rs  # KafkaDataSource implementation
â”‚   â”œâ”€â”€ data_sink.rs    # KafkaDataSink implementation
â”‚   â”œâ”€â”€ reader.rs       # Stream-based reader
â”‚   â”œâ”€â”€ writer.rs       # Producer wrapper
â”‚   â””â”€â”€ error.rs        # Kafka-specific errors
â””â”€â”€ registry.rs     # Factory registry for dynamic source/sink creation
```

### Example Usage

```rust
// File CSV to Kafka
let source = create_source("file:///data/orders.csv?format=csv&delimiter=,")?;
let sink = create_sink("kafka://localhost:9092/orders-stream")?;

// Kafka to File JSON-Lines
let source = create_source("kafka://localhost:9092/events")?;
let sink = create_sink("file:///output/events.jsonl?format=jsonl")?;

// File Parquet to PostgreSQL
let source = create_source("file:///data/analytics/*.parquet")?;
let sink = create_sink("postgresql://localhost/warehouse?table=facts")?;

// PostgreSQL CDC to Kafka
let source = create_source("postgresql://localhost/db?table=orders&cdc=true")?;
let sink = create_sink("kafka://localhost:9092/orders-stream")?;

// S3 to ClickHouse Analytics
let source = create_source("s3://bucket/data/*.parquet?region=us-west-2")?;
let sink = create_sink("clickhouse://localhost:8123/warehouse?table=facts")?;

// File to Iceberg
let source = create_source("file:///data/input/*.json?format=json")?;
let sink = create_sink("iceberg://catalog/namespace/table")?;
```

## ğŸ”— Quick Links

- [Current Implementation](../../src/ferris/sql/datasource/)
- [SQL Engine Core](../../src/ferris/sql/)
- [Kafka Module](../../src/ferris/kafka/) (being decoupled)

## ğŸ“Š Progress Tracking

```
Week 1: Core Decoupling
[â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“] 100% - Days 1-5 Complete âœ…
  âœ… Day 1: Kafka audit
  âœ… Day 2: Core traits
  âœ… Day 3: Kafka adapter
  âœ… Day 4: ProcessorContext
  âœ… Day 5: Integration testing

Week 2: Advanced Features  
[â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘] 80% - Days 6-9 In Progress
  âœ… Day 6: Schema management
  âœ… Day 7: Error handling
  âœ… Day 8: Configuration & URI
  ğŸš§ Day 9: Documentation
  â³ Day 10: Performance

Overall Progress
[â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘] 80% - 8/10 Days Complete âœ…
```

## ğŸ¯ Next Steps

1. **Day 9** (In Progress): Complete documentation updates
2. **Day 10**: Performance optimization and benchmarking
3. **Post-Plan**: Begin implementing additional data sources:
   - File I/O adapter (CSV, JSON, Parquet)
   - PostgreSQL CDC adapter
   - S3 batch processing
   - ClickHouse analytics sink
   - Iceberg table format support

## ğŸ“ Notes

- âœ… Architecture successfully decoupled from Kafka
- âœ… SQL engine core now uses trait-based abstractions
- âœ… Full backward compatibility maintained
- âœ… Module organization follows Rust best practices (no definitions in mod.rs)
- âœ… Comprehensive configuration system with URI parsing
- âœ… Schema management with provider-based architecture
- âœ… Advanced error handling with circuit breakers
- ğŸ¯ Ready to add new data sources without breaking changes