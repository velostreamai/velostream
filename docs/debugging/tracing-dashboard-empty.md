# Distributed Tracing Dashboard Empty - Root Cause Analysis

**Date**: 2025-10-12
**Status**: üöß **STUB IMPLEMENTATION - NOT PRODUCTION READY**
**Impact**: Velostream Distributed Tracing dashboard shows no data

---

## üîç Problem

The Grafana distributed tracing dashboard (`velostream-tracing.json`) is empty when running the trading demo, despite the `--enable-tracing` flag being set.

**Dashboard Queries Failing:**
```promql
sum(rate(traces_spanmetrics_calls_total[5m])) by (service_name)
sum(rate(traces_spanmetrics_calls_total{status_code="STATUS_CODE_ERROR"}[5m]))
histogram_quantile(0.95, sum(rate(traces_spanmetrics_latency_bucket[5m])) by (le, service_name))
```

---

## üî¥ Root Causes

### 1. TelemetryProvider is a Stub Implementation ‚ùå

**File**: `src/velostream/observability/telemetry.rs`

The `TelemetryProvider` only logs debug messages to console - it does NOT export traces via OpenTelemetry:

```rust
// Current implementation - lines 16-38
pub async fn new(config: TracingConfig) -> Result<Self, SqlError> {
    log::warn!("‚ö†Ô∏è  TRACING STUB: TelemetryProvider is a mock implementation");
    log::warn!("‚ö†Ô∏è  Traces are logged to console but NOT exported to Tempo/OTLP");

    // No OpenTelemetry SDK initialization
    // No OTLP exporter configuration
    // No tracer provider setup

    Ok(Self {
        config,
        active: true,
    })
}
```

**What's Missing:**
- ‚ùå No `opentelemetry` crate integration
- ‚ùå No `opentelemetry-otlp` exporter
- ‚ùå No `TracerProvider` initialization
- ‚ùå No span processor configuration
- ‚ùå No OTLP endpoint configuration

### 2. No Tempo Service in Docker Compose ‚ùå

**File**: `demo/trading/kafka-compose.yml`

The docker-compose only includes:
- ‚úÖ Kafka + Zookeeper
- ‚úÖ Prometheus
- ‚úÖ Grafana
- ‚ùå **NO Tempo service**

**Missing Configuration:**
```yaml
# This service does NOT exist in kafka-compose.yml
tempo:
  image: grafana/tempo:latest
  ports:
    - "4317:4317"  # OTLP gRPC
    - "4318:4318"  # OTLP HTTP
  # ... tempo configuration
```

### 3. Dashboard Expects Tempo Span Metrics ‚ùå

The Grafana dashboard queries Prometheus metrics that should be generated by Tempo's **span metrics processor**:

```json
{
  "expr": "sum(rate(traces_spanmetrics_calls_total[5m])) by (service_name)",
  "legendFormat": "{{service_name}}"
}
```

**These metrics are generated by Tempo**, not by the application code.

---

## ‚úÖ Current Behavior (With Debug Logging)

When you run the demo with `RUST_LOG=info`, you'll now see:

```
‚ö†Ô∏è  TRACING STUB: TelemetryProvider is a mock implementation
‚ö†Ô∏è  Traces are logged to console but NOT exported to Tempo/OTLP
‚ö†Ô∏è  To see traces in Grafana, implement OpenTelemetry SDK integration
üîç Phase 4: Initializing distributed tracing (STUB) for service 'velostream-trading'
üìä Tracing configuration: sampling_ratio=1.0, console_output=true (logging only)
üîç Span creation enabled - spans will be logged as debug messages
üîç No OTLP exporter configured - traces will NOT appear in Grafana

üîç [STUB] Starting SQL query span: select from source: kafka_topic (not exported to Tempo)
üîç SPAN: sql_query:select | source=kafka_topic | query=SELECT price, volume FROM market_data...
üîç [STUB] Starting streaming span: data_ingestion with 1000 records (not exported to Tempo)
```

---

## üõ†Ô∏è How to Fix

### Phase 1: Add Tempo Service to Docker Compose

**File**: `demo/trading/kafka-compose.yml`

```yaml
services:
  # ... existing services ...

  tempo:
    image: grafana/tempo:latest
    container_name: velo-tempo
    ports:
      - "4317:4317"  # OTLP gRPC
      - "4318:4318"  # OTLP HTTP
      - "3200:3200"  # Tempo query API
    command:
      - "-config.file=/etc/tempo.yaml"
    volumes:
      - ./monitoring/tempo/tempo.yaml:/etc/tempo.yaml
      - tempo_data:/tmp/tempo
    depends_on:
      - prometheus

volumes:
  # ... existing volumes ...
  tempo_data:
```

**Create**: `demo/trading/monitoring/tempo/tempo.yaml`

```yaml
server:
  http_listen_port: 3200

distributor:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318

storage:
  trace:
    backend: local
    local:
      path: /tmp/tempo/traces

metrics_generator:
  registry:
    external_labels:
      source: tempo
  storage:
    path: /tmp/tempo/generator/wal
    remote_write:
      - url: http://prometheus:9090/api/v1/write
        send_exemplars: true
  processor:
    span_metrics:
      dimensions:
        - name: service.name
          source_labels: [service_name]
        - name: span.name
        - name: span.kind
        - name: status.code
```

### Phase 2: Implement OpenTelemetry SDK Integration

**Add Dependencies**: `Cargo.toml`

```toml
[dependencies]
opentelemetry = "0.21"
opentelemetry-otlp = "0.14"
opentelemetry_sdk = { version = "0.21", features = ["rt-tokio"] }
opentelemetry-semantic-conventions = "0.13"
```

**Update TelemetryProvider**: `src/velostream/observability/telemetry.rs`

```rust
use opentelemetry::{global, trace::{Tracer, TracerProvider}, KeyValue};
use opentelemetry_sdk::{
    trace::{self, RandomIdGenerator, Sampler},
    Resource,
};
use opentelemetry_otlp::WithExportConfig;

pub struct TelemetryProvider {
    config: TracingConfig,
    tracer: Box<dyn Tracer + Send + Sync>,
    active: bool,
}

impl TelemetryProvider {
    pub async fn new(config: TracingConfig) -> Result<Self, SqlError> {
        let otlp_endpoint = config.otlp_endpoint
            .unwrap_or_else(|| "http://localhost:4317".to_string());

        log::info!("üîç Initializing OpenTelemetry with OTLP endpoint: {}", otlp_endpoint);

        // Initialize OTLP exporter
        let exporter = opentelemetry_otlp::new_exporter()
            .tonic()
            .with_endpoint(otlp_endpoint)
            .build_span_exporter()?;

        // Create tracer provider
        let provider = trace::TracerProvider::builder()
            .with_batch_exporter(exporter, opentelemetry_sdk::runtime::Tokio)
            .with_config(
                trace::config()
                    .with_sampler(Sampler::AlwaysOn)
                    .with_id_generator(RandomIdGenerator::default())
                    .with_resource(Resource::new(vec![
                        KeyValue::new("service.name", config.service_name.clone()),
                    ]))
            )
            .build();

        let tracer = provider.tracer("velostream");
        global::set_tracer_provider(provider);

        log::info!("‚úÖ Distributed tracing initialized - spans will be exported to Tempo");

        Ok(Self {
            config,
            tracer: Box::new(tracer),
            active: true,
        })
    }

    pub fn start_sql_query_span(&self, query: &str, source: &str) -> QuerySpan {
        if !self.active {
            return QuerySpan::new_inactive();
        }

        let span = self.tracer
            .span_builder(format!("sql_query:{}", Self::extract_operation_name(query)))
            .with_attributes(vec![
                KeyValue::new("db.statement", query.to_string()),
                KeyValue::new("source", source.to_string()),
            ])
            .start(&*self.tracer);

        log::debug!("üîç Started SQL span (will be exported to Tempo)");
        QuerySpan::new_active(span)
    }
}
```

### Phase 3: Configure Grafana Tempo Data Source

**File**: `demo/trading/monitoring/grafana/provisioning/datasources/datasources.yml`

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true

  - name: Tempo
    type: tempo
    access: proxy
    url: http://tempo:3200
    jsonData:
      httpMethod: GET
      serviceMap:
        datasourceUid: prometheus
```

### Phase 4: Update Dashboard Data Source

**File**: `demo/trading/monitoring/grafana/dashboards/velostream-tracing.json`

Change all panel datasources from `prometheus` to `tempo` for trace queries.

---

## üìä Expected Result After Implementation

Once implemented, you'll see:

1. **Startup Logs:**
   ```
   üîç Initializing OpenTelemetry with OTLP endpoint: http://localhost:4317
   ‚úÖ Distributed tracing initialized - spans will be exported to Tempo
   üîç Started SQL span (will be exported to Tempo)
   ```

2. **Tempo Receiving Traces:**
   ```
   level=info ts=2025-10-12T10:00:00Z caller=handler.go msg="received trace" traceID=abc123...
   ```

3. **Grafana Dashboard Populated:**
   - Request rate by service
   - Error rate percentages
   - Latency percentiles (p50/p95/p99)
   - Service dependency map
   - Active trace statistics

---

## üéØ Implementation Priority

1. **High Priority**: Add Tempo service to docker-compose (30 minutes)
2. **High Priority**: Implement OpenTelemetry SDK integration (2-4 hours)
3. **Medium Priority**: Configure Grafana data sources (15 minutes)
4. **Low Priority**: Update dashboard queries (30 minutes)

---

## üìù Notes

- The current stub implementation is useful for development (low overhead)
- The dashboard JSON is correct - it just needs a Tempo backend
- Metrics like `traces_spanmetrics_calls_total` are generated by Tempo's span metrics processor
- 100% sampling is already configured (`sampling_ratio=1.0`)

---

## üîó References

- [OpenTelemetry Rust SDK](https://github.com/open-telemetry/opentelemetry-rust)
- [Grafana Tempo Documentation](https://grafana.com/docs/tempo/latest/)
- [OTLP Exporter Guide](https://opentelemetry.io/docs/specs/otlp/)
- [Tempo Span Metrics](https://grafana.com/docs/tempo/latest/metrics-generator/span_metrics/)
