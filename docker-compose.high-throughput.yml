# FerrisStreams High Throughput Docker Compose Override  
# Use: docker-compose -f docker-compose.yml -f docker-compose.high-throughput.yml up

version: '3.8'

services:
  # Override the main FerrisStreams service for high throughput
  ferris-streams:
    environment:
      - RUST_LOG=info                           # Balanced logging
      - FERRIS_PERFORMANCE_PROFILE=high_throughput
      - KAFKA_BATCH_SIZE=65536                  # 64KB batches
      - KAFKA_LINGER_MS=5                       # 5ms batching window
      - KAFKA_FETCH_MIN_BYTES=50000             # 50KB minimum fetch
      - KAFKA_MAX_POLL_RECORDS=5000             # Large consumer batches
      - SQL_WORKER_THREADS=16                   # Many processing threads
      - SQL_MEMORY_LIMIT_MB=8192                # Lots of memory
    volumes:
      - ./configs/ferris-high-throughput.yaml:/app/sql-config.yaml
      - ./examples:/app/examples:ro
      - ./data:/app/data
    deploy:
      resources:
        limits:
          cpus: '8.0'              # Use up to 8 CPUs
          memory: 12G              # 12GB memory limit
        reservations:
          cpus: '4.0'              # Reserve 4 CPUs
          memory: 8G               # Reserve 8GB memory

  # Kafka optimized for high throughput
  kafka:
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      # High throughput Kafka settings
      KAFKA_NUM_NETWORK_THREADS: 12
      KAFKA_NUM_IO_THREADS: 24
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 1048576       # 1MB
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 1048576    # 1MB
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600     # 100MB
      KAFKA_NUM_PARTITIONS: 12                     # More partitions for parallelism
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_MIN_INSYNC_REPLICAS: 1
      # Optimize for throughput
      KAFKA_LOG_SEGMENT_BYTES: 536870912           # 512MB segments
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000 # 5 minutes
      KAFKA_COMPRESSION_TYPE: lz4                   # Fast compression
      # Increase batch sizes
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10485760      # 10MB
      KAFKA_MESSAGE_MAX_BYTES: 10485760            # 10MB
      KAFKA_REPLICA_FETCH_WAIT_MAX_MS: 500         # 500ms wait
    deploy:
      resources:
        limits:
          cpus: '4.0'              # Dedicated CPUs for Kafka
          memory: 8G               # More memory for Kafka
        reservations:
          cpus: '2.0'
          memory: 4G

  # Enhanced multi-format producer for high volume testing
  multi-format-producer:
    command: >
      bash -c "
        # Wait for services
        sleep 15
        
        # Create topics with more partitions for throughput
        kafka-topics --bootstrap-server kafka:29092 --create --topic json-financial --partitions 12 --replication-factor 1 --if-not-exists
        kafka-topics --bootstrap-server kafka:29092 --create --topic avro-trades --partitions 12 --replication-factor 1 --if-not-exists  
        kafka-topics --bootstrap-server kafka:29092 --create --topic proto-positions --partitions 12 --replication-factor 1 --if-not-exists
        
        echo 'High throughput topics created. Producing large volume financial data...'
        
        # Produce high volume JSON financial data
        for batch in {1..100}; do
          for i in {1..1000}; do
            echo '{\"symbol\":\"AAPL\",\"price\":\"'$$(( $$RANDOM % 1000 + 100 ))'.25\",\"quantity\":'$$(( $$RANDOM % 1000 + 1 ))',\"batch\":'\$$batch',\"timestamp\":\"2024-01-01T10:00:\$$i\"}' | kafka-console-producer --broker-list kafka:29092 --topic json-financial --batch-size 65536
          done
          echo \"Produced batch \$$batch of 1000 messages\"
          sleep 1
        done
        
        echo 'High throughput data production complete. Total: 100k messages'
        tail -f /dev/null
      "