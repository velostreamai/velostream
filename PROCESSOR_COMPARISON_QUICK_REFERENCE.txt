================================================================================
QUICK REFERENCE: SimpleJobProcessor vs AdaptiveJobProcessor
================================================================================

CORE PROCESSING LOOPS:

SimpleJobProcessor (V1):
  1. Read batch from source
  2. Check if all sources finished (polling loop)
  3. Lock engine (read/write as needed)
  4. Lock context Mutex
  5. Process records sequentially
  6. Decide commit based on failure strategy
  7. Write to sink (async)
  8. Commit sources (async, per-source)
  9. Flush sinks (async)
  10. Repeat for next batch

AdaptiveJobProcessor (V2):
  1. Read batch from source
  2. Route to partition receivers (non-blocking channel send)
  3. Each partition (in parallel):
     a. Receive batch via channel
     b. Process records synchronously (NO LOCKS)
     c. Write to sink continuously
  4. Close channels when done
  5. Wait for partitions (crude 100ms sleep)
  6. Final flush/commit

EXECUTION MODEL COMPARISON:

┌─────────────────────────────────────────────────────────────────┐
│ V1: Single Engine, Shared State                                 │
├─────────────────────────────────────────────────────────────────┤
│ Reader ──→ Batch ──→ Lock Engine (RwLock)                      │
│                ├──→ Lock Context (Mutex)                        │
│                ├──→ Process records sequentially                │
│                ├──→ Decision point (commit or skip)             │
│                ├──→ Write to sink                               │
│                └──→ Commit/Flush                                │
│                                                                 │
│ Performance: 100K-200K rec/sec (single-threaded)              │
└─────────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────────┐
│ V2: Distributed Engines, Isolated State                          │
├──────────────────────────────────────────────────────────────────┤
│ Reader ──→ Batch ──→ Route to Partitions (channel send)        │
│                ├──→ Partition 0: Own Engine, Process sync       │
│                ├──→ Partition 1: Own Engine, Process sync       │
│                ├──→ Partition 2: Own Engine, Process sync       │
│                └──→ Partition 3: Own Engine, Process sync       │
│                           ↓ (all parallel)                      │
│                       Write to sink                             │
│                                                                 │
│ Performance: 400K-800K rec/sec (4 partitions, parallel)        │
└──────────────────────────────────────────────────────────────────┘

LOCK OPERATIONS PER BATCH:

V1 (SimpleJobProcessor):
  ✓ engine.read() ──────────────────────── 1-2 times
  ✓ engine.write() ─────────────────────── 0-1 times
  ✓ Mutex lock on ProcessorContext ──── 1 time
  ✓ write_batch_to.await (sink) ──────── async
  ✓ commit/flush ────────────────────────── async
  ─────────────────────────────────────────────────
  Total: 2-4 locks per batch in hot path

V2 (AdaptiveJobProcessor):
  ✓ Channel send (batch routing) ──── 1 time (negligible)
  ✓ Direct method calls (no locks) ─ per record
  ✓ Mutex lock (writer only) ──────── 1 time per write
  ─────────────────────────────────────────────────
  Total: 0 locks in hot path + 1 writer lock

ASYNC vs SYNC EXECUTION:

V1: process_batch() is ASYNC
   └─ State machine overhead: 3-5% of total time
   └─ Multiple await points in hot path
   └- Task scheduling cost

V2: PartitionReceiver::process_batch() is SYNCHRONOUS
   └─ Direct function calls
   └─ No state machine overhead
   └─ No task scheduling cost
   └─ Per-record output available immediately

SYNCHRONIZATION STRATEGIES:

V1: Per-Batch Commit
   ├─ Evaluate failure_strategy for each batch
   ├─ Atomic batch semantics (all or nothing)
   ├─ Sequential source commits
   ├─ Explicit flush operation
   └─ High per-batch overhead (100-1000μs)

V2: End-of-Job Commit
   ├─ Continuous processing (no decision per batch)
   ├─ Channel close signals EOF
   ├─ 100ms sleep waits for partitions
   ├─ Single final flush/commit
   └─ Low per-batch overhead (negligible)
   └─ Issue: 100ms sleep is crude barrier

BOTTLENECKS:

V1:
  1. Engine RwLock acquisition (1-2μs per batch)
  2. Sequential batch processing (no parallelism)
  3. Context Mutex lock during batch (blocks other records)
  4. Async overhead in hot path (3-5%)
  5. Per-batch commit overhead (100-1000μs)

V2:
  1. Writer Mutex contention (multiple partitions writing)
  2. Channel overhead (minimal, 100-500ns)
  3. 100ms sleep barrier at job end (only at finish)
  4. Partition spawning (one-time at start)

EXPECTED PERFORMANCE:

V1: 100K-200K rec/sec
    ├─ Constant regardless of query complexity
    ├─ Single thread (no parallelism)
    └─ Query processing dominates

V2: 400K-800K rec/sec (scales with partitions)
    ├─ 4x improvement with 4 partitions
    ├─ Linear scaling with CPU cores
    ├─ Parallel query processing
    └─ Writer becomes bottleneck at extreme scale

USE CASES:

V1 - Good For:
  • Simple queries (passthrough, basic filtering)
  • Low throughput scenarios (<50K rec/sec)
  • Single-threaded, low-latency requirements
  • Minimal resource usage

V1 - Bad For:
  • High throughput (>200K rec/sec)
  • Complex aggregations
  • Multi-partition queries
  • Multi-core systems

V2 - Good For:
  • High throughput (>400K rec/sec)
  • Complex aggregations with GROUP BY
  • Utilizing multi-core systems
  • Partition-aware queries

V2 - Bad For:
  • Ultra-low latency (<10ms)
  • Simple passthrough queries
  • Single-core systems
  • Very small datasets

================================================================================
